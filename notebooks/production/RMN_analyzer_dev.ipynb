{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a corpus-wide RMN with tfidf embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/rocassius/w266_final/scripts/assembly\")\n",
    "sys.path.append(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document import load_documents\n",
    "from constant import DOC_PRAYER_PATH, MIN_SESSION, MAX_SESSION, DOC_ALL_PATH\n",
    "from subject import subject_keywords\n",
    "\n",
    "sessions = list(range(MIN_SESSION, MAX_SESSION+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from rmn import *\n",
    "from rmn_data_generator import RMN_DataGenerator\n",
    "from rmn_analyzer import RMN_Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding tools\n",
    "prayer_tools_path = \"/home/rocassius/gen-data/tools/prayer_tools\"\n",
    "metadata_dict = load_pickled_object(os.path.join(prayer_tools_path, \"metadata_dict\"))\n",
    "tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict\"))\n",
    "embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"idf_embedding_matrix\"))\n",
    "global_embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"embedding_matrix_wg\"))\n",
    "global_tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict_wg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = load_documents([66], DOC_PRAYER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = docs_df.sample(2347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35475, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models_path = \"/home/rocassius/gen-data/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rocassius/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/rocassius/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "rmn = RigidRMN()\n",
    "rmn.load_rmn(\"SuaveRanger\", local_models_path)\n",
    "rmn.infer_embedding_matrix = global_embedding_matrix\n",
    "rmn.infer_tokenizer_dict = global_tokenizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======#====================#\n",
    "#=*= RMN Analyzer =*=#\n",
    "#====================#\n",
    "\n",
    "# Class for analyzing an RMN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from analysis import *\n",
    "\n",
    "# variable constants\n",
    "SUB = 'subject'\n",
    "SPEAK = 'speakerid'\n",
    "PARTY = 'party'\n",
    "SESS = 'session'\n",
    "# party constants\n",
    "R = 'R'\n",
    "D = 'D'\n",
    "# metric constants\n",
    "JS = 'js'\n",
    "HH = 'hh'\n",
    "EN = 'entr'\n",
    "N_REC = 'n_records'\n",
    "N_NAN = 'n_nan_preds'\n",
    "TP = 'topic_use'\n",
    "\n",
    "\n",
    "class RMN_Analyzer(object):\n",
    "    \"\"\"Class for Analyzing an RMN with respect to a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rmn, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - rmn: (RMN) the RMN to be used for analysis\n",
    "        - df : (DataFrame) the dataframe to analyze\n",
    "        \"\"\"\n",
    "        \n",
    "        'Initialization'\n",
    "        self.rmn = rmn\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.topic_preds = None\n",
    "        self.y_preds = None\n",
    "        \n",
    "    @property\n",
    "    def index(self):\n",
    "        return self.df.index\n",
    "         \n",
    "        \n",
    "    def predict_topics(self, use_generator=True):\n",
    "        \"\"\"Computes the topic predictions for all observations\n",
    "        \"\"\"\n",
    "        self.topic_preds = self.rmn.predict_topics(self.df, use_generator)\n",
    "        \n",
    "    \n",
    "    def predict_y(self, use_generator=True):\n",
    "        \"\"\"Computes the sentence vector predictions for all observations\n",
    "        \"\"\"\n",
    "        self.y_preds = self.rmn.predict_y(self.df, use_generator)\n",
    "        \n",
    "        \n",
    "    def sample_indices(self, indices, n):\n",
    "        \"\"\"Returns a SRR of the indices provided\n",
    "        \"\"\"\n",
    "        return np.random.choice(indices, n, replace=True)\n",
    "\n",
    "    \n",
    "    def bool_subset(self, col, value):\n",
    "        \"\"\"\n",
    "        Returns a boolean vector for each observation in the\n",
    "        dataframe indicating whether it meets the col == value condition\n",
    "        \"\"\"\n",
    "        assert col in self.df.columns\n",
    "        return self.df[col] == value\n",
    "    \n",
    "    \n",
    "    def bool_index(self, conditions):\n",
    "        \"\"\"\n",
    "        Returns a boolean vector for each observation in the\n",
    "        dataframe indicating whether it meets all conditions\n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        \n",
    "        Returns: \n",
    "        - pandas series of booleans indicating where all \n",
    "          of the conditions hold\n",
    "        \"\"\"\n",
    "        # initialize bool index\n",
    "        bool_index = (pd.Series(True)\n",
    "                      .repeat(self.index.shape[0])\n",
    "                      .reset_index(drop=True))\n",
    "        \n",
    "        for col, val in conditions.items():\n",
    "            bool_index = bool_index & self.bool_subset(col, val)\n",
    "            \n",
    "        return bool_index\n",
    "    \n",
    "    \n",
    "    def cond_index(self, conditions):\n",
    "        \"\"\"Returns indices of records meeting the conditions\n",
    "        \"\"\"\n",
    "        return self.index[self.bool_index(conditions)]\n",
    "    \n",
    "    \n",
    "    def n_records(self, conditions={}):\n",
    "        \"\"\"Returns the number of records meetings the conditions\n",
    "        \"\"\"\n",
    "        return len(self.cond_index(conditions))\n",
    "    \n",
    "    \n",
    "    def n_nan_preds(self, conditions={}):\n",
    "        \"\"\"Returns the number of records which have nan predictions\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        return np.isnan(self.topic_preds[cond_index].sum(axis=-1)).sum()\n",
    "    \n",
    "    \n",
    "    def compute_JS(self, index_A, index_B, base=2):\n",
    "        \"\"\"\n",
    "        Computes the mean pair-wise JS divergence and associated CI\n",
    "        between indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p_A = self.topic_preds[index_A]\n",
    "        p_B = self.topic_preds[index_B]\n",
    "        js_list = [jensenshannon(p, q, base) for p, q in zip(p_A, p_B)]\n",
    "        \n",
    "        return mean_CI(js_list)\n",
    "        \n",
    "        \n",
    "    def compute_HH(self, index):\n",
    "        \"\"\"\n",
    "        Computes the mean HH index and associated CI between\n",
    "        indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p = self.topic_preds[index]\n",
    "        hh_list = [hh_index(q) for q in p]\n",
    "        \n",
    "        return mean_CI(hh_list)\n",
    "    \n",
    "    \n",
    "    def topic_use_RD_js(self, conditions={}):\n",
    "        \"\"\"Returns the JS divergence of the R and D topic use distributions\n",
    "        \"\"\"\n",
    "        R_topic_use = self.topic_use({**conditions, **{PARTY: R}})\n",
    "        D_topic_use = self.topic_use({**conditions, **{PARTY: D}})\n",
    "        \n",
    "        return jensenshannon(R_topic_use, D_topic_use)\n",
    "    \n",
    "    \n",
    "    def topic_use_hh(self, conditions={}):\n",
    "        \"\"\"Returns the HH-index of the RD topic use distributions\n",
    "        \"\"\"\n",
    "        return hh_index(self.topic_use(conditions))\n",
    "    \n",
    "    \n",
    "    def inter_party_js(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns the estimated inter party JS divergence and a CI.\n",
    "        \n",
    "        Computes the inter party JS divergence between \n",
    "        Republicans and Democrats on a given subject\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find R and D indicies on the subject\n",
    "        index_R = self.cond_index({**conditions, **{PARTY: R}})\n",
    "        index_D = self.cond_index({**conditions, **{PARTY: D}})\n",
    "        \n",
    "        # return None if indices are insufficient\n",
    "        if len(index_R)==0 or len(index_D)==0:\n",
    "            return None\n",
    "        \n",
    "        # sample \n",
    "        samp_index_R = self.sample_indices(index_R, n)\n",
    "        samp_index_D = self.sample_indices(index_D, n)\n",
    "    \n",
    "        return self.compute_JS(samp_index_R, samp_index_D)\n",
    "    \n",
    "    \n",
    "    def group_js(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns the estimated mean JS divergence and a CI\n",
    "        \n",
    "        Estimates the average JS divergence between any two documents of\n",
    "        a group defined by the conditions. A document by speaker _i_ is \n",
    "        never compared to another document by speaker _i_.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        - n         : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find indicies of party on the subject\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        \n",
    "        # Return none if there are fewer than 2 speakers\n",
    "        if self.df.loc[cond_index][SPEAK].nunique() < 2:\n",
    "            return None\n",
    "        \n",
    "        # Sample index pairs\n",
    "        index_AB = []\n",
    "        while len(index_AB) < n:\n",
    "            a_b = self.sample_indices(cond_index, n=2)\n",
    "            # include samples whose speakers are different\n",
    "            if self.df.loc[a_b][SPEAK].nunique() == 2:\n",
    "                index_AB.append(a_b)\n",
    "        \n",
    "        index_AB = np.asarray(index_AB)\n",
    "        assert index_AB.shape == (n, 2)\n",
    "        \n",
    "        # get indices for each group\n",
    "        index_A, index_B = index_AB[:,0], index_AB[:,1]\n",
    "        \n",
    "        return self.compute_JS(index_A, index_B)\n",
    "    \n",
    "    \n",
    "    def group_hh(self, conditions={}, n=None):\n",
    "        \"\"\"\n",
    "        Returns the estimated mean HH index and a CI\n",
    "        \n",
    "        Estimates the average Herfindahlâ€“Hirschman Index \n",
    "        of all records meetings the conditons.\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - party  : (str) party of interest\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean index point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # indicies meeting the conditions\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        \n",
    "        # return None if indices are insufficient\n",
    "        if len(cond_index)==0:\n",
    "            return None\n",
    "        \n",
    "        if n is None:\n",
    "            return self.compute_HH(cond_index)\n",
    "        else:\n",
    "            samp_index = self.sample_indices(cond_index, n)\n",
    "            return self.compute_HH(samp_index)\n",
    "        \n",
    "        \n",
    "    def analyze_subset(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of analysis metrics for the subset \n",
    "        of records defined by the conditions.\n",
    "        \n",
    "        Note: It is recommended conditions be on subject\n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        - n         : (int) sample size for estimation of metrics\n",
    "        \n",
    "        for the entire dataset and for each subject the following are computed:\n",
    "        - n_records, n_records_R\n",
    "        - n_records_D\n",
    "        - js\n",
    "        - js_R\n",
    "        - js_D\n",
    "        - js_RD\n",
    "        - hh\n",
    "        - hh_R\n",
    "        - hh_D\n",
    "        \n",
    "        Returns: a dictionary of metrics\n",
    "        \"\"\"\n",
    "        # R and D added conditions\n",
    "        conditions_R = {**conditions, **{PARTY: R}}\n",
    "        conditions_D = {**conditions, **{PARTY: D}}\n",
    "        \n",
    "        # annotation tags\n",
    "        _R = '_' + R\n",
    "        _D = '_' + D\n",
    "        _RD = _R + D\n",
    "        _TP = '_' + TP\n",
    "        \n",
    "        metrics = {\n",
    "            # n records in data\n",
    "            N_REC:    self.n_records(conditions),\n",
    "            N_REC+_R: self.n_records(conditions_R),\n",
    "            N_REC+_D: self.n_records(conditions_D),\n",
    "            N_NAN+_R: self.n_nan_preds(conditions_R),\n",
    "            N_NAN+_D: self.n_nan_preds(conditions_D),\n",
    "            # JS divergence data\n",
    "            JS:     self.group_js(conditions, n),\n",
    "            JS+_R:  self.group_js(conditions_R, n),\n",
    "            JS+_D:  self.group_js(conditions_D, n),\n",
    "            JS+_RD: self.inter_party_js(conditions, n),\n",
    "            # HH index data\n",
    "            HH:    self.group_hh(conditions, n),\n",
    "            HH+_R: self.group_hh(conditions_R, n),\n",
    "            HH+_D: self.group_hh(conditions_D, n),\n",
    "            # Topic Use Metrics\n",
    "            HH+_TP:    self.topic_use_hh(conditions),\n",
    "            HH+_TP+_R: self.topic_use_hh(conditions_R),\n",
    "            HH+_TP+_D: self.topic_use_hh(conditions_D),\n",
    "            JS+_TP:    self.topic_use_RD_js(conditions),\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "        \n",
    "    def analyze(self, subjects, n):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of analysis metrics at the subject level\n",
    "        and at the session level (assuming self.df is the data of a\n",
    "        single session).\n",
    "        \n",
    "        Args:\n",
    "        - subjects: (array-like) list of subjects\n",
    "        - n       : (int) sample size for estimation of metrics\n",
    "        \n",
    "        Returns: a dictionary of metrics\n",
    "        \"\"\"\n",
    "        # analyze entire session dataset\n",
    "        dataset_metrics = self.analyze_subset(conditions={}, n=n)\n",
    "        \n",
    "        # analyze by subject\n",
    "        subject_metrics = {}\n",
    "        for s in subjects:\n",
    "            subject_metrics[s] = self.analyze_subset({SUB: s}, n)\n",
    "        \n",
    "        metrics = {'dataset' : dataset_metrics, \n",
    "                   'subjects': subject_metrics}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def shannon_entropy(self, conditions={}):\n",
    "        \"\"\"Returns the Shannon Entropy of topic predictions meeting conditions\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        return shannon_entropy(self.topic_preds[self.cond_index(conditions)])\n",
    "    \n",
    "    \n",
    "    def mean_entropy(self, conditions={}):\n",
    "        \"\"\"Returns the mean entropy of topic predictions meeting condiditons\n",
    "        \"\"\"\n",
    "        return np.nanmean(self.shannon_entropy(conditions))\n",
    "        \n",
    "    \n",
    "    def first_topic_counts(self, conditions={}):\n",
    "        \"\"\"\n",
    "        Returns a leaderboard of topics and how many times they \n",
    "        are the primary topic associated with a document.\n",
    "        \"\"\"\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "           \n",
    "        cond_index = self.cond_index(conditions)\n",
    "        topic_counts = pd.Series(np.argmax(self.topic_preds[cond_index], axis=-1)).value_counts()\n",
    "        \n",
    "        return topic_counts\n",
    "    \n",
    "    \n",
    "    def topic_use(self, conditions={}):\n",
    "        \"\"\"\n",
    "        Returns a leaderboard of topics based on the percentage of \n",
    "        total weight given to them in all of the documents\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        topic_sums = pd.Series(np.nansum(self.topic_preds[cond_index], axis=0))\n",
    "        topic_use = topic_sums.sort_values(ascending=False) / topic_sums.sum()\n",
    "        \n",
    "        return topic_use\n",
    "    \n",
    "    \n",
    "    def primary_topics(self, conditions={}, k=5):\n",
    "        \"\"\"Returns top k most prominent topics for documents\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        primary_topics = np.flip(np.argsort(self.topic_preds[cond_index]))[:,:k]\n",
    "        \n",
    "        return primary_topics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = RMN_Analyzer(rmn, data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "analyzer.predict_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = rmn.inspect_topics([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.topic_preds[0,2] = np.nan\n",
    "analyzer.topic_preds[2,44] = np.nan\n",
    "analyzer.topic_preds[944,44] = np.nan\n",
    "analyzer.topic_preds[944,1] = np.nan\n",
    "analyzer.topic_preds[900,1] = np.nan\n",
    "analyzer.topic_preds[200,5] = np.nan\n",
    "analyzer.topic_preds[245,5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_records': 35,\n",
       " 'n_records_R': 21,\n",
       " 'n_records_D': 14,\n",
       " 'n_nan_preds_R': 0,\n",
       " 'n_nan_preds_D': 0,\n",
       " 'js': {'mean': 0.9070792027697758,\n",
       "  'lower': 0.9050941816396604,\n",
       "  'upper': 0.9090642238998908},\n",
       " 'js_R': {'mean': 0.9075859307599854,\n",
       "  'lower': 0.9055981708919111,\n",
       "  'upper': 0.9095736906280597},\n",
       " 'js_D': {'mean': 0.9077800437708343,\n",
       "  'lower': 0.905785994976378,\n",
       "  'upper': 0.9097740925652904},\n",
       " 'js_RD': {'mean': 0.9027487772244022,\n",
       "  'lower': 0.9007128726488136,\n",
       "  'upper': 0.9047846817999907},\n",
       " 'hh': {'mean': 0.41508641839027405,\n",
       "  'lower': 0.41015806765690727,\n",
       "  'upper': 0.4200147644640477},\n",
       " 'hh_R': {'mean': 0.384602814912796,\n",
       "  'lower': 0.3805210227801077,\n",
       "  'upper': 0.38868461184514835},\n",
       " 'hh_D': {'mean': 0.4594230055809021,\n",
       "  'lower': 0.45356966759467127,\n",
       "  'upper': 0.465276305891037},\n",
       " 'hh_topic_use': 0.03803202,\n",
       " 'hh_topic_use_R': 0.04300683,\n",
       " 'hh_topic_use_D': 0.05627721,\n",
       " 'js_topic_use': 0.10515827823173647}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze_subset(conditions={'subject':'immigration'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_records': 2347,\n",
       " 'n_records_R': 1382,\n",
       " 'n_records_D': 959,\n",
       " 'n_nan_preds_R': 3,\n",
       " 'n_nan_preds_D': 3,\n",
       " 'js': {'mean': 0.8971323229504494,\n",
       "  'lower': 0.8947437974957849,\n",
       "  'upper': 0.8995208484051139},\n",
       " 'js_R': {'mean': 0.8992934233777085,\n",
       "  'lower': 0.8968493795483355,\n",
       "  'upper': 0.9017374672070818},\n",
       " 'js_D': {'mean': 0.8942565858023946,\n",
       "  'lower': 0.8917554643188438,\n",
       "  'upper': 0.896757707285946},\n",
       " 'js_RD': {'mean': 0.8957261537283714,\n",
       "  'lower': 0.8932374861091186,\n",
       "  'upper': 0.8982148213476242},\n",
       " 'hh': {'mean': 0.36306512355804443,\n",
       "  'lower': 0.3583856193909551,\n",
       "  'upper': 0.36774460049679786},\n",
       " 'hh_R': {'mean': 0.36951836943626404,\n",
       "  'lower': 0.36474630403541963,\n",
       "  'upper': 0.37429049701362194},\n",
       " 'hh_D': {'mean': 0.35585999488830566,\n",
       "  'lower': 0.3512379868835622,\n",
       "  'upper': 0.36048197847332086},\n",
       " 'hh_topic_use': 0.023277294,\n",
       " 'hh_topic_use_R': 0.023165543,\n",
       " 'hh_topic_use_D': 0.023929607,\n",
       " 'js_topic_use': 0.026178849637814673}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze_subset(conditions={}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.250809"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.mean_entropy({'subject':'trade'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.n_nan_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     0.039877\n",
       "14    0.037185\n",
       "8     0.034378\n",
       "1     0.033188\n",
       "15    0.033075\n",
       "45    0.030230\n",
       "0     0.029634\n",
       "44    0.029541\n",
       "2     0.028929\n",
       "11    0.028855\n",
       "28    0.027529\n",
       "34    0.026888\n",
       "38    0.024861\n",
       "24    0.023889\n",
       "33    0.023650\n",
       "49    0.022767\n",
       "9     0.022456\n",
       "25    0.021869\n",
       "31    0.021347\n",
       "12    0.021307\n",
       "46    0.020716\n",
       "17    0.020505\n",
       "3     0.020373\n",
       "37    0.020315\n",
       "32    0.019827\n",
       "26    0.019779\n",
       "39    0.019517\n",
       "40    0.018955\n",
       "47    0.017728\n",
       "20    0.017339\n",
       "6     0.017262\n",
       "22    0.017211\n",
       "36    0.017140\n",
       "7     0.017084\n",
       "21    0.015750\n",
       "4     0.015567\n",
       "30    0.015072\n",
       "35    0.014653\n",
       "18    0.013638\n",
       "13    0.013507\n",
       "10    0.011416\n",
       "48    0.011272\n",
       "29    0.011057\n",
       "27    0.009990\n",
       "42    0.009428\n",
       "41    0.009170\n",
       "23    0.007621\n",
       "16    0.006873\n",
       "19    0.006244\n",
       "43    0.003536\n",
       "dtype: float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    104\n",
       "5     100\n",
       "1      96\n",
       "8      89\n",
       "15     89\n",
       "11     79\n",
       "45     72\n",
       "0      68\n",
       "34     68\n",
       "44     67\n",
       "24     63\n",
       "2      58\n",
       "9      57\n",
       "38     54\n",
       "28     53\n",
       "25     52\n",
       "33     52\n",
       "37     52\n",
       "26     51\n",
       "31     51\n",
       "39     49\n",
       "46     49\n",
       "32     47\n",
       "12     47\n",
       "49     47\n",
       "17     47\n",
       "3      46\n",
       "47     43\n",
       "40     43\n",
       "20     41\n",
       "22     41\n",
       "7      40\n",
       "6      39\n",
       "21     37\n",
       "36     36\n",
       "30     33\n",
       "35     33\n",
       "4      31\n",
       "10     31\n",
       "48     27\n",
       "13     27\n",
       "29     23\n",
       "41     23\n",
       "18     19\n",
       "42     19\n",
       "23     15\n",
       "27     13\n",
       "19     11\n",
       "16     10\n",
       "43      5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.first_topic_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 45, 48, 26, 35],\n",
       "       [42, 44, 20, 22,  4],\n",
       "       [37, 10, 17, 39, 14],\n",
       "       ...,\n",
       "       [44, 36, 28, 31,  2],\n",
       "       [39, 33,  8, 14, 34],\n",
       "       [ 2,  5, 26,  7, 12]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.primary_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_records': 2347,\n",
       " 'n_records_R': 1382,\n",
       " 'n_records_D': 959,\n",
       " 'js': {'mean': 0.8929648605424753,\n",
       "  'lower': 0.8622081332272088,\n",
       "  'upper': 0.9237215878577419},\n",
       " 'js_R': {'mean': 0.8918299181276494,\n",
       "  'lower': 0.8616113843204268,\n",
       "  'upper': 0.9220484519348716},\n",
       " 'js_D': {'mean': 0.8911728389681157,\n",
       "  'lower': 0.8629463573982648,\n",
       "  'upper': 0.9193993205379667},\n",
       " 'js_RD': {'mean': 0.8984762036662614,\n",
       "  'lower': 0.86969344624948,\n",
       "  'upper': 0.9272589610830427},\n",
       " 'hh': {'mean': 0.3929731249809265,\n",
       "  'lower': 0.3409234876789598,\n",
       "  'upper': 0.44502267406801893},\n",
       " 'hh_R': {'mean': 0.3422833979129791,\n",
       "  'lower': 0.3015297350782954,\n",
       "  'upper': 0.38303707460574277},\n",
       " 'hh_D': {'mean': 0.412963330745697,\n",
       "  'lower': 0.361676728024675,\n",
       "  'upper': 0.4642500619147285}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze_subset(conditions={}, n =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5464397e-03, 5.9141582e-03,           nan, 3.3592423e-03,\n",
       "       4.1585718e-03, 4.6430629e-02, 4.1875597e-03, 1.9036815e-02,\n",
       "       9.0436758e-03, 7.0622428e-03, 8.2960709e-05, 3.4936439e-04,\n",
       "       1.9014528e-02, 1.0048404e-02, 8.9517375e-03, 3.3230886e-03,\n",
       "       6.1772432e-04, 4.3901094e-04, 9.3819568e-04, 3.9070428e-04,\n",
       "       1.1351042e-03, 1.2289655e-03, 8.3495874e-04, 1.4976026e-04,\n",
       "       6.4706677e-03, 1.0228754e-02, 2.6048886e-02, 3.7255071e-03,\n",
       "       1.2589485e-02, 3.1999624e-04, 9.3538733e-04, 4.6058921e-03,\n",
       "       1.9303185e-04, 4.7712303e-03, 1.5027004e-02, 8.6307497e-04,\n",
       "       1.9048254e-03, 6.8330963e-05, 1.1928496e-03, 3.1921931e-04,\n",
       "       1.1857332e-02, 1.4633985e-03, 9.5607882e-04, 2.4262549e-04,\n",
       "       5.8718733e-03, 1.1712279e-02, 4.0127989e-03, 2.5536839e-03,\n",
       "       3.2589883e-03, 4.3265073e-04], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001, 0.   , 0.001, 0.   , 0.   , 0.001, 0.   , 0.004, 0.018,\n",
       "       0.   , 0.001, 0.   , 0.002, 0.   , 0.014, 0.   , 0.   , 0.   ,\n",
       "       0.002, 0.001, 0.   , 0.004, 0.   , 0.   , 0.003, 0.004, 0.001,\n",
       "       0.001, 0.002, 0.   , 0.   , 0.   , 0.001, 0.15 , 0.011, 0.   ,\n",
       "       0.001, 0.001, 0.004, 0.76 , 0.001, 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.001, 0.005, 0.   , 0.001], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_preds[1].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.22723024165165603,\n",
       " 'lower': -0.495917801341507,\n",
       " 'upper': 0.9503782846448191}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.compute_JS(index_A = [1, 0, 2, 55], index_B = [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02473030078269329"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_use_RD_js()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_records': 2347,\n",
       " 'n_records_R': 1382,\n",
       " 'n_records_D': 959,\n",
       " 'js': {'mean': 0.9029303002275355,\n",
       "  'lower': 0.8860208753153576,\n",
       "  'upper': 0.9198397251397135},\n",
       " 'js_R': {'mean': 0.9040589519613792,\n",
       "  'lower': 0.8913284254532201,\n",
       "  'upper': 0.9167894784695385},\n",
       " 'js_D': {'mean': 0.9081885013289568,\n",
       "  'lower': 0.8957064633162953,\n",
       "  'upper': 0.9206705393416182},\n",
       " 'js_RD': {'mean': 0.9084532652511502,\n",
       "  'lower': 0.8944668450011393,\n",
       "  'upper': 0.922439685501161},\n",
       " 'hh': {'mean': 0.4090844988822937,\n",
       "  'lower': 0.37325187809312105,\n",
       "  'upper': 0.44491703528147303},\n",
       " 'hh_R': {'mean': 0.4018021523952484,\n",
       "  'lower': 0.3672640527670878,\n",
       "  'upper': 0.43634022505230724},\n",
       " 'hh_D': {'mean': 0.35837921500205994,\n",
       "  'lower': 0.3235949378712012,\n",
       "  'upper': 0.39316348460783224}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze_subset(conditions={}, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': nan, 'lower': nan, 'upper': nan}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.n_nan_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
