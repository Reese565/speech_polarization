{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/rocassius/w266_final/scripts/assembly\")\n",
    "sys.path.append(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document import load_documents\n",
    "from constant import DOC_PRAYER_PATH, MIN_SESSION, MAX_SESSION, DOC_ALL_PATH\n",
    "from subject import subject_keywords\n",
    "\n",
    "sessions = list(range(MIN_SESSION, MAX_SESSION+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_pickled_object\n",
    "from rmn import *\n",
    "from rmn_analyzer import RMN_Analyzer\n",
    "from rmn_data_generator import RMN_DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding tools\n",
    "prayer_tools_path = \"/home/rocassius/gen-data/tools/prayer_tools\"\n",
    "metadata_dict = load_pickled_object(os.path.join(prayer_tools_path, \"metadata_dict\"))\n",
    "tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict\"))\n",
    "embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"idf_embedding_matrix\"))\n",
    "global_embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"embedding_matrix_wg\"))\n",
    "global_tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict_wg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_documents([111], DOC_PRAYER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=*= RMN Module =*=#\n",
    "#==================#\n",
    "\n",
    "# RMN Class for training Relationship Modeling Networks \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking, Reshape, Concatenate\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rmn_data_generator import RMN_DataGenerator\n",
    "from helper import pickle_object, load_pickled_object\n",
    "from vector_math import find_nn_cos\n",
    "\n",
    "# constants\n",
    "MAX_SPAN_LENGTH = 50\n",
    "NUM_TOPICS = 20\n",
    "LAMBDA = 1.0\n",
    "GAMMA = 1.0\n",
    "\n",
    "# hyperparameters\n",
    "OPTIMIZER = 'adam'\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "\n",
    "# saving tags\n",
    "RMN_TAG = \"rmn_%s\"\n",
    "MODEL = \"model.h5\"\n",
    "ATTR = \"attributes\"\n",
    "\n",
    "# attribute keys\n",
    "N_TOP_KEY = 'num_topics'\n",
    "EMBED_KEY = 'emedding_matrix'\n",
    "TOKEN_KEY = 'tokenizer_dict'\n",
    "META_KEY  = 'metadata_dict'\n",
    "DIM_KEY = 'meta_embedding_dim'\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \"\"\"\n",
    "    Class for constructing a Relationship Modeling Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # model attrbiutes\n",
    "        self.num_topics = NUM_TOPICS\n",
    "        self.embedding_matrix = None\n",
    "        self.meta_embedding_dim = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        # inference attributes\n",
    "        self.infer_embedding_matrix = None\n",
    "        self.infer_tokenizer_dict = None\n",
    "        \n",
    "        # models \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def topic_matrix(self):\n",
    "        \"\"\"Return the topic matrix associated with the rmn\"\"\"\n",
    "        # dim = [num_topics, embedding_dim]\n",
    "        return self.model.get_layer('Wd').get_weights()[0].T\n",
    "    \n",
    "    @property\n",
    "    def tuned_embedding_matrix(self):\n",
    "        \"\"\"Return the current embedding matrix of the rmn\"\"\"\n",
    "        return rmn.model.get_layer('Span.Embedding').get_weights()[0]\n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"Hinge loss function.\n",
    "        \"\"\"\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            \n",
    "            # hinge_loss\n",
    "            y_true_normalized = K.l2_normalize(y_true, axis=-1)\n",
    "            y_pred_normalized = K.l2_normalize(y_pred, axis=-1)\n",
    "            dot_product = K.sum(y_true_normalized * y_pred_normalized, axis=-1)\n",
    "            hinge_loss = K.mean(K.maximum(0., 1. - dot_product))\n",
    "\n",
    "            return hinge_loss \n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self, embedding_trainable=False, bias_reconstruct=True,\n",
    "                    gamma = 1., theta = 1., omega = 1., lamb = 1., word_dropout = 0.5):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # Span Input\n",
    "        span_input = Input(shape=(self.tokenizer_dict['max_span_length'],), \n",
    "                           name='Span.Input')\n",
    "        span_embedding = Embedding(input_dim=len(self.tokenizer_dict['word_index']) + 1, \n",
    "                                   output_dim=self.embedding_dim, \n",
    "                                   weights=[self.embedding_matrix],\n",
    "                                   input_length=self.tokenizer_dict['max_span_length'],\n",
    "                                   trainable=embedding_trainable, \n",
    "                                   name = 'Span.Embedding')(span_input)\n",
    "        \n",
    "        # Mask for randomly dropping words\n",
    "        dropout_mask = K.stack(\n",
    "            [K.random_binomial((span_embedding.shape[1],), p=word_dropout)]*span_embedding.shape[2], axis=1)\n",
    "        # Average over the remaining words\n",
    "        span_avg = Lambda(lambda x: K.mean(x * K.expand_dims(dropout_mask, axis=0), axis=1), \n",
    "                          name = \"Span.Avg.Layer\")(span_embedding)\n",
    "\n",
    "        input_layers = [span_input]\n",
    "        embedding_layers = [span_avg]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.meta_embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.meta_embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concatenate span vector with metadata embeddings\n",
    "        _ht = Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer whose output is a probability distribution\n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\",\n",
    "                   activity_regularizer = Purity(gamma, theta, omega),\n",
    "                   name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   use_bias = bias_reconstruct,\n",
    "                   kernel_regularizer = Orthogonality(lamb),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        #model.compile(optimizer = OPTIMIZER, loss='mean_squared_error')\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss())\n",
    "        self.model = model\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def set_topic_vectors(self, words):\n",
    "        \"\"\"Set the topic vectors with vectors corresponding to the given words\n",
    "        \"\"\"\n",
    "        # get the word ids\n",
    "        word_ids = self.tokenizer_dict['tokenize_pad'](words)[:,0]\n",
    "        \n",
    "        # replicate associated weights up to num_topics\n",
    "        weights = np.tile(self.embedding_matrix[word_ids], \n",
    "                          (-(self.num_topics // -len(words)),1))[:self.num_topics]\n",
    "        \n",
    "        # set weights layer weights\n",
    "        r = self.model.get_layer(\"R\")\n",
    "        if len(r.get_weights()) == 1:\n",
    "            r.set_weights([weights])\n",
    "        else:\n",
    "            r.set_weights([weights, r.get_weights()[1]])\n",
    "        \n",
    "        \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "          \n",
    "    \n",
    "    def prep_spans(self, documents):\n",
    "        \"\"\"Returns the lists of word ids associated with the text\n",
    "        \"\"\"\n",
    "        return self.tokenizer_dict['tokenize_pad'](documents)\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_x = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                      for col in self.metadata_dict.keys()]\n",
    "\n",
    "        return metadata_x\n",
    "        \n",
    "    \n",
    "    def prep_X(self, df, for_training=False):\n",
    "        \"\"\"Preps metadata and spans for training or prediction\n",
    "        \"\"\"\n",
    "        spans_y = self.prep_spans(df['document'])\n",
    "        metadata_x = self.prep_metadata(df)\n",
    "        X = [spans_y] + metadata_x\n",
    "        \n",
    "        if for_training:\n",
    "            y = self.embedding_matrix[spans_y].mean(axis=1)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def predict_y(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the rmn outputs for a df\n",
    "        \"\"\"\n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.model)\n",
    "        else:\n",
    "            return self.predict_(df, self.model)\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"        \n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.topic_model)\n",
    "        else:\n",
    "            return self.predict_(df, self.topic_model)\n",
    "\n",
    "        \n",
    "    def predict_(self, df, model):\n",
    "        \"\"\"Makes a predictions for a df with a model\n",
    "        \"\"\"\n",
    "        return model.predict(x=self.prep_X(df))\n",
    "        \n",
    "    \n",
    "    def predict_with_generator(self, df, model):\n",
    "        \"\"\"Predict topic distributions with a generator\n",
    "        \"\"\"\n",
    "        # Make sure data is not empty\n",
    "        assert not df.empty\n",
    "\n",
    "        # Calculate good batch size, \n",
    "        batch_size = max(1, min(10000, df.shape[0] // 10))\n",
    "        n_batches = df.shape[0] // batch_size\n",
    "\n",
    "        if n_batches < 2: \n",
    "            return self.predict_(df, model)\n",
    "        else:\n",
    "            # calculate remainder batch size\n",
    "            r = df.shape[0] % batch_size\n",
    "            if r == 0:\n",
    "                g_index = df.index[:-batch_size]\n",
    "                r_index = df.index[-batch_size:]\n",
    "            else:\n",
    "                g_index = df.index[:-r]\n",
    "                r_index = df.index[-r:]\n",
    "\n",
    "            # Make generator\n",
    "            g = RMN_DataGenerator(self, df.loc[g_index], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Predict on remainder batch\n",
    "            r_pred = self.predict_(df.loc[r_index], model)\n",
    "            # predict on generated batches\n",
    "            g_pred = model.predict_generator(g, use_multiprocessing=True, workers=10, verbose=1)\n",
    "\n",
    "            assert r_pred.shape[1] == g_pred.shape[1]\n",
    "            pred = np.vstack([g_pred, r_pred])\n",
    "\n",
    "            return pred\n",
    "        \n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Save the model's weights, architecture and attributes\n",
    "        \"\"\"\n",
    "        # assemble attribute dictionary\n",
    "        attribute_dict = {\n",
    "            N_TOP_KEY:  self.num_topics,\n",
    "            EMBED_KEY:  self.embedding_matrix,\n",
    "            TOKEN_KEY:  self.tokenizer_dict,\n",
    "            META_KEY:   self.metadata_dict, \n",
    "            DIM_KEY:    self.meta_embedding_dim}\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        # save model weights\n",
    "        self.model.save_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # save model attributes\n",
    "        pickle_object(attribute_dict, os.path.join(model_path, ATTR))\n",
    "        \n",
    "        \n",
    "    def load_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Load the model, weights, architecture and attributes from a saved model\n",
    "        \"\"\"\n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        \n",
    "        # load attributes\n",
    "        attributes_dict = load_pickled_object(os.path.join(model_path, ATTR))\n",
    "        \n",
    "        # update attributes\n",
    "        self.num_topics         = attributes_dict[N_TOP_KEY]\n",
    "        self.embedding_matrix   = attributes_dict[EMBED_KEY]\n",
    "        self.tokenizer_dict     = attributes_dict[TOKEN_KEY]\n",
    "        self.metadata_dict      = attributes_dict[META_KEY]\n",
    "        self.meta_embedding_dim = attributes_dict[DIM_KEY] \n",
    "        \n",
    "        # construct identical model architecture\n",
    "        self.build_model()\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def inspect_topics(self, which_topics='all', k_neighbors=10):\n",
    "        \"\"\"\n",
    "        Ouput the nearest neighbors of every topic vector in\n",
    "        the model's topic layer\n",
    "        \"\"\"\n",
    "        if which_topics == 'all':\n",
    "            which_topics = range(self.num_topics) \n",
    "        \n",
    "        if (self.infer_embedding_matrix is None or \n",
    "            self.infer_tokenizer_dict is None):\n",
    "            self.infer_embedding_matrix = self.embedding_matrix\n",
    "            self.infer_tokenizer_dict = self.tokenizer_dict\n",
    "        \n",
    "        E = self.infer_embedding_matrix # dim = [vocab_size, embedding_dim]\n",
    "        Wd = self.topic_matrix          # dim = [num_topics, embedding_dim]\n",
    "        \n",
    "        for i in which_topics:\n",
    "            # find nearest neighbors to topic\n",
    "            neighbors, sim = find_nn_cos(Wd[i], E, k_neighbors)\n",
    "            words = [self.infer_tokenizer_dict['tokenizer'].index_word[v] for v in neighbors]\n",
    "            print(20*\"=\" +\"\\n\")\n",
    "            print(\"Topic\", i)\n",
    "            print(words)\n",
    "    \n",
    "    \n",
    "# Orthogonality Regularizer #\n",
    "\n",
    "class Orthogonality(Regularizer):\n",
    "    \"\"\"\n",
    "    Regularizer for penalizing non-orthogonal components of a weight matrix.\n",
    "    \n",
    "    Args:\n",
    "    - lamb: (Float) regularization penalty weight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lamb = 1.):\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def __call__(self, R):\n",
    "        \"\"\"Returns a component dependence penalty for matrix R\n",
    "        \"\"\"\n",
    "        RRT = K.dot(R, K.transpose(R))\n",
    "        I = K.eye(RRT.shape.as_list()[0])\n",
    "        penalty = self.lamb * K.sqrt(K.sum(K.square(RRT - I)))\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    \n",
    "# Topic Purity Regularizer #\n",
    "\n",
    "class Purity(Regularizer):\n",
    "    \"\"\"Regularizer for penalizing highly impure probability distributions\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma = 1., theta = 1., omega = 1., entr=True):\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.omega = omega\n",
    "        self.entr = entr\n",
    "\n",
    "    def __call__(self, p):\n",
    "        \"\"\"Returns the avergage shannon entropy of the distribution(s) p\n",
    "        \"\"\"\n",
    "        # calculate impurity\n",
    "        if self.entr:\n",
    "            impurity = K.sum(p*-K.log(p)/K.log(K.constant(2)), axis=-1)\n",
    "        else: \n",
    "            # Gini impurity    \n",
    "            impurity = 1.-K.sum(p*(1-p), axis=-1)\n",
    "        concentration = K.max(p, axis=-1)\n",
    "        \n",
    "        # calculate batch similarity\n",
    "        ppt = K.dot(p, K.transpose(p)) \n",
    "        similarity = K.mean(K.sum(ppt) - K.sum(tf.linalg.diag_part(ppt)))\n",
    "        \n",
    "        \n",
    "        penalty = (self.gamma * K.mean(impurity) + \n",
    "                   self.theta * K.mean(concentration) + \n",
    "                   self.omega * similarity)\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    \n",
    "    \n",
    "# RMN child for training on Data Generators #\n",
    "\n",
    "class RigidRMN(RMN):\n",
    "    \"\"\"\n",
    "    A Derivative of the RMN class for training with a generator\n",
    "    \n",
    "    This version does not use an embedding for the span input\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        RMN.__init__(self)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def build_model(self, embedding_trainable=False, bias_reconstruct=True,\n",
    "                    gamma = 1., theta = 1., omega = 1., lamb = 1.):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # Span Input\n",
    "        span_input = Input(shape=(self.embedding_dim,), name='Span.Input')\n",
    "\n",
    "        input_layers = [span_input]\n",
    "        embedding_layers = [span_input]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.meta_embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.meta_embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concatenate span vector with metadata embeddings\n",
    "        _ht = Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer whose output is a probability distribution\n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\",\n",
    "                   activity_regularizer = Purity(gamma, theta, omega),\n",
    "                   name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   use_bias = bias_reconstruct,\n",
    "                   kernel_regularizer = Orthogonality(lamb),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        model.compile(optimizer = OPTIMIZER, loss='mean_squared_error')\n",
    "        #model.compile(optimizer = OPTIMIZER, loss = self.model_loss())\n",
    "        self.model = model\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    def prep_spans(self, documents, for_training):\n",
    "        \"\"\"Returns the lists of word ids associated with the text\n",
    "        \"\"\"\n",
    "        spans_y = self.tokenizer_dict['tokenize_pad'](documents)\n",
    "        if for_training:\n",
    "            spans_y = spans_y * np.random.binomial(1, self.dropout, spans_y.shape)\n",
    "            \n",
    "        y = self.embedding_matrix[spans_y].mean(axis=1)\n",
    "        #y = y / np.linalg.norm(y, axis=-1)[:, np.newaxis]\n",
    "        \n",
    "        return y.astype(np.float16)\n",
    "        \n",
    "    \n",
    "    def prep_X(self, df, for_training=False):\n",
    "        \"\"\"Preps metadata and spans for training or prediction\n",
    "        \"\"\"\n",
    "        vectors_y = self.prep_spans(df['document'], for_training)\n",
    "        metadata_x = self.prep_metadata(df)\n",
    "        X = [vectors_y] + metadata_x\n",
    "        \n",
    "        if for_training:\n",
    "            return X, X[0]\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RigidRMN(dropout=0.5)\n",
    "rmn.embedding_matrix = embedding_matrix\n",
    "rmn.tokenizer_dict = tokenizer_dict\n",
    "rmn.metadata_dict = metadata_dict\n",
    "rmn.infer_embedding_matrix = global_embedding_matrix\n",
    "rmn.infer_tokenizer_dict = global_tokenizer_dict\n",
    "rmn.meta_embedding_dim = 25\n",
    "rmn.num_topics = 50\n",
    "rmn.build_model(gamma=0.1, theta=0., omega=.01, lamb=0.001, bias_reconstruct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = RMN_DataGenerator(rmn=rmn, data_df=df, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "362/362 [==============================] - 9s 26ms/step - loss: 0.3907\n",
      "Epoch 2/5\n",
      "362/362 [==============================] - 5s 15ms/step - loss: 0.3652\n",
      "Epoch 3/5\n",
      "362/362 [==============================] - 5s 15ms/step - loss: 0.3618\n",
      "Epoch 4/5\n",
      "362/362 [==============================] - 5s 15ms/step - loss: 0.3594\n",
      "Epoch 5/5\n",
      "362/362 [==============================] - 6s 15ms/step - loss: 0.3554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efbbc7f5090>"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.model.fit_generator(data_generator, epochs = 5, use_multiprocessing=True, workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================#\n",
    "#=*= RMN Analyzer =*=#\n",
    "#====================#\n",
    "\n",
    "# Class for analyzing an RMN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from analysis import *\n",
    "\n",
    "# variable constants\n",
    "SUB = 'subject'\n",
    "SPEAK = 'speakerid'\n",
    "PARTY = 'party'\n",
    "SESS = 'session'\n",
    "# party constants\n",
    "R = 'R'\n",
    "D = 'D'\n",
    "# metric constants\n",
    "JS = 'js'\n",
    "HH = 'hh'\n",
    "N_REC = 'n_records'\n",
    "\n",
    "\n",
    "class RMN_Analyzer(object):\n",
    "    \"\"\"Class for Analyzing an RMN with respect to a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rmn, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - rmn: (RMN) the RMN to be used for analysis\n",
    "        - df : (DataFrame) the dataframe to analyze\n",
    "        \"\"\"\n",
    "        \n",
    "        'Initialization'\n",
    "        self.rmn = rmn\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.topic_preds = None\n",
    "        self.y_preds = None\n",
    "        \n",
    "    @property\n",
    "    def index(self):\n",
    "        return self.df.index\n",
    "         \n",
    "        \n",
    "    def predict_topics(self, use_generator=True):\n",
    "        \"\"\"Computes the topic predictions for all observations\n",
    "        \"\"\"\n",
    "        self.topic_preds = self.rmn.predict_topics(self.df, use_generator)\n",
    "        \n",
    "    \n",
    "    def predict_y(self, use_generator=True):\n",
    "        \"\"\"Computes the sentence vector predictions for all observations\n",
    "        \"\"\"\n",
    "        self.y_preds = self.rmn.predict_y(self.df, use_generator)\n",
    "        \n",
    "        \n",
    "    def sample_indices(self, indices, n):\n",
    "        \"\"\"Returns a SRR of the indices provided\n",
    "        \"\"\"\n",
    "        return np.random.choice(indices, n, replace=True)\n",
    "\n",
    "    \n",
    "    def bool_subset(self, col, value):\n",
    "        \"\"\"\n",
    "        Returns a boolean vector for each observation in the\n",
    "        dataframe indicating whether it meets the col == value condition\n",
    "        \"\"\"\n",
    "        assert col in self.df.columns\n",
    "        return self.df[col] == value\n",
    "    \n",
    "    \n",
    "    def bool_index(self, conditions):\n",
    "        \"\"\"\n",
    "        Returns a boolean vector for each observation in the\n",
    "        dataframe indicating whether it meets all conditions\n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        \n",
    "        Returns: \n",
    "        - pandas series of booleans indicating where all \n",
    "          of the conditions hold\n",
    "        \"\"\"\n",
    "        # initialize bool index\n",
    "        bool_index = (pd.Series(True)\n",
    "                      .repeat(self.index.shape[0])\n",
    "                      .reset_index(drop=True))\n",
    "        \n",
    "        for col, val in conditions.items():\n",
    "            bool_index = bool_index & self.bool_subset(col, val)\n",
    "            \n",
    "        return bool_index\n",
    "    \n",
    "    \n",
    "    def cond_index(self, conditions):\n",
    "        \"\"\"Returns indices of records meeting the conditions\n",
    "        \"\"\"\n",
    "        return self.index[self.bool_index(conditions)]\n",
    "    \n",
    "    \n",
    "    def n_records(self, conditions={}):\n",
    "        \"\"\"Returns the number of records meetings the conditions\n",
    "        \"\"\"\n",
    "        return len(self.cond_index(conditions))\n",
    "    \n",
    "    \n",
    "    def compute_JS(self, index_A, index_B, base=2):\n",
    "        \"\"\"\n",
    "        Computes the mean pair-wise JS divergence and associated CI\n",
    "        between indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p_A = self.topic_preds[index_A]\n",
    "        p_B = self.topic_preds[index_B]\n",
    "        js_list = [jensenshannon(p, q, base) for p, q in zip(p_A, p_B)]\n",
    "        \n",
    "        return mean_CI(js_list)\n",
    "        \n",
    "        \n",
    "    def compute_HH(self, index):\n",
    "        \"\"\"\n",
    "        Computes the mean HH index and associated CI between\n",
    "        indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p = self.topic_preds[index]\n",
    "        hh_list = [hh_index(q) for q in p]\n",
    "        \n",
    "        return mean_CI(hh_list)\n",
    "          \n",
    "    \n",
    "    def inter_party_js(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns the estimated inter party JS divergence and a CI.\n",
    "        \n",
    "        Computes the inter party JS divergence between \n",
    "        Republicans and Democrats on a given subject\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find R and D indicies on the subject\n",
    "        index_R = self.cond_index({**conditions, **{PARTY: R}})\n",
    "        index_D = self.cond_index({**conditions, **{PARTY: D}})\n",
    "        \n",
    "        # return None if indices are insufficient\n",
    "        if len(index_R)==0 or len(index_D)==0:\n",
    "            return None\n",
    "        \n",
    "        # sample \n",
    "        samp_index_R = self.sample_indices(index_R, n)\n",
    "        samp_index_D = self.sample_indices(index_D, n)\n",
    "    \n",
    "        return self.compute_JS(samp_index_R, samp_index_D)\n",
    "    \n",
    "    \n",
    "    def group_js(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns the estimated mean JS divergence and a CI\n",
    "        \n",
    "        Estimates the average JS divergence between any two documents of\n",
    "        a group defined by the conditions. A document by speaker _i_ is \n",
    "        never compared to another document by speaker _i_.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        - n         : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find indicies of party on the subject\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        \n",
    "        # Return none if there are fewer than 2 speakers\n",
    "        if self.df.loc[cond_index][SPEAK].nunique() < 2:\n",
    "            return None\n",
    "        \n",
    "        # Sample index pairs\n",
    "        index_AB = []\n",
    "        while len(index_AB) < n:\n",
    "            a_b = self.sample_indices(cond_index, n=2)\n",
    "            # include samples whose speakers are different\n",
    "            if self.df.loc[a_b][SPEAK].nunique() == 2:\n",
    "                index_AB.append(a_b)\n",
    "        \n",
    "        index_AB = np.asarray(index_AB)\n",
    "        assert index_AB.shape == (n, 2)\n",
    "        \n",
    "        # get indices for each group\n",
    "        index_A, index_B = index_AB[:,0], index_AB[:,1]\n",
    "        \n",
    "        return self.compute_JS(index_A, index_B)\n",
    "    \n",
    "    \n",
    "    def group_hh(self, conditions={}, n=None):\n",
    "        \"\"\"\n",
    "        Returns the estimated mean HH index and a CI\n",
    "        \n",
    "        Estimates the average Herfindahl–Hirschman Index \n",
    "        of all records meetings the conditons.\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - party  : (str) party of interest\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean index point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # indicies meeting the conditions\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        \n",
    "        # return None if indices are insufficient\n",
    "        if len(cond_index)==0:\n",
    "            return None\n",
    "        \n",
    "        if n is None:\n",
    "            return self.compute_HH(cond_index)\n",
    "        else:\n",
    "            samp_index = self.sample_indices(cond_index, n)\n",
    "            return self.compute_HH(samp_index)\n",
    "        \n",
    "        \n",
    "    def analyze_subset(self, conditions, n):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of analysis metrics for the subset \n",
    "        of records defined by the conditions.\n",
    "        \n",
    "        Note: It is recommended conditions be on subject\n",
    "        \n",
    "        Args:\n",
    "        - conditions: (dict) dictionary of conditions\n",
    "        - n         : (int) sample size for estimation of metrics\n",
    "        \n",
    "        for the entire dataset and for each subject the following are computed:\n",
    "        - n_records, n_records_R\n",
    "        - n_records_D\n",
    "        - js\n",
    "        - js_R\n",
    "        - js_D\n",
    "        - js_RD\n",
    "        - hh\n",
    "        - hh_R\n",
    "        - hh_D\n",
    "        \n",
    "        Returns: a dictionary of metrics\n",
    "        \"\"\"\n",
    "        # R and D added conditions\n",
    "        conditions_R = {**conditions, **{PARTY: R}}\n",
    "        conditions_D = {**conditions, **{PARTY: D}}\n",
    "        \n",
    "        # annotation tags\n",
    "        _R = '_' + R\n",
    "        _D = '_' + D\n",
    "        _RD = _R + D\n",
    "        \n",
    "        metrics = {\n",
    "            # n record data\n",
    "            N_REC:    self.n_records(conditions),\n",
    "            N_REC+_R: self.n_records(conditions_R),\n",
    "            N_REC+_D: self.n_records(conditions_D),\n",
    "            # JS divergence data\n",
    "            JS:     self.group_js(conditions, n),\n",
    "            JS+_R:  self.group_js(conditions_R, n),\n",
    "            JS+_D:  self.group_js(conditions_D, n),\n",
    "            JS+_RD: self.inter_party_js(conditions, n),\n",
    "            # HH index data\n",
    "            HH:    self.group_hh(conditions, n),\n",
    "            HH+_R: self.group_hh(conditions_R, n),\n",
    "            HH+_D: self.group_hh(conditions_D, n)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "        \n",
    "    def analyze(self, subjects, n):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of analysis metrics at the subject level\n",
    "        and at the session level (assuming self.df is the data of a\n",
    "        single session).\n",
    "        \n",
    "        Args:\n",
    "        - subjects: (array-like) list of subjects\n",
    "        - n       : (int) sample size for estimation of metrics\n",
    "        \n",
    "        Returns: a dictionary of metrics\n",
    "        \"\"\"\n",
    "        # analyze entire session dataset\n",
    "        dataset_metrics = self.analyze_subset(conditions={}, n=n)\n",
    "        \n",
    "        # analyze by subject\n",
    "        subject_metrics = {}\n",
    "        for s in subjects:\n",
    "            subject_metrics[s] = self.analyze_subset({SUB: s}, n)\n",
    "        \n",
    "        metrics = {'dataset' : dataset_metrics, \n",
    "                   'subjects': subject_metrics}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def shannon_entropy(self):\n",
    "        \"\"\"Returns the Shannon Entropy of every topic prediction\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        return shannon_entropy(self.topic_preds)\n",
    "    \n",
    "    def average_topic_max(self, conditions={}):\n",
    "        \"\"\"Return the average of the maximum probability assigned to a topic\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        avg_topic_max = np.max(self.topic_preds[cond_index], axis=-1).mean().round(3)\n",
    "        \n",
    "        return avg_topic_max\n",
    "    \n",
    "    \n",
    "    def first_topic_counts(self, conditions={}):\n",
    "        \"\"\"\n",
    "        Returns a leaderboard of topics and how many times they \n",
    "        are the primary topic associated with a document.\n",
    "        \"\"\"\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "           \n",
    "        cond_index = self.cond_index(conditions)\n",
    "        topic_counts = pd.Series(np.argmax(self.topic_preds[cond_index], axis=-1)).value_counts()\n",
    "        \n",
    "        return topic_counts\n",
    "    \n",
    "    \n",
    "    def topic_use(self, conditions={}):\n",
    "        \"\"\"\n",
    "        Returns a leaderboard of topics based on the percentage of \n",
    "        total weight given to them in all of the documents\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        topic_sums = pd.Series(np.sum(self.topic_preds[cond_index], axis=0))\n",
    "        topic_use = topic_sums.sort_values(ascending=False) / topic_sums.sum()\n",
    "        \n",
    "        return topic_use\n",
    "    \n",
    "    \n",
    "    def primary_topics(self, conditions={}, k=5):\n",
    "        \"\"\"Returns top k most prominent topics for documents\n",
    "        \"\"\"\n",
    "        cond_index = self.cond_index(conditions)\n",
    "        primary_topics = np.flip(np.argsort(self.topic_preds[cond_index]))[:,:k]\n",
    "        \n",
    "        return primary_topics\n",
    "    \n",
    "    \n",
    "#     def topic_stats(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = RMN_Analyzer(rmn, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 223ms/step\n"
     ]
    }
   ],
   "source": [
    "analyzer.predict_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.208"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.shannon_entropy().mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.average_topic_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    0.046295\n",
       "24    0.041911\n",
       "5     0.036861\n",
       "4     0.035688\n",
       "11    0.035676\n",
       "39    0.035258\n",
       "9     0.034875\n",
       "47    0.027730\n",
       "46    0.027399\n",
       "32    0.027045\n",
       "43    0.026586\n",
       "7     0.025356\n",
       "18    0.024735\n",
       "34    0.024522\n",
       "19    0.023531\n",
       "28    0.023268\n",
       "42    0.022963\n",
       "13    0.021544\n",
       "3     0.019678\n",
       "1     0.019516\n",
       "10    0.019314\n",
       "21    0.018854\n",
       "35    0.018816\n",
       "26    0.018396\n",
       "37    0.017708\n",
       "12    0.017573\n",
       "20    0.016800\n",
       "22    0.016673\n",
       "8     0.016552\n",
       "36    0.015538\n",
       "23    0.015496\n",
       "49    0.015483\n",
       "33    0.015425\n",
       "44    0.014418\n",
       "29    0.014133\n",
       "17    0.014035\n",
       "0     0.013475\n",
       "40    0.013022\n",
       "27    0.012656\n",
       "25    0.012477\n",
       "48    0.012277\n",
       "15    0.012216\n",
       "14    0.011710\n",
       "2     0.010991\n",
       "41    0.010702\n",
       "30    0.009904\n",
       "45    0.009273\n",
       "6     0.008881\n",
       "16    0.008455\n",
       "31    0.008309\n",
       "dtype: float32"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    4686\n",
       "24    4150\n",
       "4     3963\n",
       "5     3484\n",
       "9     3468\n",
       "11    3451\n",
       "39    3388\n",
       "43    2850\n",
       "46    2631\n",
       "47    2549\n",
       "32    2524\n",
       "7     2469\n",
       "34    2337\n",
       "18    2190\n",
       "28    2157\n",
       "42    2104\n",
       "19    2074\n",
       "13    2044\n",
       "10    1888\n",
       "21    1777\n",
       "3     1684\n",
       "12    1650\n",
       "8     1643\n",
       "26    1617\n",
       "22    1598\n",
       "1     1585\n",
       "37    1580\n",
       "20    1571\n",
       "35    1557\n",
       "49    1449\n",
       "44    1426\n",
       "36    1406\n",
       "23    1253\n",
       "29    1211\n",
       "40    1204\n",
       "17    1187\n",
       "33    1183\n",
       "2     1122\n",
       "6     1074\n",
       "27    1070\n",
       "0     1053\n",
       "25    1002\n",
       "15     986\n",
       "30     961\n",
       "48     956\n",
       "45     773\n",
       "41     769\n",
       "14     723\n",
       "31     705\n",
       "16     572\n",
       "dtype: int64"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.first_topic_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n",
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "\n",
      "Topic 38\n",
      "['shoh', 'bucks', 'siders', 'keh', 'xxxx', 'niner', 'decent', 'rakh', 'eighty', 'ensnares']\n",
      "====================\n",
      "\n",
      "Topic 24\n",
      "['revues', 'musicals', 'minireviews', 'marcoussis', 'mcquarrie', 'isherwood', 'discounts', 'lome', 'seventeen', 'songwriters']\n",
      "====================\n",
      "\n",
      "Topic 4\n",
      "['furled', 'extricate', 'flier', 'floured', 'housekeeping', 'sabbatical', 'cushy', 'advantaged', 'reins', 'entangled']\n",
      "====================\n",
      "\n",
      "Topic 5\n",
      "['meed', 'arcadius', '1347', 'honorius', 'frontpage', 'syrmia', 'malm', 'inds', 'focaccia', 'tigranes']\n",
      "====================\n",
      "\n",
      "Topic 9\n",
      "['algeria', 'iceland', 'kayseri', 'js', 'kemerovo', 'galicia', 'lavezzi', 'kantō', 'siberia', 'hamit']\n",
      "====================\n",
      "\n",
      "Topic 11\n",
      "['equaling', 'cheesecloth', 'maghreb', 'cuga', 'therien', 'hons', 'eccellenza', 'kumaun', 'newcourt', 'dawat']\n",
      "====================\n",
      "\n",
      "Topic 39\n",
      "['jooss', 'thistlethwaite', 'stradlin', 'chawda', 'higelin', 'skempton', 'barkleys', 'bhattacharyya', 'rasi', 'voros']\n",
      "====================\n",
      "\n",
      "Topic 43\n",
      "['muentefering', 'companhia', 'classification', 'shutouts', 'unicellular', 'ilves', 'dismal', 'morganatic', 'chengjiang', 'lowly']\n",
      "====================\n",
      "\n",
      "Topic 46\n",
      "['1965', '1968', 'carpenter', '1973', '1977', 'animation', '1969', '1960', '1976', '1971']\n",
      "====================\n",
      "\n",
      "Topic 47\n",
      "['exhausted', 'lighten', 'sider', 'desperately', 'heft', 'urgently', 'hampers', 'wholeheartedly', 'needed', 'unbalanced']\n",
      "====================\n",
      "\n",
      "Topic 32\n",
      "['mutambara', 'espen', 'lichen', 'bantu', 'ehrlichman', 'fsu', 'warrick', 'feith', 'macculloch', 'yarbrough']\n",
      "====================\n",
      "\n",
      "Topic 7\n",
      "['bavasi', 'praetor', 'mhk', 'doukas', 'dimitroff', 'coadjutor', 'administrator', 'quaestor', 'ghaffar', 'headmaster']\n",
      "====================\n",
      "\n",
      "Topic 34\n",
      "['whio', 'girded', 'ilion', 'bisco', 'affordably', 'letterboxed', 'unitedly', 'belgraders', 'goede', 'electrochemically']\n",
      "====================\n",
      "\n",
      "Topic 18\n",
      "['numberings', 'banyamurenge', 'tarok', 'ampelographers', 'tequilas', 'lounes', 'borgs', 'parabellum', 'mazzanti', 'financers']\n",
      "====================\n",
      "\n",
      "Topic 28\n",
      "['athor', 'miette', 'saitek', 'vanaja', 'hoofers', 'conciliators', 'millepied', 'onlys', 'frenchwomen', 'peple']\n",
      "====================\n",
      "\n",
      "Topic 42\n",
      "['multi', 'yuan', 'totaled', 'isl', 'invested', 'paperless', 'tdc', 'mainland', '€', 'endeavor']\n",
      "====================\n",
      "\n",
      "Topic 19\n",
      "['ender', 'caput', 'lund', 'whisenhunt', 'jimbo', 'eastenders', 'dobbs', 'alt', 'plumber', 'rusa']\n",
      "====================\n",
      "\n",
      "Topic 13\n",
      "['concur', 'addendum', 'resemblances', 'suture', 'understandings', 'sympathize', 'opine', 'disagree', 'washy', 'affinities']\n",
      "====================\n",
      "\n",
      "Topic 10\n",
      "['duckie', 'jordin', 'smashbox', 'wilko', 'kyla', 'jonny', 'onta', 'stromile', 'tchad', 'macklemore']\n",
      "====================\n",
      "\n",
      "Topic 21\n",
      "['unbalanced', 'powerful', 'strongest', 'astute', 'thrust', 'unfavorable', 'tilted', 'forceful', 'pseudoprime', 'hostile']\n",
      "====================\n",
      "\n",
      "Topic 3\n",
      "['decomposition', 'convolution', 'brasileiras', 'corresponded', 'alberich', 'unbounded', 'generalized', 'legolas', 'binomial', 'integers']\n",
      "====================\n",
      "\n",
      "Topic 12\n",
      "['skvortsov', 'retinas', 'contorted', 'martials', '20n', 'ousters', 'roki', 'kith', 'interlocked', 'benmerzouga']\n",
      "====================\n",
      "\n",
      "Topic 8\n",
      "['vcu', 'gumshoes', 'trunklines', 'comptrollers', 'withlacoochee', 'patrolmen', 'antigambling', 'highpoints', 'csuf', 'rockhurst']\n",
      "====================\n",
      "\n",
      "Topic 26\n",
      "['bloodiest', 'solider', 'wwi', 'wwii', 'deadliest', 'ww2', 'holdover', 'airstrike', 'subgiant', 'war']\n",
      "====================\n",
      "\n",
      "Topic 22\n",
      "['orkney', 'fergus', 'confessor', 'aleutian', 'kartli', 'thessaly', 'frobisher', 'shetland', 'siberia', 'epirus']\n",
      "====================\n",
      "\n",
      "Topic 1\n",
      "['kasprowicz', 'stebic', 'florenzie', 'damien', 'braised', 'bichel', 'catching', 'kamande', 'abdelraziq', 'depante']\n",
      "====================\n",
      "\n",
      "Topic 37\n",
      "['sarang', 'targum', 'senan', 'cytokines', 'mannie', 'whk', 'césaire', 'santomero', 'moishe', 'marcelli']\n",
      "====================\n",
      "\n",
      "Topic 20\n",
      "['corporate', 'holder', 'taxpayer', 'record', 'approval', 'finances', 'taxes', 'approvals', 'revenues', 'thanks']\n",
      "====================\n",
      "\n",
      "Topic 35\n",
      "['valvoline', 'amd', 'merck', 'grönholm', 'cuaron', 'distributors', '232nd', 'refocused', 'picabo', 'intel']\n",
      "====================\n",
      "\n",
      "Topic 49\n",
      "['resigns', 'humming', 'coxnet', 'streeter', 'stocking', 'kretzmer', 'siniscalco', 'schwed', 'alogoskoufis', 'adisai']\n",
      "====================\n",
      "\n",
      "Topic 44\n",
      "['brcic', 'grudin', 'meixler', 'mutler', 'diels', 'okamba', 'efty', 'abcarian', 'babington', 'ulbrich']\n",
      "====================\n",
      "\n",
      "Topic 36\n",
      "['umlaut', 'quinine', 'afterburners', 'zsolt', '1184', 'lászló', 'forename', 'andrás', 'lajos', 'berenger']\n",
      "====================\n",
      "\n",
      "Topic 23\n",
      "['klinsmann', 'sonne', 'twenty20s', 'matthaus', 'midweek', 'farewells', 'combattante', 'evolutions', 'englische', 'superfriends']\n",
      "====================\n",
      "\n",
      "Topic 29\n",
      "['omniscience', 'malleability', 'genuineness', 'rejoices', 'tig', 'omnipresence', 'ensminger', 'tarnum', 'hōnen', 'transamerican']\n",
      "====================\n",
      "\n",
      "Topic 40\n",
      "['witchel', 'krischer', 'gleadless', 'pleasanton', '81501', 'dieguito', 'chatard', 'enes', 'quijano', 'atoyac']\n",
      "====================\n",
      "\n",
      "Topic 17\n",
      "['acquits', 'brumidi', 'bennigan', 'jarmyn', 'remands', 'letta', 'declarant', 'morandi', 'megaupload', 'dweller']\n",
      "====================\n",
      "\n",
      "Topic 33\n",
      "['undergo', 'offal', 'pet', 'castration', 'radiological', 'microbiological', 'neutered', 'quarantine', 'halal', 'prostate']\n",
      "====================\n",
      "\n",
      "Topic 2\n",
      "['bisu', 'umbanda', 'cefn', 'utsa', 'romário', 'kalim', 'anguo', 'pauw', 'veronese', 'ncmi']\n",
      "====================\n",
      "\n",
      "Topic 6\n",
      "['roehrkasse', 'macaskill', 'digambar', 'colnbrook', 'sabarimala', 'mobbs', 'muttawa', 'kitzmiller', 'arpeggio', 'varaha']\n",
      "====================\n",
      "\n",
      "Topic 27\n",
      "['thrashing', 'matutes', 'universalist', 'avvenire', 'oblate', 'ethicist', 'orbifold', 'synodical', 'arnaut', 'anticommunist']\n",
      "====================\n",
      "\n",
      "Topic 0\n",
      "['sayan', 'hillel', 'skullcap', 'merc', 'powdery', 'vest', 'scent', 'ueli', 'stocking', 'belt']\n",
      "====================\n",
      "\n",
      "Topic 25\n",
      "['oriole', 'phoenix', 'cal', 'prosecutorial', 'oakland', 'seattle', \"o'neil\", 'leadoff', 'sacramento', 'tna']\n",
      "====================\n",
      "\n",
      "Topic 15\n",
      "['wiltern', 'misano', 'cottesloe', 'hautacam', 'mongie', '9pm', 'chitralada', 'truk', 'palais', 'amaroo']\n",
      "====================\n",
      "\n",
      "Topic 30\n",
      "['psychologists', 'howler', 'evolutionary', 'paraphyletic', 'monophyletic', 'chimpanzees', 'alacrity', 'lemurs', 'primates', 'biologists']\n",
      "====================\n",
      "\n",
      "Topic 48\n",
      "['caned', 'punished', 'cremate', 'flogged', 'geoghan', 'defrocked', 'antiquities', 'bibles', 'shahidul', 'haslett']\n",
      "====================\n",
      "\n",
      "Topic 45\n",
      "['hyneman', 'decore', 'packie', 'simper', 'sprunt', 'sachse', 'mcaloon', 'duprez', 'sackman', 'spectating']\n",
      "====================\n",
      "\n",
      "Topic 41\n",
      "['swoons', 'outcry', 'balakrishnan', 'marquardt', 'alberoni', 'wranglings', 'wrangles', 'speedily', 'outcries', 'cahn']\n",
      "====================\n",
      "\n",
      "Topic 14\n",
      "['superbrawl', 'invalidated', 'indult', 'validated', 'einseln', 'winkelman', 'antisemitic', 'inartful', 'ungenerous', 'uncanonical']\n",
      "====================\n",
      "\n",
      "Topic 31\n",
      "['amparo', 'martino', 'corsini', 'meisner', 'caiaphas', 'montagna', 'abney', 'balmes', 'atanasio', 'pauw']\n",
      "====================\n",
      "\n",
      "Topic 16\n",
      "['allyl', 'incantation', 'annal', 'zooxanthellae', 'aperiodic', 'epistemic', 'luciferase', 'polyphenols', 'triadic', 'prismatic']\n"
     ]
    }
   ],
   "source": [
    "analyzer.rmn.inspect_topics(analyzer.first_topic_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3060486"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.shannon_entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     0.0842\n",
       "9     0.0777\n",
       "35    0.0549\n",
       "21    0.0523\n",
       "46    0.0504\n",
       "2     0.0461\n",
       "20    0.0419\n",
       "7     0.0387\n",
       "22    0.0385\n",
       "13    0.0377\n",
       "14    0.0368\n",
       "41    0.0308\n",
       "34    0.0305\n",
       "19    0.0280\n",
       "1     0.0249\n",
       "40    0.0236\n",
       "48    0.0231\n",
       "32    0.0217\n",
       "26    0.0203\n",
       "11    0.0182\n",
       "45    0.0165\n",
       "25    0.0132\n",
       "39    0.0111\n",
       "17    0.0110\n",
       "3     0.0107\n",
       "0     0.0104\n",
       "47    0.0100\n",
       "36    0.0096\n",
       "10    0.0093\n",
       "49    0.0086\n",
       "44    0.0078\n",
       "33    0.0068\n",
       "29    0.0064\n",
       "42    0.0061\n",
       "37    0.0061\n",
       "38    0.0061\n",
       "4     0.0057\n",
       "27    0.0057\n",
       "30    0.0056\n",
       "16    0.0056\n",
       "43    0.0056\n",
       "18    0.0053\n",
       "6     0.0050\n",
       "5     0.0049\n",
       "28    0.0048\n",
       "23    0.0047\n",
       "24    0.0045\n",
       "31    0.0044\n",
       "12    0.0042\n",
       "15    0.0041\n",
       "dtype: float32"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_use().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9, 22,  2, 46],\n",
       "       [ 9,  8, 21, 22, 35],\n",
       "       [ 8,  9, 22,  7, 35],\n",
       "       ...,\n",
       "       [29, 37, 23,  4, 31],\n",
       "       [21, 17, 32, 41, 35],\n",
       "       [ 2, 32, 41, 35, 17]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.primary_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.347"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_preds[11].round(3).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8821866551207175,\n",
       " 'lower': 0.8796436010095868,\n",
       " 'upper': 0.8847297092318481}"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abortion', 'drug_alcohol', 'budget', 'guns', 'defense', 'economy', 'education', 'foreign', 'health', 'immigration', 'labor', 'minorities', 'women', 'tax', 'trade'])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_keywords.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8543544875543024,\n",
       " 'lower': 0.8515277901812843,\n",
       " 'upper': 0.8571811849273205}"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'immigration'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8614940578368009,\n",
       " 'lower': 0.8583278252398969,\n",
       " 'upper': 0.8646602904337048}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'guns'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8201166751350382,\n",
       " 'lower': 0.8169926580013287,\n",
       " 'upper': 0.8232406922687476}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'tax'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8325434899237656,\n",
       " 'lower': 0.8294430355481792,\n",
       " 'upper': 0.835643944299352}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'women'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.837679005868254,\n",
       " 'lower': 0.834513355527901,\n",
       " 'upper': 0.8408446562086067}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'education'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8587723603182558,\n",
       " 'lower': 0.8560560399141686,\n",
       " 'upper': 0.8614886807223434}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'minorities'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8318494436201203,\n",
       " 'lower': 0.8289183253696087,\n",
       " 'upper': 0.8347805618706315}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'budget'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8145034280806976,\n",
       " 'lower': 0.8113631981805421,\n",
       " 'upper': 0.8176436579808531}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'drug_alcohol'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8238726988337747,\n",
       " 'lower': 0.820945972597457,\n",
       " 'upper': 0.8267994250700923}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'health'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8297398497028997,\n",
       " 'lower': 0.8269322897794448,\n",
       " 'upper': 0.8325474096263547}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'abortion'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8738162841298852,\n",
       " 'lower': 0.8713402076086982,\n",
       " 'upper': 0.8762923606510722}"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject':'foreign'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8208487595973093,\n",
       " 'lower': 0.8176917141483929,\n",
       " 'upper': 0.8240058050462259}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.inter_party_js(conditions={'subject':'tax'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8249732705518298,\n",
       " 'lower': 0.8218381811337004,\n",
       " 'upper': 0.8281083599699591}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'party': 'R', 'subject':'tax'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.8169848236010119,\n",
       " 'lower': 0.8138217310517034,\n",
       " 'upper': 0.8201479161503203}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'party': 'D', 'subject':'tax'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
