{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/rocassius/w266_final/scripts/assembly\")\n",
    "sys.path.append(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document import load_documents\n",
    "from constant import DOC_PRAYER_PATH\n",
    "from subject import subject_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_pickled_object\n",
    "from rmn import *\n",
    "from rmn_data_generator import RMN_DataGenerator\n",
    "from rmn_analyzer import RMN_Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding tools\n",
    "prayer_tools_path = \"/home/rocassius/gen-data/tools/prayer_tools\"\n",
    "\n",
    "tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict\"))\n",
    "metadata_dict = load_pickled_object(os.path.join(prayer_tools_path, \"metadata_dict\"))\n",
    "embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"idf_embedding_matrix\"))\n",
    "global_embedding_matrix = load_pickled_object(os.path.join(prayer_tools_path, \"embedding_matrix_wg\"))\n",
    "global_tokenizer_dict = load_pickled_object(os.path.join(prayer_tools_path, \"tokenizer_dict_wg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_path = \"/home/rocassius/gen-data/tools/\"\n",
    "metadata_dict_global = load_pickled_object(os.path.join(tools_path, \"global_metadata_dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = load_documents([111], DOC_PRAYER_PATH)\n",
    "#d = docs_df.sample(000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>document</th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111116451</td>\n",
       "      <td>KOHL</td>\n",
       "      <td>HERBERT</td>\n",
       "      <td>S</td>\n",
       "      <td>WI</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>responsibility to protect those who cannot pro...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111117170</td>\n",
       "      <td>TOWNS</td>\n",
       "      <td>EDOLPHUS</td>\n",
       "      <td>H</td>\n",
       "      <td>NY</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>obama said i thought that was pretty tough and...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111115330</td>\n",
       "      <td>BEAN</td>\n",
       "      <td>MELISSA</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>change the laws of this country of seeking to ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111121930</td>\n",
       "      <td>FOSTER</td>\n",
       "      <td>BILL</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>good character restraint respect for law and r...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111117650</td>\n",
       "      <td>HOLT</td>\n",
       "      <td>RUSH</td>\n",
       "      <td>H</td>\n",
       "      <td>NJ</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>this bill contains outofcontrol spending it co...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92749</td>\n",
       "      <td>111120531</td>\n",
       "      <td>MIKULSKI</td>\n",
       "      <td>BARBARA</td>\n",
       "      <td>S</td>\n",
       "      <td>MD</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>of a more accurate food facility registry impr...</td>\n",
       "      <td>trade</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92750</td>\n",
       "      <td>111120170</td>\n",
       "      <td>LATTA</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>their lives to save others that day these meda...</td>\n",
       "      <td>trade</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92751</td>\n",
       "      <td>111116441</td>\n",
       "      <td>HUTCHISON</td>\n",
       "      <td>KAY</td>\n",
       "      <td>S</td>\n",
       "      <td>TX</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>investment this year the development assistanc...</td>\n",
       "      <td>trade</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92752</td>\n",
       "      <td>111120860</td>\n",
       "      <td>POLIS</td>\n",
       "      <td>JARED</td>\n",
       "      <td>H</td>\n",
       "      <td>CO</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>economytoo much in the last decade of folks pa...</td>\n",
       "      <td>trade</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92753</td>\n",
       "      <td>111118680</td>\n",
       "      <td>BORDALLO</td>\n",
       "      <td>MADELEINE</td>\n",
       "      <td>H</td>\n",
       "      <td>GU</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>central valley the majority has ignored yet an...</td>\n",
       "      <td>trade</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92754 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       speakerid   lastname  firstname chamber state gender party  \\\n",
       "0      111116451       KOHL    HERBERT       S    WI      M     D   \n",
       "1      111117170      TOWNS   EDOLPHUS       H    NY      M     D   \n",
       "2      111115330       BEAN    MELISSA       H    IL      F     D   \n",
       "3      111121930     FOSTER       BILL       H    IL      M     D   \n",
       "4      111117650       HOLT       RUSH       H    NJ      M     D   \n",
       "...          ...        ...        ...     ...   ...    ...   ...   \n",
       "92749  111120531   MIKULSKI    BARBARA       S    MD      F     D   \n",
       "92750  111120170      LATTA     ROBERT       H    OH      M     R   \n",
       "92751  111116441  HUTCHISON        KAY       S    TX      F     R   \n",
       "92752  111120860      POLIS      JARED       H    CO      M     D   \n",
       "92753  111118680   BORDALLO  MADELEINE       H    GU      F     D   \n",
       "\n",
       "                                                document   subject session  \n",
       "0      responsibility to protect those who cannot pro...  abortion     111  \n",
       "1      obama said i thought that was pretty tough and...  abortion     111  \n",
       "2      change the laws of this country of seeking to ...  abortion     111  \n",
       "3      good character restraint respect for law and r...  abortion     111  \n",
       "4      this bill contains outofcontrol spending it co...  abortion     111  \n",
       "...                                                  ...       ...     ...  \n",
       "92749  of a more accurate food facility registry impr...     trade     111  \n",
       "92750  their lives to save others that day these meda...     trade     111  \n",
       "92751  investment this year the development assistanc...     trade     111  \n",
       "92752  economytoo much in the last decade of folks pa...     trade     111  \n",
       "92753  central valley the majority has ignored yet an...     trade     111  \n",
       "\n",
       "[92754 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=*= RMN Module =*=#\n",
    "#==================#\n",
    "\n",
    "# RMN Class for training Relationship Modeling Networks \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking, Reshape, Concatenate\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rmn_data_generator import RMN_DataGenerator\n",
    "from helper import pickle_object, load_pickled_object\n",
    "from vector_math import find_nn_cos\n",
    "\n",
    "# constants\n",
    "MAX_SPAN_LENGTH = 50\n",
    "NUM_TOPICS = 20\n",
    "LAMBDA = 1.0\n",
    "GAMMA = 1.0\n",
    "\n",
    "# hyperparameters\n",
    "OPTIMIZER = 'adam'\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "\n",
    "# saving tags\n",
    "RMN_TAG = \"rmn_%s\"\n",
    "MODEL = \"model.h5\"\n",
    "ATTR = \"attributes\"\n",
    "\n",
    "# attribute keys\n",
    "N_TOP_KEY = 'num_topics'\n",
    "LAMB_KEY  = 'lambda'\n",
    "EMBED_KEY = 'emedding_matrix'\n",
    "TOKEN_KEY = 'tokenizer_dict'\n",
    "META_KEY  = 'metadata_dict'\n",
    "DIM_KEY = 'meta_embedding_dim'\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \"\"\"\n",
    "    Class for constructing a Relationship Modeling Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # model attrbiutes\n",
    "        self.num_topics = NUM_TOPICS\n",
    "        self.embedding_matrix = None\n",
    "        self.meta_embedding_dim = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        # inference attributes\n",
    "        self.infer_embedding_matrix = None\n",
    "        self.infer_tokenizer_dict = None\n",
    "        \n",
    "        # models \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def topic_matrix(self):\n",
    "        \"\"\"Return the topic matrix associated with the rmn\"\"\"\n",
    "        # dim = [num_topics, embedding_dim]\n",
    "        return self.model.get_layer('Wd').get_weights()[0].T\n",
    "    \n",
    "    @property\n",
    "    def tuned_embedding_matrix(self):\n",
    "        \"\"\"Return the current embedding matrix of the rmn\"\"\"\n",
    "        return rmn.model.get_layer('Span.Embedding').get_weights()[0]\n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"Hinge loss function.\n",
    "        \"\"\"\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            \n",
    "            # hinge_loss\n",
    "            y_true_normalized = K.l2_normalize(y_true, axis=-1)\n",
    "            y_pred_normalized = K.l2_normalize(y_pred, axis=-1)\n",
    "            dot_product = K.sum(y_true_normalized * y_pred_normalized, axis=-1)\n",
    "            hinge_loss = K.mean(K.maximum(0., 1. - dot_product))\n",
    "\n",
    "            return hinge_loss \n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self, embedding_trainable=False, bias_reconstruct=True,\n",
    "                    gamma = 1., theta = 1., omega = 1., word_dropout = 0.5):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # Span Input\n",
    "        span_input = Input(shape=(self.tokenizer_dict['max_span_length'],), \n",
    "                           name='Span.Input')\n",
    "        span_embedding = Embedding(input_dim=len(self.tokenizer_dict['word_index']) + 1, \n",
    "                                   output_dim=self.embedding_dim, \n",
    "                                   weights=[self.embedding_matrix],\n",
    "                                   input_length=self.tokenizer_dict['max_span_length'],\n",
    "                                   trainable=embedding_trainable, \n",
    "                                   name = 'Span.Embedding')(span_input)\n",
    "        \n",
    "        # Mask for randomly dropping words\n",
    "        dropout_mask = K.stack(\n",
    "            [K.random_binomial((span_embedding.shape[1],), p=word_dropout)]*span_embedding.shape[2], axis=1)\n",
    "        # Average over the remaining words\n",
    "        span_avg = Lambda(lambda x: K.mean(x * K.expand_dims(dropout_mask, axis=0), axis=1), \n",
    "                          name = \"Span.Avg.Layer\")(span_embedding)\n",
    "\n",
    "        input_layers = [span_input]\n",
    "        embedding_layers = [span_avg]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.meta_embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.meta_embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concatenate span vector with metadata embeddings\n",
    "        _ht = Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer whose output is a probability distribution\n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\",\n",
    "                   activity_regularizer = Purity(gamma, theta, omega),\n",
    "                   name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   use_bias = bias_reconstruct,\n",
    "                   kernel_regularizer = Orthogonality(self.lamb),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        #model.compile(optimizer = OPTIMIZER, loss='mean_squared_error')\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss())\n",
    "        self.model = model\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def set_topic_vectors(self, words):\n",
    "        \"\"\"Set the topic vectors with vectors corresponding to the given words\n",
    "        \"\"\"\n",
    "        # get the word ids\n",
    "        word_ids = self.tokenizer_dict['tokenize_pad'](words)[:,0]\n",
    "        \n",
    "        # replicate associated weights up to num_topics\n",
    "        weights = np.tile(self.embedding_matrix[word_ids], \n",
    "                          (-(self.num_topics // -len(words)),1))[:self.num_topics]\n",
    "        \n",
    "        # set weights layer weights\n",
    "        r = self.model.get_layer(\"R\")\n",
    "        if len(r.get_weights()) == 1:\n",
    "            r.set_weights([weights])\n",
    "        else:\n",
    "            r.set_weights([weights, r.get_weights()[1]])\n",
    "        \n",
    "        \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "          \n",
    "    \n",
    "    def prep_spans(self, documents):\n",
    "        \"\"\"Returns the lists of word ids associated with the text\n",
    "        \"\"\"\n",
    "        return self.tokenizer_dict['tokenize_pad'](documents)\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_x = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                      for col in self.metadata_dict.keys()]\n",
    "\n",
    "        return metadata_x\n",
    "        \n",
    "    \n",
    "    def prep_X(self, df, for_training=False):\n",
    "        \"\"\"Preps metadata and spans for training or prediction\n",
    "        \"\"\"\n",
    "        spans_y = self.prep_spans(df['document'])\n",
    "        metadata_x = self.prep_metadata(df)\n",
    "        X = [spans_y] + metadata_x\n",
    "        \n",
    "        if for_training:\n",
    "            y = self.embedding_matrix[spans_y].mean(axis=1)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def predict_y(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the rmn outputs for a df\n",
    "        \"\"\"\n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.model)\n",
    "        else:\n",
    "            return self.predict_(df, self.model)\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"        \n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.topic_model)\n",
    "        else:\n",
    "            return self.predict_(df, self.topic_model)\n",
    "\n",
    "        \n",
    "    def predict_(self, df, model):\n",
    "        \"\"\"Makes a predictions for a df with a model\n",
    "        \"\"\"\n",
    "        return model.predict(x=self.prep_X(df))\n",
    "        \n",
    "    \n",
    "    def predict_with_generator(self, df, model):\n",
    "        \"\"\"Predict topic distributions with a generator\n",
    "        \"\"\"\n",
    "        # Make sure data is not empty\n",
    "        assert not df.empty\n",
    "\n",
    "        # Calculate good batch size, \n",
    "        batch_size = max(1, min(10000, df.shape[0] // 10))\n",
    "        n_batches = df.shape[0] // batch_size\n",
    "\n",
    "        if n_batches < 2: \n",
    "            return self.predict_(df, model)\n",
    "        else:\n",
    "            # calculate remainder batch size\n",
    "            r = df.shape[0] % batch_size\n",
    "            if r == 0:\n",
    "                g_index = df.index[:-batch_size]\n",
    "                r_index = df.index[-batch_size:]\n",
    "            else:\n",
    "                g_index = df.index[:-r]\n",
    "                r_index = df.index[-r:]\n",
    "\n",
    "            # Make generator\n",
    "            g = RMN_DataGenerator(self, df.loc[g_index], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Predict on remainder batch\n",
    "            r_pred = self.predict_(df.loc[r_index], model)\n",
    "            # predict on generated batches\n",
    "            g_pred = model.predict_generator(g, use_multiprocessing=True, workers=10, verbose=1)\n",
    "\n",
    "            assert r_pred.shape[1] == g_pred.shape[1]\n",
    "            pred = np.vstack([g_pred, r_pred])\n",
    "\n",
    "            return pred\n",
    "        \n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Save the model's weights, architecture and attributes\n",
    "        \"\"\"\n",
    "        # assemble attribute dictionary\n",
    "        attribute_dict = {\n",
    "            N_TOP_KEY:  self.num_topics,\n",
    "            LAMB_KEY:   self.lamb,\n",
    "            EMBED_KEY:  self.embedding_matrix,\n",
    "            TOKEN_KEY:  self.tokenizer_dict,\n",
    "            META_KEY:   self.metadata_dict, \n",
    "            DIM_KEY:    self.meta_embedding_dim}\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        # save model weights\n",
    "        self.model.save_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # save model attributes\n",
    "        pickle_object(attribute_dict, os.path.join(model_path, ATTR))\n",
    "        \n",
    "        \n",
    "    def load_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Load the model, weights, architecture and attributes from a saved model\n",
    "        \"\"\"\n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        \n",
    "        # load attributes\n",
    "        attributes_dict = load_pickled_object(os.path.join(model_path, ATTR))\n",
    "        \n",
    "        # update attributes\n",
    "        self.num_topics         = attributes_dict[N_TOP_KEY]\n",
    "        self.lamb               = attributes_dict[LAMB_KEY]\n",
    "        self.embedding_matrix   = attributes_dict[EMBED_KEY]\n",
    "        self.tokenizer_dict     = attributes_dict[TOKEN_KEY]\n",
    "        self.metadata_dict      = attributes_dict[META_KEY]\n",
    "        self.meta_embedding_dim = attributes_dict[DIM_KEY] \n",
    "        \n",
    "        # construct identical model architecture\n",
    "        self.build_model()\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def inspect_topics(self, which_topics='all', k_neighbors=10):\n",
    "        \"\"\"\n",
    "        Ouput the nearest neighbors of every topic vector in\n",
    "        the model's topic layer\n",
    "        \"\"\"\n",
    "        if which_topics == 'all':\n",
    "            which_topics = range(self.num_topics) \n",
    "        \n",
    "        if (self.infer_embedding_matrix is None or \n",
    "            self.infer_tokenizer_dict is None):\n",
    "            self.infer_embedding_matrix = self.embedding_matrix\n",
    "            self.infer_tokenizer_dict = self.tokenizer_dict\n",
    "        \n",
    "        E = self.infer_embedding_matrix # dim = [vocab_size, embedding_dim]\n",
    "        Wd = self.topic_matrix          # dim = [num_topics, embedding_dim]\n",
    "        \n",
    "        for i in which_topics:\n",
    "            # find nearest neighbors to topic\n",
    "            neighbors, sim = find_nn_cos(Wd[i], E, k_neighbors)\n",
    "            words = [self.infer_tokenizer_dict['tokenizer'].index_word[v] for v in neighbors]\n",
    "            print(20*\"=\" +\"\\n\")\n",
    "            print(\"Topic\", i)\n",
    "            print(words)\n",
    "    \n",
    "    \n",
    "# Orthogonality Regularizer #\n",
    "\n",
    "class Orthogonality(Regularizer):\n",
    "    \"\"\"\n",
    "    Regularizer for penalizing non-orthogonal components of a weight matrix.\n",
    "    \n",
    "    Args:\n",
    "    - lamb: (Float) regularization penalty weight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lamb = 1.):\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def __call__(self, R):\n",
    "        \"\"\"Returns a component dependence penalty for matrix R\n",
    "        \"\"\"\n",
    "        RRT = K.dot(R, K.transpose(R))\n",
    "        I = K.eye(RRT.shape.as_list()[0])\n",
    "        penalty = self.lamb * K.sqrt(K.sum(K.square(RRT - I)))\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    \n",
    "# Topic Purity Regularizer #\n",
    "\n",
    "class Purity(Regularizer):\n",
    "    \"\"\"Regularizer for penalizing highly impure probability distributions\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma = 1., theta = 1., omega = 1.):\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.omega = omega\n",
    "\n",
    "    def __call__(self, p):\n",
    "        \"\"\"Returns the avergage shannon entropy of the distribution(s) p\n",
    "        \"\"\"\n",
    "        # calculate impurity and concentration\n",
    "        impurity = K.sum(p*-K.log(p)/K.log(K.constant(2)), axis=-1)\n",
    "        concentration = K.max(p, axis=-1)\n",
    "        # calculate batch similarity\n",
    "        ppt = K.dot(p, K.transpose(p)) \n",
    "        similarity = K.mean(ppt) - K.mean(tf.linalg.diag_part(ppt))\n",
    "        \n",
    "        penalty = (self.gamma * K.mean(impurity) + \n",
    "                   self.theta * K.mean(concentration) + \n",
    "                   self.omega * similarity)\n",
    "        \n",
    "        return penalty\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.33333334,   1.3333334 ,   0.33333334,  11.666667  ,\n",
       "          2.        ],\n",
       "       [ 23.333334  ,   4.3333335 ,  -0.33333334, -29.666666  ,\n",
       "         -2.        ]], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2, 3, 5 \n",
    "x = K.constant([[[1,4,1,35,6], [89,5,7,23,9],[-5,8,78,-55,666]], \n",
    "                [[70,13,-1,-89,-6], [-8,-5,7,8,8], [-70,-65,17,9,3]]])\n",
    "x\n",
    "\n",
    "K.get_value(K.mean(\n",
    "    x*tf.stack([tf.stack([K.random_binomial((x.shape[1],), p=0.5)]*x.shape[2],axis=1)]*2),\n",
    "    axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "m = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   0.,   0.,   0.,   0.],\n",
       "        [ 89.,   5.,   7.,  23.,   9.],\n",
       "        [ -5.,   8.,  78., -55., 666.]],\n",
       "\n",
       "       [[  0.,   0.,  -0.,  -0.,  -0.],\n",
       "        [ -8.,  -5.,   7.,   8.,   8.],\n",
       "        [-70., -65.,  17.,   9.,   3.]]], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_matrix = K.stack([K.random_binomial((m,), p=0.5)] * d , axis=1)\n",
    "K.get_value(x*K.expand_dims(dropout_matrix, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RMN()\n",
    "rmn.embedding_matrix = embedding_matrix\n",
    "rmn.tokenizer_dict = tokenizer_dict\n",
    "rmn.infer_embedding_matrix = global_embedding_matrix\n",
    "rmn.infer_tokenizer_dict = global_tokenizer_dict\n",
    "rmn.metadata_dict = metadata_dict_global\n",
    "# rmn.metadata_dict = {'party': metadata_dict['party']}\n",
    "rmn.meta_embedding_dim = 25\n",
    "rmn.num_topics = 100\n",
    "rmn.lamb = 1.0\n",
    "rmn.build_model(gamma=.1, theta=0., omega=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Span.Input (InputLayer)         [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "speakerid.Input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chamber.Input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "state.Input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender.Input (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "party.Input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "session.Input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subject.Input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Span.Embedding (Embedding)      (None, 40, 100)      5503900     Span.Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_35 (Embedding)        (None, 1, 25)        801275      speakerid.Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_36 (Embedding)        (None, 1, 25)        75          chamber.Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_37 (Embedding)        (None, 1, 25)        1450        state.Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)        (None, 1, 25)        75          gender.Input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 1, 25)        300         party.Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 1, 25)        1750        session.Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_41 (Embedding)        (None, 1, 25)        675         subject.Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Span.Avg.Layer (Lambda)         (None, 100)          0           Span.Embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "speakerid.Embed.Layer (Reshape) (None, 25)           0           embedding_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "chamber.Embed.Layer (Reshape)   (None, 25)           0           embedding_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "state.Embed.Layer (Reshape)     (None, 25)           0           embedding_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gender.Embed.Layer (Reshape)    (None, 25)           0           embedding_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "party.Embed.Layer (Reshape)     (None, 25)           0           embedding_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "session.Embed.Layer (Reshape)   (None, 25)           0           embedding_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "subject.Embed.Layer (Reshape)   (None, 25)           0           embedding_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Concat.Layer (Concatenate)      (None, 275)          0           Span.Avg.Layer[0][0]             \n",
      "                                                                 speakerid.Embed.Layer[0][0]      \n",
      "                                                                 chamber.Embed.Layer[0][0]        \n",
      "                                                                 state.Embed.Layer[0][0]          \n",
      "                                                                 gender.Embed.Layer[0][0]         \n",
      "                                                                 party.Embed.Layer[0][0]          \n",
      "                                                                 session.Embed.Layer[0][0]        \n",
      "                                                                 subject.Embed.Layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Wh (Dense)                      (None, 100)          27600       Concat.Layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Wd (Dense)                      (None, 100)          10100       Wh[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "R (Dense)                       (None, 100)          10100       Wd[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 6,357,300\n",
      "Trainable params: 853,400\n",
      "Non-trainable params: 5,503,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rmn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmn.set_topic_vectors(subject_keywords.keys())\n",
    "rmn.set_topic_vectors(['abortion', 'children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = docs_df[docs_df['subject']=='abortion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rmn.prep_X(d, for_training=True)\n",
    "#X, y = rmn.prep_X(docs_df.sample(frac=1), for_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 407 samples\n",
      "Epoch 1/60\n",
      "407/407 [==============================] - 0s 257us/sample - loss: 68933.6466\n",
      "Epoch 2/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 68373.2431\n",
      "Epoch 3/60\n",
      "407/407 [==============================] - 0s 255us/sample - loss: 67817.2569\n",
      "Epoch 4/60\n",
      "407/407 [==============================] - 0s 251us/sample - loss: 67265.6694\n",
      "Epoch 5/60\n",
      "407/407 [==============================] - 0s 248us/sample - loss: 66718.4567\n",
      "Epoch 6/60\n",
      "407/407 [==============================] - 0s 260us/sample - loss: 66175.5271\n",
      "Epoch 7/60\n",
      "407/407 [==============================] - 0s 254us/sample - loss: 65636.9099\n",
      "Epoch 8/60\n",
      "407/407 [==============================] - 0s 254us/sample - loss: 65102.5015\n",
      "Epoch 9/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 64572.2929\n",
      "Epoch 10/60\n",
      "407/407 [==============================] - 0s 255us/sample - loss: 64046.2424\n",
      "Epoch 11/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 63524.3177\n",
      "Epoch 12/60\n",
      "407/407 [==============================] - 0s 248us/sample - loss: 63006.4735\n",
      "Epoch 13/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 62492.7257\n",
      "Epoch 14/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 61982.9552\n",
      "Epoch 15/60\n",
      "407/407 [==============================] - 0s 242us/sample - loss: 61477.1490\n",
      "Epoch 16/60\n",
      "407/407 [==============================] - 0s 252us/sample - loss: 60975.2853\n",
      "Epoch 17/60\n",
      "407/407 [==============================] - 0s 256us/sample - loss: 60477.3341\n",
      "Epoch 18/60\n",
      "407/407 [==============================] - 0s 254us/sample - loss: 59983.2730\n",
      "Epoch 19/60\n",
      "407/407 [==============================] - 0s 252us/sample - loss: 59493.0384\n",
      "Epoch 20/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 59006.5705\n",
      "Epoch 21/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 58523.9442\n",
      "Epoch 22/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 58045.1025\n",
      "Epoch 23/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 57569.9076\n",
      "Epoch 24/60\n",
      "407/407 [==============================] - 0s 253us/sample - loss: 57098.4076\n",
      "Epoch 25/60\n",
      "407/407 [==============================] - 0s 253us/sample - loss: 56630.5844\n",
      "Epoch 26/60\n",
      "407/407 [==============================] - 0s 248us/sample - loss: 56166.3407\n",
      "Epoch 27/60\n",
      "407/407 [==============================] - 0s 251us/sample - loss: 55705.7366\n",
      "Epoch 28/60\n",
      "407/407 [==============================] - 0s 252us/sample - loss: 55248.6346\n",
      "Epoch 29/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 54795.0994\n",
      "Epoch 30/60\n",
      "407/407 [==============================] - 0s 244us/sample - loss: 54345.0805\n",
      "Epoch 31/60\n",
      "407/407 [==============================] - 0s 246us/sample - loss: 53898.5025\n",
      "Epoch 32/60\n",
      "407/407 [==============================] - 0s 246us/sample - loss: 53455.4087\n",
      "Epoch 33/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 53015.7187\n",
      "Epoch 34/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 52579.4217\n",
      "Epoch 35/60\n",
      "407/407 [==============================] - 0s 242us/sample - loss: 52146.5237\n",
      "Epoch 36/60\n",
      "407/407 [==============================] - 0s 253us/sample - loss: 51716.8827\n",
      "Epoch 37/60\n",
      "407/407 [==============================] - 0s 242us/sample - loss: 51290.6207\n",
      "Epoch 38/60\n",
      "407/407 [==============================] - 0s 252us/sample - loss: 50867.6109\n",
      "Epoch 39/60\n",
      "407/407 [==============================] - 0s 244us/sample - loss: 50447.8684\n",
      "Epoch 40/60\n",
      "407/407 [==============================] - 0s 244us/sample - loss: 50031.3791\n",
      "Epoch 41/60\n",
      "407/407 [==============================] - 0s 247us/sample - loss: 49618.0870\n",
      "Epoch 42/60\n",
      "407/407 [==============================] - 0s 257us/sample - loss: 49207.9657\n",
      "Epoch 43/60\n",
      "407/407 [==============================] - 0s 245us/sample - loss: 48801.0020\n",
      "Epoch 44/60\n",
      "407/407 [==============================] - 0s 255us/sample - loss: 48397.1977\n",
      "Epoch 45/60\n",
      "407/407 [==============================] - 0s 251us/sample - loss: 47996.4658\n",
      "Epoch 46/60\n",
      "407/407 [==============================] - 0s 252us/sample - loss: 47598.8195\n",
      "Epoch 47/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 47204.2362\n",
      "Epoch 48/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 46812.6969\n",
      "Epoch 49/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 46424.1935\n",
      "Epoch 50/60\n",
      "407/407 [==============================] - 0s 244us/sample - loss: 46038.6383\n",
      "Epoch 51/60\n",
      "407/407 [==============================] - 0s 246us/sample - loss: 45656.0674\n",
      "Epoch 52/60\n",
      "407/407 [==============================] - 0s 248us/sample - loss: 45276.4410\n",
      "Epoch 53/60\n",
      "407/407 [==============================] - 0s 244us/sample - loss: 44899.7335\n",
      "Epoch 54/60\n",
      "407/407 [==============================] - 0s 250us/sample - loss: 44525.9232\n",
      "Epoch 55/60\n",
      "407/407 [==============================] - 0s 247us/sample - loss: 44154.9853\n",
      "Epoch 56/60\n",
      "407/407 [==============================] - 0s 253us/sample - loss: 43786.8828\n",
      "Epoch 57/60\n",
      "407/407 [==============================] - 0s 253us/sample - loss: 43421.6455\n",
      "Epoch 58/60\n",
      "407/407 [==============================] - 0s 249us/sample - loss: 43059.2109\n",
      "Epoch 59/60\n",
      "407/407 [==============================] - 0s 246us/sample - loss: 42699.5271\n",
      "Epoch 60/60\n",
      "407/407 [==============================] - 0s 247us/sample - loss: 42342.6335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4a5c7264d0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.model.fit(X, y, epochs=60, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = RMN_Analyzer(rmn, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91456074"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.shannon_entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_preds.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 48, 33, 84, 34],\n",
       "       [77, 48, 33, 34, 84],\n",
       "       [77, 48, 33, 34, 84],\n",
       "       ...,\n",
       "       [77, 48, 33, 34, 84],\n",
       "       [77, 48, 33, 84, 34],\n",
       "       [77, 48, 33, 84, 34]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.primary_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77    402\n",
       "48      5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.first_topic_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77    0.66\n",
       "48    0.34\n",
       "33    0.00\n",
       "34    0.00\n",
       "84    0.00\n",
       "      ... \n",
       "64    0.00\n",
       "68    0.00\n",
       "56    0.00\n",
       "19    0.00\n",
       "46    0.00\n",
       "Length: 100, dtype: float32"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.topic_use().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.352, 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.648, 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   ], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(analyzer.topic_preds[0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.71, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  ], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(analyzer.topic_preds[1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.327, 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.673, 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   ], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(analyzer.topic_preds[70],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n",
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "\n",
      "Topic 77\n",
      "['idp', 'entangled', 'serviceman', 'ancestral', 'lineage', 'securely', 'quaternions', 'tethered', 'identity', 'albatross']\n",
      "====================\n",
      "\n",
      "Topic 48\n",
      "['activex', 'maneuver', 'outpost', 'invisibility', 'lapse', 'maneuvering', 'clone', 'paintbrush', 'samurai', 'pcu']\n"
     ]
    }
   ],
   "source": [
    "analyzer.rmn.inspect_topics(analyzer.first_topic_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.288\n",
       "1    0.270\n",
       "9    0.209\n",
       "6    0.135\n",
       "4    0.098\n",
       "5    0.000\n",
       "2    0.000\n",
       "8    0.000\n",
       "0    0.000\n",
       "3    0.000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(analyzer.topic_use(conditions={'subject': 'abortion'}),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.10666140194929351,\n",
       " 'lower': 0.10510082393074104,\n",
       " 'upper': 0.10822197996784592}"
      ]
     },
     "execution_count": 1099,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.inter_party_js(conditions={}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.1085427735816004,\n",
       " 'lower': 0.10697649360462963,\n",
       " 'upper': 0.11010905355857117}"
      ]
     },
     "execution_count": 1100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'party':'R'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.1022140048119605,\n",
       " 'lower': 0.10070788089142411,\n",
       " 'upper': 0.10372012873249688}"
      ]
     },
     "execution_count": 1101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'party':'D'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'labor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.1811149685760053,\n",
       " 'lower': 0.1801685756634546,\n",
       " 'upper': 0.18206136148855603}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject': s, 'party':'R'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.18142981918058432,\n",
       " 'lower': 0.18048422123088034,\n",
       " 'upper': 0.1823754171302883}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js(conditions={'subject': s, 'party':'D'}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.1801105393970354,\n",
       " 'lower': 0.17916237570626425,\n",
       " 'upper': 0.18105870308780653}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.inter_party_js(conditions={'subject':s}, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
