{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/rocassius/w266_final/scripts/assembly\")\n",
    "from constant import DOC_SAMPLE_PATH, DOC_PROPER_PATH\n",
    "from document import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = load_documents(sessions=list(range(104,105)), read_path=DOC_PROPER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/rocassius/w266_final/scripts/modeling\")\n",
    "from rmn import *\n",
    "from analysis import *\n",
    "from rmn_data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=*= RMN Module =*=#\n",
    "#==================#\n",
    "\n",
    "# RMN Class for training Relationship Modeling Networks \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking, Reshape\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "\n",
    "from helper import pickle_object, load_pickled_object\n",
    "from vector_math import find_nn_cos\n",
    "\n",
    "# constants\n",
    "MAX_SPAN_LENGTH = 50\n",
    "NUM_TOPICS = 20\n",
    "LAMBDA = 5.0\n",
    "\n",
    "# hyperparameters\n",
    "OPTIMIZER = 'adam'\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "\n",
    "# saving tags\n",
    "RMN_TAG = \"rmn_%s\"\n",
    "MODEL = \"model.h5\"\n",
    "ATTR = \"attributes\"\n",
    "\n",
    "# attribute keys\n",
    "N_TOP_KEY = 'num_topics'\n",
    "LAMB_KEY  = 'lambda'\n",
    "EMBED_KEY = 'emedding_matrix'\n",
    "TOKEN_KEY = 'tokenizer_dict'\n",
    "META_KEY  = 'metadata_dict'\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \"\"\"\n",
    "    Class for constructing a Relationship Modeling Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # model parameters\n",
    "        self.num_topics = NUM_TOPICS\n",
    "        self.lamb = LAMBDA\n",
    "        \n",
    "        # model attrbiutes\n",
    "        self.embedding_matrix = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        # models \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"Hinge loss function.\n",
    "        \"\"\"\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            # hinge_loss\n",
    "            y_true_normalized = K.l2_normalize(y_true, axis=-1)\n",
    "            y_pred_normalized = K.l2_normalize(y_pred, axis=-1)\n",
    "            dot_product = K.sum(y_true_normalized * y_pred_normalized, axis=-1)\n",
    "            hinge_loss = K.mean(K.maximum(0., 1. - dot_product))\n",
    "\n",
    "            return hinge_loss \n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # document span input\n",
    "        vt = Input(shape=(self.embedding_dim, ), name='Span.Input')\n",
    "    \n",
    "        input_layers = [vt]\n",
    "        embedding_layers = [vt]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            \n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concat speaker metadata embeddings\n",
    "        _ht = tf.keras.layers.Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer with softmax activation, (where previous states will eventually be inserted) \n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\", name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   kernel_regularizer = Orthogonality(self.lamb),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss())\n",
    "        self.model = model\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "    \n",
    "    \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "    \n",
    "    def prep_y(self, y):\n",
    "        \"\"\"Returns the average of the vectors in each span of text\n",
    "        \"\"\"\n",
    "        padded_spans = self.tokenizer_dict['tokenize_pad'](y)\n",
    "        vector_spans = self.embedding_matrix[padded_spans].mean(axis=1)\n",
    "        \n",
    "        return vector_spans\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_ids = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                        for col in self.metadata_dict.keys()]\n",
    "\n",
    "        return metadata_ids\n",
    "        \n",
    "    \n",
    "    def prep_inputs(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        vector_spans = self.prep_y(df['document'])\n",
    "        metadata_ids = self.prep_metadata(df)\n",
    "        inputs = [vector_spans] + metadata_ids\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"\n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "            \n",
    "        topic_preds = self.topic_model.predict(x=self.prep_inputs(df))\n",
    "        \n",
    "        return topic_preds\n",
    "    \n",
    "    \n",
    "    def predict_topics_generator(self, df):\n",
    "        \"\"\"Predict topic distributions with a generator\n",
    "        \"\"\"\n",
    "        # Make sure data is not empty\n",
    "        assert not df.empty\n",
    "        \n",
    "        # Calculate good batch size, \n",
    "        batch_size = max(1, min(10000, df.shape[0] // 10))\n",
    "        n_batches = df.shape[0] // batch_size\n",
    "        \n",
    "        if n_batches < 2: \n",
    "            return self.predict_topics(df)\n",
    "        else:\n",
    "            # calculate remainder batch size\n",
    "            r = df.shape[0] % batch_size\n",
    "            \n",
    "            if r == 0:\n",
    "                g_index = df.index[:-batch_size]\n",
    "                r_index = df.index[-batch_size:]\n",
    "            else:\n",
    "                g_index = df.index[:-r]\n",
    "                r_index = df.index[-r:]\n",
    "            \n",
    "            # Make generator, predict on generator\n",
    "            g = RMN_DataGenerator(self, df.loc[g_index], batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Predict on remainder batch\n",
    "            r_pred = self.predict_topics(df.loc[r_index])\n",
    "            g_pred = self.topic_model.predict_generator(\n",
    "                g, use_multiprocessing=True, workers=10, verbose=1)\n",
    "            \n",
    "            assert r_pred.shape[1] == g_pred.shape[1]\n",
    "            topic_preds = np.vstack([g_pred, r_pred])\n",
    "            \n",
    "            return topic_preds\n",
    "\n",
    "    \n",
    "    def fit(self, df, batch_size = BATCH_SIZE, epochs = EPOCHS):\n",
    "        \n",
    "        inputs = self.prep_inputs(df)\n",
    "        y_true = self.prep_y(df['document'])\n",
    "        \n",
    "        self.model.fit(x = inputs, \n",
    "                       y = y_true, \n",
    "                       batch_size = batch_size, \n",
    "                       epochs = epochs)\n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Save the model's weights, architecture and attributes\n",
    "        \"\"\"\n",
    "        \n",
    "        # assemble attribute dictionary\n",
    "        attribute_dict = {\n",
    "            N_TOP_KEY:  self.num_topics,\n",
    "            LAMB_KEY:   self.lamb,\n",
    "            EMBED_KEY:  self.embedding_matrix,\n",
    "            TOKEN_KEY:  self.tokenizer_dict,\n",
    "            META_KEY:   self.metadata_dict}\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        # save model weights\n",
    "        self.model.save_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # save model attributes\n",
    "        pickle_object(attribute_dict, os.path.join(model_path, ATTR))\n",
    "        \n",
    "        \n",
    "    def load_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Load the model, weights, architecture and attributes from a saved model\n",
    "        \"\"\"\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        \n",
    "        # load attributes\n",
    "        attributes_dict = load_pickled_object(os.path.join(model_path, ATTR))\n",
    "        \n",
    "        # update attributes\n",
    "        self.num_topics       = attributes_dict[N_TOP_KEY]\n",
    "        self.lamb             = attributes_dict[LAMB_KEY]\n",
    "        self.embedding_matrix = attributes_dict[EMBED_KEY]\n",
    "        self.tokenizer_dict   = attributes_dict[TOKEN_KEY]\n",
    "        self.metadata_dict    = attributes_dict[META_KEY]\n",
    "        \n",
    "        # construct identical model architecture\n",
    "        self.build_model()\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def inspect_topics(self, k_neighbors=10):\n",
    "        \"\"\"\n",
    "        Ouput the nearest neighbors of every topic vector in\n",
    "        the model's topic layer\n",
    "        \"\"\"\n",
    "    \n",
    "        # get embedding matrix, dim = [num_words, embedding_dim]\n",
    "        E = self.embedding_matrix\n",
    "        \n",
    "        # get topic matrix, dim = [num_topics, embedding_dim]\n",
    "        Wd = self.model.get_layer('Wd').get_weights()[0].T\n",
    "        \n",
    "        for i in range(Wd.shape[0]):\n",
    "            \n",
    "            neighbors, sim = find_nn_cos(Wd[i], E, k_neighbors)\n",
    "            words = [self.tokenizer_dict['tokenizer'].index_word[v] for v in neighbors]\n",
    "            \n",
    "            print(20*\"=\" +\"\\n\")\n",
    "            print(\"Topic\", i)\n",
    "            print(words)\n",
    "    \n",
    "    \n",
    "# Orthogonality Regularizer #\n",
    "\n",
    "class Orthogonality(Regularizer):\n",
    "    \"\"\"Regularizer for discouraging non-orthogonal components.\n",
    "    \n",
    "    # Arguments\n",
    "        lamb: Float; regularization penalty weight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lamb = 1.):\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def __call__(self, R):\n",
    "        RRT = K.dot(R, K.transpose(R))\n",
    "        I = K.eye(int(RRT.shape[0]))\n",
    "        penalty = self.lamb * K.sqrt(K.sum(K.square(RRT - I)))\n",
    "        \n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RMN()\n",
    "rmn.load_rmn(name=\"half\", save_path = \"/home/rocassius/gen-data/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.sample(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1\n",
      "n_batches 8\n",
      "substracting one batch\n",
      "7/7 [==============================] - 2s 235ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = rmn.predict_topics_generator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = rmn.predict_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(np.abs(p - preds)))\n",
    "np.sum(p - preds) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SUB_KEY = 'subject'\n",
    "SPEAKER = 'speakerid'\n",
    "PARTY = 'party'\n",
    "REP = 'R'\n",
    "DEM = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RMN_Analyzer(object):\n",
    "    \"\"\"Class for Analyzing an RMN with respect to a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rmn, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - rmn: (RMN) the RMN to be used for analysis\n",
    "        - df : (DataFrame) the dataframe to analyze\n",
    "        \"\"\"\n",
    "        \n",
    "        'Initialization'\n",
    "        self.rmn = rmn\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.topic_preds = None\n",
    "        \n",
    "    @property\n",
    "    def index(self):\n",
    "        return self.df.index\n",
    "         \n",
    "        \n",
    "    def predict_topics(self, use_generator=True):\n",
    "        \"\"\"Computes the topic predictions for all observations\n",
    "        \"\"\"\n",
    "        if use_generator:\n",
    "            self.topic_preds = self.rmn.predict_topics_generator(self.df)\n",
    "        else:\n",
    "            self.topic_preds = self.rmn.predict_topics(self.df)\n",
    "        \n",
    "\n",
    "    def bool_subset(self, col, value):\n",
    "        \"\"\"\n",
    "        Returns a boolean vector for each observation in the\n",
    "        dataframe indicating whether it meets the col = value condition\n",
    "        \"\"\"\n",
    "        assert col in self.df.columns\n",
    "        return self.df[col] == value\n",
    "    \n",
    "    \n",
    "    def sample_indices(self, indices, n):\n",
    "        \"\"\"Returns a SRR of the indices provided\n",
    "        \"\"\"\n",
    "        return np.random.choice(indices, n, replace=True)\n",
    "    \n",
    "    \n",
    "    def compute_JS(self, index_A, index_B, base=2):\n",
    "        \"\"\"\n",
    "        Computes the mean pair-wise JS divergence and associated CI\n",
    "        between indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p_A = self.topic_preds[index_A]\n",
    "        p_B = self.topic_preds[index_B]\n",
    "        js_list = [jensenshannon(p, q, base) for p, q in zip(p_A, p_B)]\n",
    "        \n",
    "        return mean_CI(js_list)\n",
    "        \n",
    "        \n",
    "    def compute_HH(self, index):\n",
    "        \"\"\"\n",
    "        Computes the mean HH index and associated CI between\n",
    "        indices in index_A and indices in index_B\n",
    "        \"\"\"\n",
    "        p = self.topic_preds[index]\n",
    "        hh_list = [hh_index(q) for q in p]\n",
    "        \n",
    "        return mean_CI(hh_list)\n",
    "          \n",
    "    \n",
    "    def intra_party_divergence(self, party, subject, n):\n",
    "        \"\"\"\n",
    "        Computes the intraparty divergence for a given party \n",
    "        on a given subject\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - party  : (str) party of interest\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find party indicies on the subject\n",
    "        party_index = self.index[self.bool_subset(PARTY, party) & \n",
    "                                 self.bool_subset(SUB_KEY, subject)]\n",
    "        \n",
    "        # Return none if there are fewer than 2 speakers\n",
    "        if self.df.loc[party_index][SPEAKER].nunique() < 2:\n",
    "            return None\n",
    "        \n",
    "        # Sample index pairs\n",
    "        index_AB = []\n",
    "        while len(index_AB) < n:\n",
    "            a_b = self.sample_indices(party_index, n=2)\n",
    "            # include samples whose speakers are different\n",
    "            if self.df.loc[a_b][SPEAKER].nunique() == 2:\n",
    "                index_AB.append(a_b)\n",
    "        \n",
    "        index_AB = np.asarray(index_AB)\n",
    "        assert index_AB.shape == (n, 2)\n",
    "        \n",
    "        # get indices for each group\n",
    "        index_A, index_B = index_AB[:,0], index_AB[:,1]\n",
    "        \n",
    "        return self.compute_JS(index_A, index_B)\n",
    "    \n",
    "    \n",
    "    def inter_party_divergence(self, subject, n):\n",
    "        \"\"\"\n",
    "        Computes the inter party JS divergence between Republicans and Democrats\n",
    "        on a given subject\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where\n",
    "        - 0 is the mean divergence point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find R and D indicies on the subject\n",
    "        is_subject = self.bool_subset(SUB_KEY, subject)\n",
    "        index_R = self.index[self.bool_subset(PARTY, REP) & is_subject]\n",
    "        index_D = self.index[self.bool_subset(PARTY, DEM) & is_subject]\n",
    "        \n",
    "        # return None if indices are insufficient\n",
    "        if len(index_R)==0 or len(index_D)==0:\n",
    "            return None\n",
    "        \n",
    "        # sample \n",
    "        samp_index_R = self.sample_indices(index_R, n)\n",
    "        samp_index_D = self.sample_indices(index_D, n)\n",
    "    \n",
    "        return self.compute_JS(index_R, index_D)\n",
    "    \n",
    "    \n",
    "    def intra_party_hh(self, party, subject, n=None):\n",
    "        \"\"\"\n",
    "        Estimates the average Herfindahlâ€“Hirschman Index of all \n",
    "        party members in a party on a given subject\n",
    "        \n",
    "        Args:\n",
    "        - subject: (str) subject to examine\n",
    "        - party  : (str) party of interest\n",
    "        - n      : (int) sample size\n",
    "        \n",
    "        Returns: a numpy array of length 3, where index...\n",
    "        - 0 is the mean index point estimate:\n",
    "        - 1 is the lower bound of a 95% CI\n",
    "        - 2 is the upper bound of a 95% CI\n",
    "        \"\"\"\n",
    "        # ensure that the topic predictions exist\n",
    "        if self.topic_preds is None:\n",
    "            self.predict_topics()\n",
    "        \n",
    "        # find party indicies on the subject\n",
    "        party_index = self.index[self.bool_subset(PARTY, party) & \n",
    "                                 self.bool_subset(SUB_KEY, subject)]\n",
    "        \n",
    "        if n is None:\n",
    "            samp_index = party_index\n",
    "        else:\n",
    "            samp_index = self.sample_indices(party_index, n)\n",
    "            \n",
    "        return self.compute_HH(samp_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = RMN_Analyzer(rmn, df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 10000\n",
      "n_batches 23\n",
      "substracting remainder\n",
      "23/23 [==============================] - 10s 433ms/step\n"
     ]
    }
   ],
   "source": [
    "analyzer.predict_topics()\n",
    "# analyzer.topic_preds = topics_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimate': 0.07783780329807893,\n",
       " 'lower': 0.07721183367236817,\n",
       " 'upper': 0.0784637729237897}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.inter_party_divergence('money', 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimate': 0.07776822350949786,\n",
       " 'lower': 0.07738263003089346,\n",
       " 'upper': 0.07815381698810225}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.intra_party_divergence('R', 'money',  10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimate': 0.07784643726544932,\n",
       " 'lower': 0.07745109434470905,\n",
       " 'upper': 0.07824178018618959}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.intra_party_divergence('D', 'money',  10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimate': 0.010901829227805138,\n",
       " 'lower': 0.010883876027584448,\n",
       " 'upper': 0.01091978384363614}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.intra_party_hh('R', 'money', n=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
