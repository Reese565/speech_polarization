{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Baseline of Baselines: LDA on speaker bigrams in the 111th Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up file system\n",
    "data_path = \"gs://rwc1/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not informative, os can't detect file existence in a gs bucket.\n",
    "os.path.isfile(os.path.join(data_path, \"hein-bound/byspeaker_2gram_110.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run these you need to have the pythonic file system installed on your vm: \n",
    "`pip install gcsfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import master vocaulary with bigram classifications\n",
    "bigrams_master = bigrams = pd.read_csv(os.path.join(data_path, \"vocabulary/master_list.txt\"), sep = \"|\")\n",
    "\n",
    "# improt speaker bigram for session X\n",
    "speaker_bigrams = pd.read_csv(os.path.join(data_path, \"hein-bound/byspeaker_2gram_111.txt\"), sep = \"|\")\n",
    "\n",
    "# import speaker map for session X\n",
    "speaker_map = pd.read_csv(os.path.join(data_path, \"hein-bound/111_SpeakerMap.txt\"), sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rwc1/data/vocabulary/master_list.txt\n",
      "gs://rwc1/data/hein-bound/byspeaker_2gram_110.txt\n"
     ]
    }
   ],
   "source": [
    "# Note that the files are being retrieved from here:\n",
    "print(os.path.join(data_path, \"vocabulary/master_list.txt\"))\n",
    "print(os.path.join(data_path, \"hein-bound/byspeaker_2gram_111.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest should be the same because we are computing in memory hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6822118, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>_classify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0 0</td>\n",
       "      <td>bad_syntax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0 00</td>\n",
       "      <td>bad_syntax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0 000</td>\n",
       "      <td>bad_syntax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0 0000</td>\n",
       "      <td>bad_syntax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0 00000</td>\n",
       "      <td>bad_syntax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    phrase   _classify\n",
       "0      0 0  bad_syntax\n",
       "1     0 00  bad_syntax\n",
       "2    0 000  bad_syntax\n",
       "3   0 0000  bad_syntax\n",
       "4  0 00000  bad_syntax"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bigrams_master.shape)\n",
    "bigrams_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10467042, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>phrase</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111113931</td>\n",
       "      <td>0 01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111113931</td>\n",
       "      <td>0 fervent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111113931</td>\n",
       "      <td>0 hope</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111113931</td>\n",
       "      <td>0 sugar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111113931</td>\n",
       "      <td>000 20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speakerid     phrase  count\n",
       "0  111113931       0 01      1\n",
       "1  111113931  0 fervent      1\n",
       "2  111113931     0 hope      1\n",
       "3  111113931    0 sugar      1\n",
       "4  111113931     000 20      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(speaker_bigrams.shape)\n",
    "speaker_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112550, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>district</th>\n",
       "      <th>nonvoting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111120160</td>\n",
       "      <td>1110000007</td>\n",
       "      <td>LARSON</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>H</td>\n",
       "      <td>CT</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111117010</td>\n",
       "      <td>1110000009</td>\n",
       "      <td>PENCE</td>\n",
       "      <td>MIKE</td>\n",
       "      <td>H</td>\n",
       "      <td>IN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>6.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111118060</td>\n",
       "      <td>1110000013</td>\n",
       "      <td>BOEHNER</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>8.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111120780</td>\n",
       "      <td>1110000014</td>\n",
       "      <td>PELOSI</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>H</td>\n",
       "      <td>CA</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>8.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111119830</td>\n",
       "      <td>1110000015</td>\n",
       "      <td>HOYER</td>\n",
       "      <td>STENY</td>\n",
       "      <td>H</td>\n",
       "      <td>MD</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>5.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speakerid   speech_id lastname firstname chamber state gender party  \\\n",
       "0  111120160  1110000007   LARSON      JOHN       H    CT      M     D   \n",
       "1  111117010  1110000009    PENCE      MIKE       H    IN      M     R   \n",
       "2  111118060  1110000013  BOEHNER      JOHN       H    OH      M     R   \n",
       "3  111120780  1110000014   PELOSI     NANCY       H    CA      F     D   \n",
       "4  111119830  1110000015    HOYER     STENY       H    MD      M     D   \n",
       "\n",
       "   district nonvoting  \n",
       "0       1.0    voting  \n",
       "1       6.0    voting  \n",
       "2       8.0    voting  \n",
       "3       8.0    voting  \n",
       "4       5.0    voting  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(speaker_map.shape)\n",
    "speaker_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 55163, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e29703734ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# line 55163 removed extar pipe \"vote. (|) 1116 Messrs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m session_speeches = bigrams = pd.read_csv(\n\u001b[0;32m----> 4\u001b[0;31m     os.path.join(data_path, \"hein-bound/speeches_111.txt\"), sep = \"|\", encoding = \"ISO-8859-1\")\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 55163, saw 3\n"
     ]
    }
   ],
   "source": [
    "# import speeches from session X\n",
    "# line 55163 removed extar pipe \"vote. (|) 1116 Messrs\"\n",
    "session_speeches = bigrams = pd.read_csv(\n",
    "    os.path.join(data_path, \"hein-bound/speeches_111.txt\"), sep = \"|\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session_speeches.shape)\n",
    "session_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_bigrams dim (10467042, 3)\n",
      "bigrams_master dim (6822118, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"session_bigrams dim\", speaker_bigrams.shape)\n",
    "print(\"bigrams_master dim\", bigrams_master.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating valid session bigrams\n",
    "session_bigrams = (speaker_bigrams\n",
    "                   .merge(bigrams_master, how = \"inner\", on =\"phrase\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_session_bigrams = session_bigrams[session_bigrams['_classify'] == \"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>phrase</th>\n",
       "      <th>count</th>\n",
       "      <th>_classify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>111113931</td>\n",
       "      <td>abil produc</td>\n",
       "      <td>1</td>\n",
       "      <td>vocab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16136</th>\n",
       "      <td>111114091</td>\n",
       "      <td>abil produc</td>\n",
       "      <td>2</td>\n",
       "      <td>vocab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>111114101</td>\n",
       "      <td>abil produc</td>\n",
       "      <td>1</td>\n",
       "      <td>vocab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>111115360</td>\n",
       "      <td>abil produc</td>\n",
       "      <td>1</td>\n",
       "      <td>vocab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>111116310</td>\n",
       "      <td>abil produc</td>\n",
       "      <td>2</td>\n",
       "      <td>vocab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       speakerid       phrase  count _classify\n",
       "16135  111113931  abil produc      1     vocab\n",
       "16136  111114091  abil produc      2     vocab\n",
       "16137  111114101  abil produc      1     vocab\n",
       "16138  111115360  abil produc      1     vocab\n",
       "16139  111116310  abil produc      2     vocab"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_session_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration ignoring phrase counts\n",
    "# speaker_phrases = valid_session_bigrams.groupby('speakerid')['phrase'].apply(lambda x: \"[%s]\" % ', '.join(x))\n",
    "speaker_phrases = valid_session_bigrams.groupby('speakerid')['phrase'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speakerid\n",
       "111113931    [abil produc, abil secur, abil strike, abl ass...\n",
       "111113951    [accomplish thing, accord cbo, across aisl, ac...\n",
       "111113981    [abl provid, abus practic, accord cbo, account...\n",
       "111114011    [abl provid, abort right, academ institut, acc...\n",
       "111114021    [academ standard, accomplish just, achiev awar...\n",
       "111114091    [abil produc, abl provid, abl us, abus practic...\n",
       "111114101    [abil produc, abl contribut, abl provid, accom...\n",
       "111114121    [academ success, accept premis, accord cbo, ac...\n",
       "111114171    [abl assess, accomplish just, accomplish thing...\n",
       "111114321    [abl effect, abl provid, accomplish busi, achi...\n",
       "111114331    [accomplish jurist, across america, across cou...\n",
       "111114451    [baghdad iraq, better credit, bill also, commu...\n",
       "111114511    [abl contribut, abort right, abund natur, acad...\n",
       "111114561    [abl provid, account offic, across aisl, acros...\n",
       "111114621    [account offic, across countri, across state, ...\n",
       "111114651    [across state, admir much, advoc behalf, arm f...\n",
       "111114751    [accord cbo, account use, across america, acro...\n",
       "111114941    [across aisl, across america, across countri, ...\n",
       "111115280    [addit feder, address issu, also like, altern ...\n",
       "111115290    [across america, across countri, air forc, als...\n",
       "111115300    [account offic, across america, across countri...\n",
       "111115310    [academ institut, across countri, across natio...\n",
       "111115320    [across countri, across nation, across state, ...\n",
       "111115330    [abl provid, accord cbo, across aisl, across c...\n",
       "111115340    [across aisl, across countri, american peopl, ...\n",
       "111115351    [act like, among us, civil right, colleagu kno...\n",
       "111115360    [abil produc, abl provid, abund natur, across ...\n",
       "111115370    [across countri, across nation, addit resourc,...\n",
       "111115380    [across countri, address issu, air forc, ameri...\n",
       "111115390    [achiev award, across america, across countri,...\n",
       "                                   ...                        \n",
       "111121500    [academi famili, account offic, acquisit progr...\n",
       "111121510    [across countri, air forc, also help, arm forc...\n",
       "111121520    [academ success, achiev award, across aisl, ac...\n",
       "111121530    [account offic, across america, across countri...\n",
       "111121541    [accept premis, accord cbo, account use, acros...\n",
       "111121550    [across countri, also help, american famili, a...\n",
       "111121570    [abus practic, academ institut, academ perform...\n",
       "111121590    [academ success, across countri, also help, am...\n",
       "111121600    [across america, across countri, across state,...\n",
       "111121610    [abil produc, across america, across countri, ...\n",
       "111121620    [abus practic, account offic, across countri, ...\n",
       "111121630    [account offic, across countri, across nation,...\n",
       "111121640    [account offic, achiev award, achiev medal, ac...\n",
       "111121651    [accept offer, account offic, achiev award, ac...\n",
       "111121670    [also like, america great, american famili, am...\n",
       "111121680    [abil produc, achiev award, across aisl, acros...\n",
       "111121690    [across aisl, across america, across countri, ...\n",
       "111121700    [abus practic, academ perform, academ success,...\n",
       "111121720    [across america, across countri, address impor...\n",
       "111121730    [across countri, afghanistan member, airborn d...\n",
       "111121740    [account offic, across aisl, across america, a...\n",
       "111121760    [academ success, achiev past, across aisl, acr...\n",
       "111121791    [abil secur, abl effect, abus practic, access ...\n",
       "111121800    [abl provid, accomplish thing, across countri,...\n",
       "111121820    [activ member, air forc, also like, around wor...\n",
       "111121840    [account offic, across countri, across nation,...\n",
       "111121930    [abl provid, across countri, agenc congress, a...\n",
       "111121940    [abund natur, academ institut, accomplish just...\n",
       "111121950    [across america, across countri, across nation...\n",
       "111121960    [abl provid, academ success, across countri, a...\n",
       "Name: phrase, Length: 551, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_phrases#.apply(len).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abil produc\n",
      "1 abil secur\n",
      "2 abil strike\n",
      "3 abl assess\n",
      "4 abl contribut\n",
      "5 abl effect\n",
      "6 abl provid\n",
      "7 abl us\n",
      "8 abort right\n",
      "9 abroad carri\n",
      "10 abund natur\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(speaker_phrases)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in speaker_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=100, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"need victim\" + 0.000*\"communiti safeti\" + 0.000*\"congress commiss\" + 0.000*\"maintain system\" + 0.000*\"help survivor\" + 0.000*\"state dc\" + 0.000*\"illeg trade\" + 0.000*\"servic crime\" + 0.000*\"campus public\" + 0.000*\"year rebuild\"\n",
      "Topic: 1 \n",
      "Words: 0.000*\"troubl troubl\" + 0.000*\"head execut\" + 0.000*\"give thing\" + 0.000*\"japan korea\" + 0.000*\"like illeg\" + 0.000*\"told thing\" + 0.000*\"spend rememb\" + 0.000*\"bill hadnt\" + 0.000*\"elect that\" + 0.000*\"regul theyr\"\n",
      "Topic: 2 \n",
      "Words: 0.000*\"engin depart\" + 0.000*\"among strongest\" + 0.000*\"colleg largest\" + 0.000*\"servic extraordinari\" + 0.000*\"sad learn\" + 0.000*\"becom ever\" + 0.000*\"firefight everi\" + 0.000*\"colombia remain\" + 0.000*\"leftbehind parent\" + 0.000*\"gave nation\"\n",
      "Topic: 3 \n",
      "Words: 0.000*\"secur imper\" + 0.000*\"impact unit\" + 0.000*\"understand occur\" + 0.000*\"world requir\" + 0.000*\"stryker vehicl\" + 0.000*\"author servic\" + 0.000*\"iran becom\" + 0.000*\"fuel tank\" + 0.000*\"support must\" + 0.000*\"area natur\"\n",
      "Topic: 4 \n",
      "Words: 0.000*\"goal clean\" + 0.000*\"assist requir\" + 0.000*\"environ import\" + 0.000*\"clear economi\" + 0.000*\"year dramat\" + 0.000*\"popul one\" + 0.000*\"preserv wildlif\" + 0.000*\"ago congression\" + 0.000*\"erad poverti\" + 0.000*\"chair transport\"\n",
      "Topic: 5 \n",
      "Words: 0.000*\"took nation\" + 0.000*\"competit option\" + 0.000*\"tax design\" + 0.000*\"watch offic\" + 0.000*\"fast peopl\" + 0.000*\"dollar issu\" + 0.000*\"told current\" + 0.000*\"lead folk\" + 0.000*\"percent wors\" + 0.000*\"go strike\"\n",
      "Topic: 6 \n",
      "Words: 0.000*\"time juvenil\" + 0.000*\"product proper\" + 0.000*\"safe includ\" + 0.000*\"troubl youth\" + 0.000*\"access effect\" + 0.000*\"repeal flaw\" + 0.000*\"align stem\" + 0.000*\"great possess\" + 0.000*\"vote brother\" + 0.000*\"mandat unit\"\n",
      "Topic: 7 \n",
      "Words: 0.000*\"retrofit home\" + 0.000*\"got kid\" + 0.000*\"america cours\" + 0.000*\"american million\" + 0.000*\"made payment\" + 0.000*\"time attack\" + 0.000*\"cell technolog\" + 0.000*\"present danger\" + 0.000*\"survivor unit\" + 0.000*\"care took\"\n",
      "Topic: 8 \n",
      "Words: 0.000*\"health care\" + 0.000*\"urg colleagu\" + 0.000*\"american peopl\" + 0.000*\"men women\" + 0.000*\"year ago\" + 0.000*\"make sure\" + 0.000*\"colleagu support\" + 0.000*\"side aisl\" + 0.000*\"across countri\" + 0.000*\"creat job\"\n",
      "Topic: 9 \n",
      "Words: 0.000*\"manufactur anyth\" + 0.000*\"general comment\" + 0.000*\"particular get\" + 0.000*\"manag everi\" + 0.000*\"document detail\" + 0.000*\"reward bonus\" + 0.000*\"exchang access\" + 0.000*\"global busi\" + 0.000*\"commerc board\" + 0.000*\"probabl china\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_id    111120961\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker with most speeches\n",
    "speaker_map.groupby('speakerid').agg({'speech_id':'count'}).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad_syntax', 'roberts_and_riddicks', 'co-occurring', 'riddicks',\n",
       "       'vocab', 'stopword', 'roberts'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram classes\n",
    "session_bigrams[\"_classify\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session speaker count\n",
    "speaker_map['speakerid'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>district</th>\n",
       "      <th>nonvoting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111120160</td>\n",
       "      <td>LARSON</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>H</td>\n",
       "      <td>CT</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111117010</td>\n",
       "      <td>PENCE</td>\n",
       "      <td>MIKE</td>\n",
       "      <td>H</td>\n",
       "      <td>IN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>6.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111118060</td>\n",
       "      <td>BOEHNER</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>8.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111120780</td>\n",
       "      <td>PELOSI</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>H</td>\n",
       "      <td>CA</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>8.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111119830</td>\n",
       "      <td>HOYER</td>\n",
       "      <td>STENY</td>\n",
       "      <td>H</td>\n",
       "      <td>MD</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>5.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speakerid lastname firstname chamber state gender party  district nonvoting\n",
       "0  111120160   LARSON      JOHN       H    CT      M     D       1.0    voting\n",
       "1  111117010    PENCE      MIKE       H    IN      M     R       6.0    voting\n",
       "2  111118060  BOEHNER      JOHN       H    OH      M     R       8.0    voting\n",
       "3  111120780   PELOSI     NANCY       H    CA      F     D       8.0    voting\n",
       "4  111119830    HOYER     STENY       H    MD      M     D       5.0    voting"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_map.drop(['speech_id'], axis=1).drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
