{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Reshape\n",
    "\n",
    "from embeddings import EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from document import *\n",
    "from subject import subject_keywords\n",
    "from constant import SPEECHES, SPEAKER_MAP, HB_PATH, EMBEDDINGS, DOC_PATH, DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../modeling\")\n",
    "from token_mapping import *\n",
    "from embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_SAMPLE_PATH = os.path.join(DOC_PATH, \"doc-sample/\")\n",
    "docs_df = load_documents(subject_keywords.keys(), DOC_SAMPLE_PATH)\n",
    "N = 100\n",
    "docs_df = docs_df.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    " 'speakerid',\n",
    " 'chamber',\n",
    " 'state',\n",
    " 'gender',\n",
    " 'party',\n",
    " 'congress',\n",
    " 'subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = build_tokenizer_dict(docs_df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party', 'congress', 'subject'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict = build_metadata_dict(docs_df, feature_columns)\n",
    "metadata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(word_index, embeddings_index):\n",
    "\n",
    "    embedding_dim = len(embeddings_index['the'])\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector \n",
    "            \n",
    "    embedding_matrix = embedding_matrix.astype('float16')\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "embedding_file = \"/home/rocassius/gen-data/tools/embbedding_index_50d\"\n",
    "\n",
    "f = open(embedding_file, \"rb\")\n",
    "embeddings_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "embeddings_matrix = build_embedding_matrix(token_dict['document']['token_index'], embeddings_index)#.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party', 'congress', 'subject'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [\n",
    " 'speakerid',\n",
    " 'chamber',\n",
    " 'state',\n",
    " 'gender',\n",
    " 'party',\n",
    " 'congress',\n",
    " 'subject']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_dict(subject_df):\n",
    "    \n",
    "    max_len = WINDOW + 1\n",
    "    \n",
    "    # building tokenizers, word indecies, and train data\n",
    "    speech_tokenizer = Tokenizer()\n",
    "    speech_tokenizer.fit_on_texts(subject_df['speech'].values)\n",
    "    speeches_word_index = speech_tokenizer.word_index\n",
    "    speeches_train = speech_tokenizer.texts_to_sequences(subject_df['speech'].values)\n",
    "    speeches_train_padded = pad_sequences(speeches_train, maxlen=WINDOW + 1, padding=\"post\")\n",
    "    \n",
    "\n",
    "\n",
    "    tokenizers = {}\n",
    "    tokenizers['speech'] = {'tokenizer': speech_tokenizer,\n",
    "                            'train': speeches_train,\n",
    "                            'train_padded': speeches_train_padded,\n",
    "                            'word_index': speeches_word_index}\n",
    "        \n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata_dict(document_df, metadata_columns):\n",
    "\n",
    "    metadata_dict = {}\n",
    "    \n",
    "    for col in metadata_columns:\n",
    "        \n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(document_df[col])\n",
    "    \n",
    "        metadata_dict[col] = {\n",
    "            'tokenizer': tokenizer,\n",
    "            'tokenize': tokenizer.texts_to_sequences,\n",
    "            'token_index': tokenizer.word_index, \n",
    "            'input_dim': len(tokenizer.word_index)}        \n",
    "        \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad(documents, tokenizer, max_span_len):\n",
    "    \"\"\"Tokenize and pad documents using a tokenizer\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer.texts_to_sequences(documents)\n",
    "    padded = pad_sequences(tokenized, maxlen=max_span_len, padding = \"post\")\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_dict(document_df, max_span_len = MIN_TOKENS):\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(document_df['document'])\n",
    "    tokenizer_pad = partial(tokenize_pad, \n",
    "                            tokenizer=tokenizer, \n",
    "                            max_span_len=max_span_len)\n",
    "    \n",
    "#     def tokenize_pad(documents):\n",
    "#         tokenized = tokenizer.texts_to_sequences(documents)\n",
    "#         padded = pad_sequences(tokenized, maxlen=max_span_len, padding = \"post\")\n",
    "#         return padded\n",
    "    \n",
    "    tokenizer_dict = {\n",
    "        'tokenizer': tokenizer, \n",
    "        'tokenize_pad': tokenizer_pad, \n",
    "        'word_index': tokenizer.word_index,\n",
    "        'max_span_length': max_span_len}\n",
    "    \n",
    "    return tokenizer_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = build_metadata_dict(docs_df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict = build_tokenizer_dict(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMN = \"rmn_%s_dir\"\n",
    "MOD = \"model\"\n",
    "ARCH = \"architecture\"\n",
    "ATTR = \"attributes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# constants\n",
    "NUM_TOPICS = 20\n",
    "OPTIMIZER = 'adam'\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.num_topics = NUM_TOPICS\n",
    "        \n",
    "        self.embedding_matrix = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    \n",
    "    def model_loss(self, layer, lamb = 1.0):\n",
    "        \"\"\"Custom loss function to engourage \n",
    "        orthoganality of dictionary matrix R.\"\"\"\n",
    "\n",
    "        R = K.transpose(layer)\n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "\n",
    "            hinge_loss = tf.keras.losses.hinge(y_true, y_pred)\n",
    "\n",
    "            RR_t = K.dot(R, K.transpose(R))\n",
    "            Id_mat = K.eye(self.embedding_dim)\n",
    "\n",
    "            orth_penalty = K.sqrt(K.sum(K.square(RR_t - Id_mat)))\n",
    "\n",
    "            return hinge_loss + lamb*orth_penalty\n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # document span input\n",
    "        vt = Input(shape=(self.tokenizer_dict['max_span_length'], ), name='Span.Input')\n",
    "    \n",
    "        input_layers = [vt]\n",
    "        embedding_layers = [vt]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            \n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concat speaker metadata embeddings\n",
    "        _ht = tf.keras.layers.Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer with softmax activation, (where previous states will eventually be inserted) \n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\", name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   # kernel_regularizer = Orthoganal(),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss(rt))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "    \n",
    "    \n",
    "    def prep_y(self, y):\n",
    "        \"\"\"Returns the average of the vectors in each span of text\n",
    "        \"\"\"\n",
    "        padded_spans = self.tokenizer_dict['tokenize_pad'](y)\n",
    "        vector_spans = self.embedding_matrix[padded_spans].mean(axis=1)\n",
    "        \n",
    "        return vector_spans\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_ids = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                        for col in metadata_dict.keys()]\n",
    "\n",
    "        return metadata_ids\n",
    "        \n",
    "    \n",
    "    def prep_inputs(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        vector_spans = self.prep_y(df['document'])\n",
    "        metadata_ids = self.prep_metadata(df)\n",
    "        inputs = [vector_spans] + metadata_ids\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"\n",
    "        \n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "            \n",
    "        topic_preds = self.topic_model.predict(x=self.prep_inputs(df))\n",
    "        \n",
    "        return topic_preds\n",
    "    \n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.num_topics = None\n",
    "# self.embedding_matrix = None\n",
    "# self.embedding_dim = None\n",
    "\n",
    "# self.tokenizer_dict = None\n",
    "# self.metadata_dict = None\n",
    "# self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RMN()\n",
    "rmn.num_topics = 40\n",
    "rmn.metadata_dict = metadata_dict\n",
    "rmn.tokenizer_dict = tokenizer_dict\n",
    "rmn.embedding_matrix = embeddings_matrix\n",
    "\n",
    "rmn.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = rmn.prep_inputs(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 8.0093\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 7.9710\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 7.9635\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 7.9627\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 7.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b20b14510>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.model.fit(x=inputs, y = inputs[0], batch_size = 1, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn.build_topic_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = rmn.topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12728667e-05, 7.61330057e-06, 6.69647590e-04, ...,\n",
       "        1.71813335e-05, 1.78559560e-06, 4.75757133e-06],\n",
       "       [1.86095385e-05, 1.47195879e-05, 9.28174879e-04, ...,\n",
       "        3.22538181e-05, 4.34476215e-06, 1.09852326e-05],\n",
       "       [1.45056529e-05, 1.07052783e-05, 8.27712880e-04, ...,\n",
       "        2.23721418e-05, 2.69438419e-06, 7.47895729e-06],\n",
       "       ...,\n",
       "       [1.53639830e-05, 1.09285975e-05, 8.41916946e-04, ...,\n",
       "        2.48461438e-05, 2.39745236e-06, 7.06023366e-06],\n",
       "       [1.40847606e-05, 9.70476322e-06, 6.71438698e-04, ...,\n",
       "        2.51738729e-05, 2.33795140e-06, 6.18180275e-06],\n",
       "       [2.75614293e-05, 2.11138104e-05, 1.19606068e-03, ...,\n",
       "        3.66882196e-05, 5.07829600e-06, 1.18210473e-05]], dtype=float32)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'num_topics': rmn.num_topics,\n",
    "    'metadata_dict': rmn.metadata_dict,\n",
    "    'tokenizer_dict': rmn.tokenizer_dict,\n",
    "    'emebedding_matrix': rmn.embedding_matrix\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/rocassius/gen-data/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tokenizer.texts_to_sequences of <keras_preprocessing.text.Tokenizer object at 0x7f4b22495c10>>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.metadata_dict['speakerid']['tokenize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rmn_model_dict'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(model_dict, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rmn_model_dict'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(model_dict, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
