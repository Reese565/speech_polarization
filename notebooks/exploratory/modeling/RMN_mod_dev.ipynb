{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Reshape\n",
    "\n",
    "from embeddings import EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from document import *\n",
    "from subject import subject_keywords\n",
    "from constant import SPEECHES, SPEAKER_MAP, HB_PATH, EMBEDDINGS, DOC_PATH, DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../modeling\")\n",
    "from token_mapping import *\n",
    "from embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_SAMPLE_PATH = os.path.join(DOC_PATH, \"doc-sample/\")\n",
    "docs_df = load_documents(subject_keywords.keys(), DOC_SAMPLE_PATH)\n",
    "N = 100\n",
    "docs_df = docs_df.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    " 'speakerid',\n",
    " 'chamber',\n",
    " 'state',\n",
    " 'gender',\n",
    " 'party',\n",
    " 'congress',\n",
    " 'subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = build_tokenizer_dict(docs_df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party', 'congress', 'subject'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict = build_metadata_dict(docs_df, feature_columns)\n",
    "metadata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "embedding_file = \"/home/rocassius/gen-data/tools/embbedding_index_50d\"\n",
    "\n",
    "f = open(embedding_file, \"rb\")\n",
    "embeddings_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "embeddings_matrix = build_embedding_matrix(token_dict['document']['token_index'], embeddings_index)#.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party', 'congress', 'subject'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [\n",
    " 'speakerid',\n",
    " 'chamber',\n",
    " 'state',\n",
    " 'gender',\n",
    " 'party',\n",
    " 'congress',\n",
    " 'subject']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata_dict(document_df, metadata_columns):\n",
    "\n",
    "    metadata_dict = {}\n",
    "    \n",
    "    for col in metadata_columns:\n",
    "        \n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(document_df[col])\n",
    "    \n",
    "        metadata_dict[col] = {\n",
    "            'tokenizer': tokenizer,\n",
    "            'tokenize': tokenizer.texts_to_sequences,\n",
    "            'token_index': tokenizer.word_index, \n",
    "            'input_dim': len(tokenizer.word_index)}        \n",
    "        \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad(documents, tokenizer, max_span_len):\n",
    "    \"\"\"Tokenize and pad documents using a tokenizer\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.texts_to_sequences(documents)\n",
    "    padded = pad_sequences(tokenized, maxlen=max_span_len, padding = \"post\")\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_dict(document_df, max_span_len = MIN_TOKENS):\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(document_df['document'])\n",
    "    tokenizer_pad = partial(tokenize_pad, tokenizer=tokenizer, max_span_len=max_span_len)\n",
    "    \n",
    "    tokenizer_dict = {\n",
    "        'tokenizer': tokenizer, \n",
    "        'tokenize_pad': tokenizer_pad, \n",
    "        'word_index': tokenizer.word_index,\n",
    "        'max_span_length': max_span_len}\n",
    "    \n",
    "    return tokenizer_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = build_metadata_dict(docs_df, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict = build_tokenizer_dict(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "\n",
    "# constants\n",
    "MAX_SPAN_LENGTH = 50\n",
    "NUM_TOPICS = 20\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "\n",
    "RMN_TAG = \"rmn_%s\"\n",
    "MODEL = \"model.h5\"\n",
    "ARCH = \"architecture\"\n",
    "ATTR = \"attributes\"\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \"\"\"\n",
    "    Class for constructing a Relationship Modeling Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # model attributes\n",
    "        self.num_topics = NUM_TOPICS\n",
    "        self.embedding_matrix = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        # models \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    \n",
    "    def model_loss(self, layer, lamb = 1.0):\n",
    "        \"\"\"Custom loss function to engourage \n",
    "        orthoganality of dictionary matrix R.\"\"\"\n",
    "\n",
    "        R = K.transpose(layer)\n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "\n",
    "            hinge_loss = tf.keras.losses.hinge(y_true, y_pred)\n",
    "\n",
    "            RR_t = K.dot(R, K.transpose(R))\n",
    "            Id_mat = K.eye(self.embedding_dim)\n",
    "\n",
    "            orth_penalty = K.sqrt(K.sum(K.square(RR_t - Id_mat)))\n",
    "\n",
    "            return hinge_loss + lamb*orth_penalty\n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # document span input\n",
    "        vt = Input(shape=(self.tokenizer_dict['max_span_length'], ), name='Span.Input')\n",
    "    \n",
    "        input_layers = [vt]\n",
    "        embedding_layers = [vt]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            \n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concat speaker metadata embeddings\n",
    "        _ht = tf.keras.layers.Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer with softmax activation, (where previous states will eventually be inserted) \n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\", name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   # kernel_regularizer = Orthoganal(),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss(rt))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "    \n",
    "    \n",
    "    def prep_y(self, y):\n",
    "        \"\"\"Returns the average of the vectors in each span of text\n",
    "        \"\"\"\n",
    "        padded_spans = self.tokenizer_dict['tokenize_pad'](y)\n",
    "        vector_spans = self.embedding_matrix[padded_spans].mean(axis=1)\n",
    "        \n",
    "        return vector_spans\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_ids = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                        for col in metadata_dict.keys()]\n",
    "\n",
    "        return metadata_ids\n",
    "        \n",
    "    \n",
    "    def prep_inputs(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        vector_spans = self.prep_y(df['document'])\n",
    "        metadata_ids = self.prep_metadata(df)\n",
    "        inputs = [vector_spans] + metadata_ids\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"\n",
    "        \n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "            \n",
    "        topic_preds = self.topic_model.predict(x=self.prep_inputs(df))\n",
    "        \n",
    "        return topic_preds\n",
    "    \n",
    "    \n",
    "    def fit(self, df, batch_size = BATCH_SIZE, epochs = EPOCHS):\n",
    "        \n",
    "        inputs = self.prep_inputs(df)\n",
    "        y_true = self.prep_y(df['document'])\n",
    "        \n",
    "        self.model.fit(x = inputs, \n",
    "                       y = y_true, \n",
    "                       batch_size = batch_size, \n",
    "                       epochs = epochs)\n",
    "\n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Save the model's weights, architecture and attributes\n",
    "        \"\"\"\n",
    "        \n",
    "        # assemble attribute dictionary\n",
    "        attribute_dict = {\n",
    "            'num_topics': self.num_topics,\n",
    "            'emedding_matrix': self.embedding_matrix,\n",
    "            'tokenizer_dict': self.tokenizer_dict,\n",
    "            'metadata_dict': self.metadata_dict}\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        # save model weights\n",
    "        self.model.save(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # save model architecture\n",
    "        pickle_object(self.model.to_json(), os.path.join(model_path, ARCH))\n",
    "        \n",
    "        # save model attributes\n",
    "        pickle_object(attribute_dict, os.path.join(model_path, ATTR))\n",
    "        \n",
    "        \n",
    "    def load_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Load the model, weights, architecture and attributes from a saved model\n",
    "        \"\"\"\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        \n",
    "        # Load architecture and weights\n",
    "        self.model = model_from_json(load_pickled_object(os.path.join(model_path, ARCH)))\n",
    "        self.model.load_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # load attributes\n",
    "        attributes_dict = load_pickled_object(os.path.join(model_path, ATTR))\n",
    "        \n",
    "        # update attributes\n",
    "        self.num_topics = attributes_dict['num_topics']\n",
    "        self.embedding_matrix = attributes_dict['emedding_matrix']\n",
    "        self.tokenizer_dict = attributes_dict['tokenizer_dict']\n",
    "        self.metadata_dict = attributes_dict['metadata_dict']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = model_from_json(open(modelFile).read())\n",
    "# model.load_weights(os.path.join(os.path.dirname(modelFile), 'model_weights.h5'))\n",
    "\n",
    "\n",
    "# self.num_topics = None\n",
    "# self.embedding_matrix = None\n",
    "# self.embedding_dim = None\n",
    "\n",
    "# self.tokenizer_dict = None\n",
    "# self.metadata_dict = None\n",
    "# self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RMN()\n",
    "rmn.num_topics = 40\n",
    "rmn.metadata_dict = metadata_dict\n",
    "rmn.tokenizer_dict = tokenizer_dict\n",
    "rmn.embedding_matrix = embeddings_matrix\n",
    "\n",
    "rmn.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = rmn.prep_inputs(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 690us/sample - loss: 8.3934\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 622us/sample - loss: 7.9897\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 621us/sample - loss: 7.9868\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 636us/sample - loss: 7.9788\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 635us/sample - loss: 7.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b084a4990>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.model.fit(x=inputs, y = y, batch_size = 5, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn.save_rmn(save_path = \"/home/rocassius/gen-data/models\", \n",
    "             name = \"albertiso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn_2 = RMN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rocassius/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/rocassius/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "rmn_2.load_rmn(name=\"albertiso\", save_path = \"/home/rocassius/gen-data/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.1652193e-04, 1.7910470e-01, 9.8269163e-03, ..., 2.5755103e-04,\n",
       "        7.9114419e-05, 1.5086564e-03],\n",
       "       [8.8874716e-03, 4.4744827e-02, 3.8583890e-02, ..., 1.8278451e-03,\n",
       "        1.3055729e-03, 5.9237620e-03],\n",
       "       [7.0628920e-03, 4.6495214e-02, 3.6701795e-02, ..., 1.2913044e-03,\n",
       "        1.1500251e-03, 4.5245788e-03],\n",
       "       ...,\n",
       "       [9.9604228e-04, 1.3856687e-01, 1.3478625e-02, ..., 2.4042113e-04,\n",
       "        1.0308698e-04, 1.5409698e-03],\n",
       "       [4.9326029e-03, 5.3708877e-02, 2.9198263e-02, ..., 7.0617168e-04,\n",
       "        5.6763762e-04, 2.7723091e-03],\n",
       "       [1.8442660e-03, 1.4638846e-01, 1.5711518e-02, ..., 5.8922678e-04,\n",
       "        1.8944810e-04, 2.6625546e-03]], dtype=float32)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn_2.predict_topics(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_object(x, file_name):\n",
    "    \"\"\"\n",
    "    Helper function for pickling an object\n",
    "    \"\"\"\n",
    "    outfile = open(file_name, \"wb\")\n",
    "    pickle.dump(x, outfile)\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickled_object(file_name):\n",
    "    \"\"\"\n",
    "    Helper function for loading a pickled object\n",
    "    \"\"\"\n",
    "    infile = open(file_name, \"rb\")\n",
    "    x = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rocassius/gen-data/models'"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/rocassius/gen-data/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = \"hello there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_object(model_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'rmn_model_dict'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(model_dict, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rmn_model_dict'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(model_dict, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
