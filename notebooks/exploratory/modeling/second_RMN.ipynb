{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RMN"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../scripts/assembly\")\n",
    "from session_speaker_assembly import *\n",
    "from preprocess import *\n",
    "from document import *\n",
    "from constant import SPEECHES, SPEAKER_MAP, HB_PATH, EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111113931</td>\n",
       "      <td>S</td>\n",
       "      <td>IN</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111113951</td>\n",
       "      <td>S</td>\n",
       "      <td>UT</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111113981</td>\n",
       "      <td>S</td>\n",
       "      <td>MO</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111114011</td>\n",
       "      <td>S</td>\n",
       "      <td>KS</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111114021</td>\n",
       "      <td>S</td>\n",
       "      <td>KY</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>111121840</td>\n",
       "      <td>H</td>\n",
       "      <td>NV</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>111121930</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>111121940</td>\n",
       "      <td>H</td>\n",
       "      <td>FL</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>111121950</td>\n",
       "      <td>H</td>\n",
       "      <td>AZ</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>111121960</td>\n",
       "      <td>H</td>\n",
       "      <td>NV</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     speakerid chamber state gender party\n",
       "0    111113931       S    IN      M     D\n",
       "1    111113951       S    UT      M     R\n",
       "2    111113981       S    MO      M     R\n",
       "3    111114011       S    KS      M     R\n",
       "4    111114021       S    KY      M     R\n",
       "..         ...     ...   ...    ...   ...\n",
       "552  111121840       H    NV      M     R\n",
       "553  111121930       H    IL      M     D\n",
       "554  111121940       H    FL      M     D\n",
       "555  111121950       H    AZ      F     D\n",
       "556  111121960       H    NV      F     D\n",
       "\n",
       "[557 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = 111\n",
    "speak_map_cols = ['speakerid','chamber','state','gender','party']\n",
    "\n",
    "speaker_map_df = pd.read_csv(os.path.join(HB_PATH,SPEAKER_MAP % session), sep = '|')[speak_map_cols]\n",
    "speaker_map_df = speaker_map_df.groupby('speakerid').last().reset_index()\n",
    "speaker_map_df"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../modeling\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../modeling\")\n",
    "from token_mapping import ohe_attribures, build_tokenizer_dict, build_metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = 111\n",
    "# speak_map_cols = ['speakerid','chamber','state','gender']\n",
    "\n",
    "# speaker_map_df = pd.read_csv(os.path.join(HB_PATH,SPEAKER_MAP % session), sep = '|')[speak_map_cols]\n",
    "# speaker_map_df = speaker_map_df.groupby('speakerid').last().reset_index()\n",
    "# speaker_map_df"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>pay their bills and keep their homes. small bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111120160.0</td>\n",
       "      <td>honest and fair prosperity for the many. not j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111121410.0</td>\n",
       "      <td>rarely has our great Nation faced such grave c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111120961.0</td>\n",
       "      <td>together. With the middle class struggling to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111114091.0</td>\n",
       "      <td>amount of pride in noting that in each of thes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     speakerid                                             speech\n",
       "0  111118060.0  pay their bills and keep their homes. small bu...\n",
       "1  111120160.0  honest and fair prosperity for the many. not j...\n",
       "2  111121410.0  rarely has our great Nation faced such grave c...\n",
       "3  111120961.0  together. With the middle class struggling to ...\n",
       "4  111114091.0  amount of pride in noting that in each of thes..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_df = subject_docs(session = session, path = HB_PATH, subject = \"health\", min_len_tokens=100)\n",
    "subject_df.head()"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_df = subject_docs(session = session,\n",
    "#                           speech_path = HB_PATH,\n",
    "#                           min_tokens=MIN_TOKENS,\n",
    "#                           span_finder=make_span_finder(\"health\", WINDOW))\n",
    "# subject_df.head()"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# megre speech and speaker metadata\n",
    "session_df = subject_df.merge(speaker_map_df, how = 'inner', on = 'speakerid')\n",
    "\n",
    "# ensure proper merge\n",
    "assert(subject_df.shape[0]==session_df.shape[0])\n",
    "assert(subject_df.shape[1] + len(speak_map_cols) - 1 == session_df.shape[1])"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = pd.read_csv('../../data/gen-docs/documents_health.txt', sep = '|')\n",
    "feature_columns = subject_df.columns.drop('speech')\n",
    "subject_df = ohe_attribures(subject_df)\n",
    "token_dict = build_tokenizer_dict(subject_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # megre speech and speaker metadata\n",
    "# session_df = subject_df.merge(speaker_map_df, how = 'inner', on = 'speakerid')\n",
    "\n",
    "# # ensure proper merge\n",
    "# assert(subject_df.shape[0]==session_df.shape[0])\n",
    "# assert(subject_df.shape[1] + len(speak_map_cols) - 1 == session_df.shape[1])"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>111113931.0</th>\n",
       "      <th>111113951.0</th>\n",
       "      <th>111113981.0</th>\n",
       "      <th>111114011.0</th>\n",
       "      <th>...</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>pay their bills and keep their homes. small bu...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>of is contained in the bill. And we also belie...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>The bill is supposed to be about creating jobs...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>administration and enforcement team consisting...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>President and our Democrat colleagues here in ...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13297</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>Asset Relief Program. TARP. repeals TARP. repe...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13298</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>on here how it was created. and it indicates n...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13299</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>You wonder why this component would be in a he...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>party. But. instead. we have proposed positive...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13301</td>\n",
       "      <td>111116870.0</td>\n",
       "      <td>infrastructure of America in this bill is goin...</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13302 rows × 605 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         speakerid                                             speech chamber  \\\n",
       "0      111118060.0  pay their bills and keep their homes. small bu...       H   \n",
       "1      111118060.0  of is contained in the bill. And we also belie...       H   \n",
       "2      111118060.0  The bill is supposed to be about creating jobs...       H   \n",
       "3      111118060.0  administration and enforcement team consisting...       H   \n",
       "4      111118060.0  President and our Democrat colleagues here in ...       H   \n",
       "...            ...                                                ...     ...   \n",
       "13297  111119610.0  Asset Relief Program. TARP. repeals TARP. repe...       H   \n",
       "13298  111119610.0  on here how it was created. and it indicates n...       H   \n",
       "13299  111119610.0  You wonder why this component would be in a he...       H   \n",
       "13300  111119610.0  party. But. instead. we have proposed positive...       H   \n",
       "13301  111116870.0  infrastructure of America in this bill is goin...       H   \n",
       "\n",
       "      state gender party  111113931.0  111113951.0  111113981.0  111114011.0  \\\n",
       "0        OH      M     R            0            0            0            0   \n",
       "1        OH      M     R            0            0            0            0   \n",
       "2        OH      M     R            0            0            0            0   \n",
       "3        OH      M     R            0            0            0            0   \n",
       "4        OH      M     R            0            0            0            0   \n",
       "...     ...    ...   ...          ...          ...          ...          ...   \n",
       "13297    GA      M     R            0            0            0            0   \n",
       "13298    GA      M     R            0            0            0            0   \n",
       "13299    GA      M     R            0            0            0            0   \n",
       "13300    GA      M     R            0            0            0            0   \n",
       "13301    IL      M     R            0            0            0            0   \n",
       "\n",
       "       ...  VT  WA  WI  WV  WY  F  M  D  I  R  \n",
       "0      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "1      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "2      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "3      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "4      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "...    ...  ..  ..  ..  ..  .. .. .. .. .. ..  \n",
       "13297  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13298  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13299  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13300  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13301  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "\n",
       "[13302 rows x 605 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset data for prelim building\n",
    "size = session_df.shape[0]\n",
    "sample_df = session_df.iloc[:size,:]\n",
    "\n",
    "sample_df['speakerid'] = sample_df['speakerid'].astype(str)\n",
    "\n",
    "# one-hot-encode speaker metadata\n",
    "for col in speak_map_cols:\n",
    "    sample_df = pd.concat([sample_df,pd.get_dummies(sample_df[col])], axis = 1)\n",
    "    \n",
    "\n",
    "sample_df"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # subset data for prelim building\n",
    "# size = subject_df.shape[0]\n",
    "# sample_df = subject_df.iloc[:size,:]\n",
    "\n",
    "# sample_df['speakerid'] = sample_df['speakerid'].astype(str)\n",
    "\n",
    "# # one-hot-encode speaker metadata\n",
    "# for col in feature_columns[:3]:\n",
    "#     sample_df = pd.concat([sample_df,pd.get_dummies(sample_df[col])], axis = 1)\n",
    "    \n",
    "\n",
    "# sample_df"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker count: 536\n"
     ]
    }
   ],
   "source": [
    "sample_speakers = sample_df['speakerid'].unique()\n",
    "print('speaker count:', len(sample_speakers))"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_speakers = sample_df['speakerid'].unique()\n",
    "# print('speaker count:', len(sample_speakers))"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 535 Members of Congress. 100 serve in the U.S. Senate and 435 serve in the U.S. House of Representatives. A length of 50 suggests that nearly everyone commented on \"health\" (in a speech of more than 50 words) at some point."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building tokenizers, word indecies, and train data\n",
    "\n",
    "speech_tokenizer = Tokenizer()\n",
    "speech_tokenizer.fit_on_texts(sample_df['speech'].values)\n",
    "speeches_word_index = speech_tokenizer.word_index\n",
    "\n",
    "tokenizers = {}\n",
    "tokenizers['speech'] = {'tokenizer': speech_tokenizer,\n",
    "                        'train': speech_tokenizer.texts_to_sequences(sample_df['speech'].values),\n",
    "                        'word_index': speeches_word_index}\n",
    "\n",
    "for col in speak_map_cols:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sample_df[col].values)\n",
    "    tokenizers[col] = {}\n",
    "    tokenizers[col]['train'] = tokenizer.texts_to_sequences(sample_df[col].values)\n",
    "    tokenizers[col]['word_index'] = tokenizer.word_index\n",
    "    tokenizers[col]['tokenizer'] = tokenizer"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # building tokenizers, word indecies, and train data\n",
    "\n",
    "# speech_tokenizer = Tokenizer()\n",
    "# speech_tokenizer.fit_on_texts(sample_df['speech'].values)\n",
    "# speeches_word_index = speech_tokenizer.word_index\n",
    "\n",
    "# tokenizers = {}\n",
    "# tokenizers['speech'] = {'tokenizer': speech_tokenizer,\n",
    "#                         'train': speech_tokenizer.texts_to_sequences(sample_df['speech'].values),\n",
    "#                         'word_index': speeches_word_index}\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(sample_df[col].values)\n",
    "#     tokenizers[col] = {}\n",
    "#     tokenizers[col]['train'] = tokenizer.texts_to_sequences(sample_df[col].values)\n",
    "#     tokenizers[col]['word_index'] = tokenizer.word_index\n",
    "#     tokenizers[col]['tokenizer'] = tokenizer\n",
    "    "
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17985"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_word_index = token_dict['speech']['word_index']\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "vocab_size = len(speeches_word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13302"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_train = tokenizers['speech']['train']\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_train = token_dict['speech']['train']\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "len(speeches_train)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = WINDOW_DEFAULT + 1\n",
    "speeches_train_padded = pad_sequences(speeches_train, maxlen=max_len, padding=\"post\")"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = WINDOW + 1\n",
    "# speeches_train_padded = pad_sequences(speeches_train, maxlen=max_len, padding=\"post\")"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 155,   25,  297, ...,    0,    0,    0],\n",
       "       [   5,   10, 2508, ...,    0,    0,    0],\n",
       "       [   1,   20,   10, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  29, 1615,  183, ...,    0,    0,    0],\n",
       "       [ 601,   38,  595, ...,    0,    0,    0],\n",
       "       [ 444,    5,  131, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_train_padded = token_dict['speech']['train_padded']\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "speeches_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the sentences need to be in integer-tokenized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Iyyer et el.\n",
    "\n",
    "\"Each input to the RMN is a tuple that contains identifiers for a book and two character, as well as the spans corresponding to their relationship: $(b, c_1, c_2, S_{c_1,c_2})$. Given one such input, our objective is to reconstruct $S_(c_1,c_2)$ using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needs for Baseline goal\n",
    "\n",
    "Let...\n",
    "* $s_{v_t}$ be the $t_{th}$ span of text in the span set $S_{c_1,c_2}$\n",
    "* $v_{s_t}$ be the vector that results from taking the element-wise average of the word vectors in $s_{v_t}$\n",
    "* $C$ be the set metadata embeddings\n",
    "* $m_{t,c}$ be the metadata embeddings vector for metadata $c$ with \n",
    "* $d$ be the dimension of the embedding\n",
    "* $k$ be the number of decsriptors\n",
    "\n",
    "\n",
    "Compute Sequence: Given $s_{v_t}$, do the following steps:\n",
    "1. compute avg speech vector, $v_{s_t}$,\n",
    "    * $v_{s_t} \\in \\mathbb{R}^{d}$\n",
    "2. concat avg span and metadate embeddings\n",
    "    * $ m_{t,c} \\in \\mathbb{R}^{d}$\n",
    "    * [$v_{s_t}; m_{t,1};...; m_{t,|C|}$]\n",
    "2. compute hidden state with Relu activation: \n",
    "    * $h_t =  relu \\space (W_h \\cdot [v_{s_t}; m_{t,1};...; m_{t,|C|}])$\n",
    "    * $W_h \\in \\mathbb{R}^{d \\times (d + d|C|)}$ \n",
    "    * $h_t \\in  \\mathbb{R}^{d}$\n",
    "3. get distribution over topics using another hidden layer: \n",
    "    * $d_t = softmax \\space (W_d \\cdot h_t)$\n",
    "    * $W_d \\in  \\mathbb{R}^{k \\times d}$\n",
    "    * $d_t \\in  \\mathbb{R}^{k}$\n",
    "    * $d_{t,i} \\in (0,1) \\space \\forall i$ \n",
    "4. recompose original sentence using the distribution over descriptors and the descriptor matrix:\n",
    "    * $r_t = R^Td_t$\n",
    "    * $R^T \\in \\mathbb{R}^{d \\times k}$\n",
    "    * $r_t \\in \\mathbb{R}^{d}$\n",
    "5. score distance between $r_t$ and $v_{s_t}$\n",
    "    * $distance = dist(r_t, v_{s_t})$\n",
    "    \n",
    "    \n",
    "#### Notes on implementing it with keras\n",
    "Every step that uses a matrix multiplication above can be implemented in keras using a dense layer, formatted like this:\n",
    "* `h = keras.layers.Dense(units = a, input_shape = (b, ), activation= \"the_activation\")(prev_layer)`\n",
    "    * This will make the dense layer use a weight matrix $W \\in \\mathbb{R}^{a \\times b}$, and activation \"`the_activation`\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": null,
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe embeddings are on a local VM, and are not yet in `gs://rwc1/embeddings/`. Attemtps to access embeddings from the gcloud bucket had bugs. You can find the embeddings used [here](https://nlp.stanford.edu/projects/glove/), which are the Wikipedia + Gigaword 5 trained embeddings with 6 billion tokens."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../modeling\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../modeling\")\n",
    "from embeddings import *\n",
    "from orthoganlity_constraint import Orthoganal\n",
    "from rmn import RMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "GLOVE_DIMS = [50, 100, 200, 300]\n",
    "EMBEDDING_DIM = GLOVE_DIMS[0]\n",
    "\n",
    "embeddings_index = {}\n",
<<<<<<< HEAD
    "glove = open('../../../glove/glove.6B.%dd.txt' % EMBEDDING_DIM)\n",
=======
    "glove = open('../../data/glove/glove.6B.%dd.txt' % EMBEDDING_DIM)\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except Exception as e:\n",
    "        print(values[1:])\n",
    "        raise\n",
    "        \n",
    "    embeddings_index[word] = coefs\n",
    "glove.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(speeches_word_index) + 1, EMBEDDING_DIM))\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((len(speeches_word_index) + 1, EMBEDDING_DIM))\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "for word, i in speeches_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
<<<<<<< HEAD
    "        embedding_matrix[i] = embedding_vector"
=======
    "        embeddings_matrix[i] = embedding_vector"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13302, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average of spane embeddings\n",
    "Vst_train = embedding_matrix[speeches_train_padded].mean(axis=1)\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build embedding matrix\n",
    "# embeddings_index = fetch_embeddings()\n",
    "# embeddings_matrix = build_embedding_matrix(speeches_word_index, embeddings_index)\n",
    "\n",
    "# average of spane embeddings\n",
    "Vst_train = embeddings_matrix[speeches_train_padded].mean(axis=1)\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "Vst_train.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   "source": [
    "# one-hot-encoded speaker metadata inputs\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
<<<<<<< HEAD
    "for col in speak_map_cols:\n",
=======
    "for col in feature_columns:\n",
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
    "    df = sample_df[sample_df[col].unique()].values\n",
    "    dim = df.shape[1]\n",
    "    metadata_dict[col] = {'input': df, 'input_dim': dim}\n",
    "\n",
    "metadata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input avg span embeddings\n",
    "vt = Input(shape=(EMBEDDING_DIM,), name='Avg.Span.Embed.Input')\n",
    "\n",
    "# masking layer to account for padding\n",
    "# masking_layer = Masking(mask_value=0.0, input_shape = (EMBEDDING_DIM,), name = \"Mask\")(vt)\n",
    "\n",
    "## initializing speaker metadata embeddings\n",
    "\n",
    "input_layers = [vt]\n",
    "embedding_layers = [vt]\n",
    "for col in speak_map_cols:\n",
    "    \n",
    "    # one-hot-encoded\n",
    "    input_layer = Input(shape=(metadata_dict[col]['input_dim'],), name= col + '.Embed.Input')\n",
    "    embedding_init = (Dense(units = EMBEDDING_DIM,\n",
    "                            kernel_initializer = 'glorot_normal',\n",
    "                            input_shape = (metadata_dict[col]['input_dim'], ),\n",
    "                            activation = \"linear\",\n",
    "                            name = 'C_' + col)(input_layer))\n",
    "    \n",
    "    # keras embedding layers\n",
    "#     input_layer = (Embedding(output_dim = EMBEDDING_DIM,\n",
    "#                              embeddings_initializer = 'glorot_normal',\n",
    "#                             input_dim = (metadata_dict[col]['input_dim'], ),\n",
    "#                             name = 'C_' + col))\n",
    "    \n",
    "    input_layers.append(input_layer)\n",
    "    embedding_layers.append(embedding_init)\n",
    "\n",
    "# concat speaker metadata embeddings\n",
    "_ht = tf.keras.layers.Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "# dense layer\n",
    "ht = Dense(units = EMBEDDING_DIM, input_shape = (_ht.shape[1], ), activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "# dense layer with softmax activation, (where previous states will eventually be inserted) \n",
    "dt = Dense(units = k, input_shape = (EMBEDDING_DIM, ), activation = \"softmax\", name = \"Wd\")(ht)\n",
    "\n",
    "# reconstruction layer\n",
    "rt = Dense(units = EMBEDDING_DIM, input_shape = (k, ), activation = \"linear\", name = \"R\")(dt)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = build_metadata_dict(feature_columns, subject_df)\n",
    "metadata_dict.keys()"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "model.compile(optimizer = 'adam', loss=\"hinge\")"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(565)\n",
    "model = RMN().build_model(metadata_dict)\n",
    "model.summary()"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "speakerid.Embed.Input (InputLay (None, 536)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chamber.Embed.Input (InputLayer (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "state.Embed.Input (InputLayer)  (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender.Embed.Input (InputLayer) (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "party.Embed.Input (InputLayer)  (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Avg.Span.Embed.Input (InputLaye (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C_speakerid (Dense)             (None, 50)           26850       speakerid.Embed.Input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "C_chamber (Dense)               (None, 50)           150         chamber.Embed.Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C_state (Dense)                 (None, 50)           2850        state.Embed.Input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "C_gender (Dense)                (None, 50)           150         gender.Embed.Input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "C_party (Dense)                 (None, 50)           200         party.Embed.Input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Concat.Layer (Concatenate)      (None, 300)          0           Avg.Span.Embed.Input[0][0]       \n",
      "                                                                 C_speakerid[0][0]                \n",
      "                                                                 C_chamber[0][0]                  \n",
      "                                                                 C_state[0][0]                    \n",
      "                                                                 C_gender[0][0]                   \n",
      "                                                                 C_party[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Wh (Dense)                      (None, 50)           15050       Concat.Layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Wd (Dense)                      (None, 20)           1020        Wh[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "R (Dense)                       (None, 50)           1050        Wd[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 47,320\n",
      "Trainable params: 47,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [Vst_train]\n",
    "for key in metadata_dict.keys():\n",
    "    inputs.append(metadata_dict[key]['input'])\n",
    "np.random.seed(565)\n",
    "model.fit(x=inputs, y=Vst_train, batch_size=50, epochs = 10)"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13302/13302 [==============================] - 3s 201us/sample - loss: 0.9722\n",
      "Epoch 2/3\n",
      "13302/13302 [==============================] - 2s 118us/sample - loss: 0.9296\n",
      "Epoch 3/3\n",
      "13302/13302 [==============================] - 2s 118us/sample - loss: 0.8961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f497c036fd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [Vst_train]\n",
    "for key in metadata_dict.keys():\n",
    "    inputs.append(metadata_dict[key]['input'])\n",
    "\n",
    "model.fit(x=inputs, y=Vst_train, batch_size=25, epochs = 3)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.transpose(model.get_layer('R').get_weights()[0])\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ = np.dot(R,np.transpose(R))\n",
    "ones_R = np.diag(np.ones(R_.shape[0]))\n",
    "(R_ - ones_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "y_pred = model.predict(inputs)\n",
    "y_truth = Vst_train\n",
    "\n",
    "\n",
    "sims = []\n",
    "for i in range(y_truth.shape[0]):\n",
    "    cos_sim = cosine(y_truth[i],y_pred[i])\n",
    "    sims.append(cos_sim)\n",
    "\n",
    "np.array(sims).mean()"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# for l in model.layers:\n",
    "#     print(l)\n",
    "#     print(50*\"=\")\n",
    "#     print(\"input shape\", l.input_shape)\n",
    "#     print(\"output shape\", l.output_shape)"
=======
    "y_pred = model.predict(inputs)\n",
    "y_truth = Vst_train\n",
    "\n",
    "\n",
    "sims = []\n",
    "for i in range(y_truth.shape[0]):\n",
    "    cos_sim = cosine(y_truth[i],y_pred[i])\n",
    "    sims.append(cos_sim)\n",
    "\n",
    "np.array(sims).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the file drawer problem? Why is the file drawer problem important from the perspective of a firm trying to learn about the effectiveness of an intervention from peer reviewed research?\n",
    "- One response to the file drawer problem is to say, if there are multiple findings that point in the same direction, the effect is  \"real.\" What is the logic of this claim? How does p-hacking subvert this logic?\n",
    "- What is the pcurve? What is it meant to demonstrate (Figure   1). What is the key comparison to make based on Figure 1?"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.0152192,  2.9998407,  2.2928033, ...,  3.3455257, -2.7334466,\n",
       "         3.4286652],\n",
       "       [ 3.0152192,  2.9998407,  2.2928033, ...,  3.3455257, -2.7334466,\n",
       "         3.4286652],\n",
       "       [ 3.0152192,  2.9998407,  2.2928033, ...,  3.3455257, -2.7334466,\n",
       "         3.4286652],\n",
       "       ...,\n",
       "       [ 3.015219 ,  2.9998407,  2.2928033, ...,  3.3455253, -2.7334466,\n",
       "         3.428665 ],\n",
       "       [ 3.015219 ,  2.9998407,  2.2928033, ...,  3.3455253, -2.7334466,\n",
       "         3.428665 ],\n",
       "       [ 3.015219 ,  2.9998407,  2.2928033, ...,  3.3455253, -2.7334466,\n",
       "         3.428665 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "client.get_bucket('rwc1')"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07116484,  0.00912573,  0.06147658, ...,  0.00436425,\n",
       "        -0.02306813, -0.00891188],\n",
       "       [ 0.0666854 ,  0.01963167, -0.00729108, ...,  0.03347551,\n",
       "         0.00260285,  0.03218179],\n",
       "       [ 0.05064938,  0.00156331,  0.00330125, ...,  0.03080652,\n",
       "        -0.0103821 ,  0.021787  ],\n",
       "       ...,\n",
       "       [ 0.05323148,  0.01309576, -0.00020366, ...,  0.03097663,\n",
       "        -0.00145796,  0.04998812],\n",
       "       [ 0.08139307,  0.01893312,  0.01637157, ...,  0.01706613,\n",
       "        -0.01064621, -0.0202357 ],\n",
       "       [ 0.04599951,  0.02573776,  0.01956473, ...,  0.03908554,\n",
       "         0.01841787,  0.05329621]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vst_train"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "GLOVE_6B = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "GLOVE_42B = \"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "GLOVE_840B = \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "\n",
    "\n",
    "import requests\n",
    "    \n",
    "r = requests.get(GLOVE_6B)\n",
    "r"
>>>>>>> dcc266ea5319ba7c001c1dc68ade1147048987b7
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
