{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../scripts/assembly\")\n",
    "from session_speaker_assembly import *\n",
    "from preprocess import *\n",
    "from document import *\n",
    "from constant import SPEECHES, SPEAKER_MAP, HB_PATH, EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111113931</td>\n",
       "      <td>S</td>\n",
       "      <td>IN</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111113951</td>\n",
       "      <td>S</td>\n",
       "      <td>UT</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111113981</td>\n",
       "      <td>S</td>\n",
       "      <td>MO</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111114011</td>\n",
       "      <td>S</td>\n",
       "      <td>KS</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111114021</td>\n",
       "      <td>S</td>\n",
       "      <td>KY</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>111121840</td>\n",
       "      <td>H</td>\n",
       "      <td>NV</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>111121930</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>111121940</td>\n",
       "      <td>H</td>\n",
       "      <td>FL</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>111121950</td>\n",
       "      <td>H</td>\n",
       "      <td>AZ</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>111121960</td>\n",
       "      <td>H</td>\n",
       "      <td>NV</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     speakerid chamber state gender party\n",
       "0    111113931       S    IN      M     D\n",
       "1    111113951       S    UT      M     R\n",
       "2    111113981       S    MO      M     R\n",
       "3    111114011       S    KS      M     R\n",
       "4    111114021       S    KY      M     R\n",
       "..         ...     ...   ...    ...   ...\n",
       "552  111121840       H    NV      M     R\n",
       "553  111121930       H    IL      M     D\n",
       "554  111121940       H    FL      M     D\n",
       "555  111121950       H    AZ      F     D\n",
       "556  111121960       H    NV      F     D\n",
       "\n",
       "[557 rows x 5 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = 111\n",
    "speak_map_cols = ['speakerid','chamber','state','gender','party']\n",
    "\n",
    "speaker_map_df = pd.read_csv(os.path.join(HB_PATH,SPEAKER_MAP % session), sep = '|')[speak_map_cols]\n",
    "speaker_map_df = speaker_map_df.groupby('speakerid').last().reset_index()\n",
    "speaker_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>pay their bills and keep their homes. small bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111120160.0</td>\n",
       "      <td>honest and fair prosperity for the many. not j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111121410.0</td>\n",
       "      <td>rarely has our great Nation faced such grave c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111120961.0</td>\n",
       "      <td>together. With the middle class struggling to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111114091.0</td>\n",
       "      <td>amount of pride in noting that in each of thes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     speakerid                                             speech\n",
       "0  111118060.0  pay their bills and keep their homes. small bu...\n",
       "1  111120160.0  honest and fair prosperity for the many. not j...\n",
       "2  111121410.0  rarely has our great Nation faced such grave c...\n",
       "3  111120961.0  together. With the middle class struggling to ...\n",
       "4  111114091.0  amount of pride in noting that in each of thes..."
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_df = subject_docs(session = session, path = HB_PATH, subject = \"health\", min_len_tokens=100)\n",
    "subject_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# megre speech and speaker metadata\n",
    "session_df = subject_df.merge(speaker_map_df, how = 'inner', on = 'speakerid')\n",
    "\n",
    "# ensure proper merge\n",
    "assert(subject_df.shape[0]==session_df.shape[0])\n",
    "assert(subject_df.shape[1] + len(speak_map_cols) - 1 == session_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>111113931.0</th>\n",
       "      <th>111113951.0</th>\n",
       "      <th>111113981.0</th>\n",
       "      <th>111114011.0</th>\n",
       "      <th>...</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>D</th>\n",
       "      <th>I</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>pay their bills and keep their homes. small bu...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>of is contained in the bill. And we also belie...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>The bill is supposed to be about creating jobs...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>administration and enforcement team consisting...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>111118060.0</td>\n",
       "      <td>President and our Democrat colleagues here in ...</td>\n",
       "      <td>H</td>\n",
       "      <td>OH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13297</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>Asset Relief Program. TARP. repeals TARP. repe...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13298</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>on here how it was created. and it indicates n...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13299</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>You wonder why this component would be in a he...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>111119610.0</td>\n",
       "      <td>party. But. instead. we have proposed positive...</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13301</td>\n",
       "      <td>111116870.0</td>\n",
       "      <td>infrastructure of America in this bill is goin...</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13302 rows × 605 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         speakerid                                             speech chamber  \\\n",
       "0      111118060.0  pay their bills and keep their homes. small bu...       H   \n",
       "1      111118060.0  of is contained in the bill. And we also belie...       H   \n",
       "2      111118060.0  The bill is supposed to be about creating jobs...       H   \n",
       "3      111118060.0  administration and enforcement team consisting...       H   \n",
       "4      111118060.0  President and our Democrat colleagues here in ...       H   \n",
       "...            ...                                                ...     ...   \n",
       "13297  111119610.0  Asset Relief Program. TARP. repeals TARP. repe...       H   \n",
       "13298  111119610.0  on here how it was created. and it indicates n...       H   \n",
       "13299  111119610.0  You wonder why this component would be in a he...       H   \n",
       "13300  111119610.0  party. But. instead. we have proposed positive...       H   \n",
       "13301  111116870.0  infrastructure of America in this bill is goin...       H   \n",
       "\n",
       "      state gender party  111113931.0  111113951.0  111113981.0  111114011.0  \\\n",
       "0        OH      M     R            0            0            0            0   \n",
       "1        OH      M     R            0            0            0            0   \n",
       "2        OH      M     R            0            0            0            0   \n",
       "3        OH      M     R            0            0            0            0   \n",
       "4        OH      M     R            0            0            0            0   \n",
       "...     ...    ...   ...          ...          ...          ...          ...   \n",
       "13297    GA      M     R            0            0            0            0   \n",
       "13298    GA      M     R            0            0            0            0   \n",
       "13299    GA      M     R            0            0            0            0   \n",
       "13300    GA      M     R            0            0            0            0   \n",
       "13301    IL      M     R            0            0            0            0   \n",
       "\n",
       "       ...  VT  WA  WI  WV  WY  F  M  D  I  R  \n",
       "0      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "1      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "2      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "3      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "4      ...   0   0   0   0   0  0  1  0  0  1  \n",
       "...    ...  ..  ..  ..  ..  .. .. .. .. .. ..  \n",
       "13297  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13298  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13299  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13300  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "13301  ...   0   0   0   0   0  0  1  0  0  1  \n",
       "\n",
       "[13302 rows x 605 columns]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset data for prelim building\n",
    "size = session_df.shape[0]\n",
    "sample_df = session_df.iloc[:size,:]\n",
    "\n",
    "# one-hot-encode speaker metadata\n",
    "for col in speak_map_cols:\n",
    "    sample_df = pd.concat([sample_df,pd.get_dummies(sample_df[col])], axis = 1)\n",
    "    \n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker count: 536\n"
     ]
    }
   ],
   "source": [
    "sample_speakers = sample_df['speakerid'].unique()\n",
    "print('speaker count:', len(sample_speakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 535 Members of Congress. 100 serve in the U.S. Senate and 435 serve in the U.S. House of Representatives. A length of 50 suggests that nearly everyone commented on \"health\" (in a speech of more than 50 words) at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(session_df[\"speech\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'health': 4,\n",
       " 'of': 5,\n",
       " 'care': 6,\n",
       " 'that': 7,\n",
       " 'in': 8,\n",
       " 'a': 9,\n",
       " 'is': 10,\n",
       " 'we': 11,\n",
       " 'for': 12,\n",
       " 'this': 13,\n",
       " 'i': 14,\n",
       " 'have': 15,\n",
       " 'it': 16,\n",
       " 'are': 17,\n",
       " 'on': 18,\n",
       " 'our': 19,\n",
       " 'bill': 20,\n",
       " 'as': 21,\n",
       " 'they': 22,\n",
       " 'with': 23,\n",
       " 'will': 24,\n",
       " 'their': 25,\n",
       " 'about': 26,\n",
       " 'not': 27,\n",
       " 'be': 28,\n",
       " 'you': 29,\n",
       " 'people': 30,\n",
       " 'reform': 31,\n",
       " 'from': 32,\n",
       " 'insurance': 33,\n",
       " 'has': 34,\n",
       " 'by': 35,\n",
       " 'all': 36,\n",
       " 'mr': 37,\n",
       " 'but': 38,\n",
       " 'who': 39,\n",
       " 'what': 40,\n",
       " 'my': 41,\n",
       " 'was': 42,\n",
       " 'going': 43,\n",
       " 'do': 44,\n",
       " 'speaker': 45,\n",
       " 'would': 46,\n",
       " 'an': 47,\n",
       " 'so': 48,\n",
       " 'more': 49,\n",
       " 'at': 50,\n",
       " 'president': 51,\n",
       " 'or': 52,\n",
       " 'there': 53,\n",
       " 'one': 54,\n",
       " 'american': 55,\n",
       " 'been': 56,\n",
       " 'if': 57,\n",
       " 'when': 58,\n",
       " 'americans': 59,\n",
       " 'which': 60,\n",
       " 'government': 61,\n",
       " 'system': 62,\n",
       " 'now': 63,\n",
       " 'he': 64,\n",
       " 'can': 65,\n",
       " 'because': 66,\n",
       " 'want': 67,\n",
       " 'these': 68,\n",
       " 'know': 69,\n",
       " 'over': 70,\n",
       " 'its': 71,\n",
       " 'country': 72,\n",
       " 'were': 73,\n",
       " 'many': 74,\n",
       " 'today': 75,\n",
       " 'out': 76,\n",
       " 'new': 77,\n",
       " 'us': 78,\n",
       " 'need': 79,\n",
       " 'legislation': 80,\n",
       " 'time': 81,\n",
       " 'other': 82,\n",
       " 'public': 83,\n",
       " 'just': 84,\n",
       " 'costs': 85,\n",
       " 'get': 86,\n",
       " 'jobs': 87,\n",
       " 'also': 88,\n",
       " 'think': 89,\n",
       " 'some': 90,\n",
       " 'those': 91,\n",
       " 'his': 92,\n",
       " 'up': 93,\n",
       " 'congress': 94,\n",
       " 'here': 95,\n",
       " 'no': 96,\n",
       " 'important': 97,\n",
       " 'education': 98,\n",
       " 'very': 99,\n",
       " 'them': 100,\n",
       " 'medicare': 101,\n",
       " 'make': 102,\n",
       " 'had': 103,\n",
       " 'years': 104,\n",
       " 'than': 105,\n",
       " 'cost': 106,\n",
       " 'percent': 107,\n",
       " 'like': 108,\n",
       " 'million': 109,\n",
       " 'senator': 110,\n",
       " 'plan': 111,\n",
       " 'dont': 112,\n",
       " 'debate': 113,\n",
       " 'families': 114,\n",
       " 'how': 115,\n",
       " 'year': 116,\n",
       " 'energy': 117,\n",
       " 'children': 118,\n",
       " 'work': 119,\n",
       " 'every': 120,\n",
       " 'say': 121,\n",
       " 'house': 122,\n",
       " 'billion': 123,\n",
       " 'support': 124,\n",
       " 'well': 125,\n",
       " 'most': 126,\n",
       " 'me': 127,\n",
       " 'said': 128,\n",
       " 'take': 129,\n",
       " 'access': 130,\n",
       " 'america': 131,\n",
       " 'provide': 132,\n",
       " 'should': 133,\n",
       " 'small': 134,\n",
       " 'program': 135,\n",
       " 'issue': 136,\n",
       " 'senate': 137,\n",
       " 'national': 138,\n",
       " 'economy': 139,\n",
       " 'states': 140,\n",
       " 'services': 141,\n",
       " 'act': 142,\n",
       " 'help': 143,\n",
       " '1': 144,\n",
       " 'tax': 145,\n",
       " 'way': 146,\n",
       " 'federal': 147,\n",
       " 'am': 148,\n",
       " 'veterans': 149,\n",
       " 'coverage': 150,\n",
       " 'last': 151,\n",
       " 'state': 152,\n",
       " 'through': 153,\n",
       " 'into': 154,\n",
       " 'pay': 155,\n",
       " 'economic': 156,\n",
       " 'benefits': 157,\n",
       " 'only': 158,\n",
       " 'where': 159,\n",
       " 'right': 160,\n",
       " 'your': 161,\n",
       " 'talk': 162,\n",
       " 'first': 163,\n",
       " 'colleagues': 164,\n",
       " 'much': 165,\n",
       " '2': 166,\n",
       " 'businesses': 167,\n",
       " 'things': 168,\n",
       " 'go': 169,\n",
       " 'before': 170,\n",
       " 'programs': 171,\n",
       " 'budget': 172,\n",
       " 'amendment': 173,\n",
       " 'could': 174,\n",
       " 'issues': 175,\n",
       " 'good': 176,\n",
       " 'quality': 177,\n",
       " 'medical': 178,\n",
       " 'business': 179,\n",
       " 'any': 180,\n",
       " 'come': 181,\n",
       " 'money': 182,\n",
       " 'why': 183,\n",
       " 'madam': 184,\n",
       " '000': 185,\n",
       " 'being': 186,\n",
       " 'thats': 187,\n",
       " 'under': 188,\n",
       " 'see': 189,\n",
       " 'affordable': 190,\n",
       " 'trillion': 191,\n",
       " 'back': 192,\n",
       " 'thank': 193,\n",
       " 'rise': 194,\n",
       " 'such': 195,\n",
       " 'lot': 196,\n",
       " 'committee': 197,\n",
       " 'nation': 198,\n",
       " 'down': 199,\n",
       " 'talking': 200,\n",
       " 'floor': 201,\n",
       " 'home': 202,\n",
       " 'her': 203,\n",
       " 'takeover': 204,\n",
       " 'fact': 205,\n",
       " 'women': 206,\n",
       " 'day': 207,\n",
       " 'does': 208,\n",
       " 'spending': 209,\n",
       " 'nations': 210,\n",
       " 'job': 211,\n",
       " 'taxes': 212,\n",
       " 'law': 213,\n",
       " 'must': 214,\n",
       " 'even': 215,\n",
       " 'passed': 216,\n",
       " 'week': 217,\n",
       " 'made': 218,\n",
       " 'increase': 219,\n",
       " 'democrats': 220,\n",
       " 'united': 221,\n",
       " 'better': 222,\n",
       " 'working': 223,\n",
       " 'she': 224,\n",
       " 'put': 225,\n",
       " 'safety': 226,\n",
       " 'number': 227,\n",
       " 'vote': 228,\n",
       " 'great': 229,\n",
       " 'across': 230,\n",
       " 'then': 231,\n",
       " 'companies': 232,\n",
       " 'look': 233,\n",
       " 'again': 234,\n",
       " 'part': 235,\n",
       " 'seniors': 236,\n",
       " 'believe': 237,\n",
       " 'side': 238,\n",
       " 'too': 239,\n",
       " 'same': 240,\n",
       " 'thing': 241,\n",
       " 'including': 242,\n",
       " 'improve': 243,\n",
       " 'after': 244,\n",
       " 'funding': 245,\n",
       " 'really': 246,\n",
       " 'family': 247,\n",
       " 'did': 248,\n",
       " 'mental': 249,\n",
       " 'two': 250,\n",
       " 'critical': 251,\n",
       " 'republican': 252,\n",
       " 'while': 253,\n",
       " 'needs': 254,\n",
       " 'able': 255,\n",
       " 'long': 256,\n",
       " 'private': 257,\n",
       " 'both': 258,\n",
       " 'without': 259,\n",
       " 'problems': 260,\n",
       " 'republicans': 261,\n",
       " 'lost': 262,\n",
       " 'industry': 263,\n",
       " 'life': 264,\n",
       " 'another': 265,\n",
       " 'may': 266,\n",
       " 'majority': 267,\n",
       " 'something': 268,\n",
       " 'community': 269,\n",
       " 'millions': 270,\n",
       " 'continue': 271,\n",
       " 'pass': 272,\n",
       " 'cant': 273,\n",
       " 'create': 274,\n",
       " 'sure': 275,\n",
       " '5': 276,\n",
       " 'security': 277,\n",
       " 'financial': 278,\n",
       " 'point': 279,\n",
       " 'making': 280,\n",
       " 'heard': 281,\n",
       " 'keep': 282,\n",
       " 'policy': 283,\n",
       " 'opportunity': 284,\n",
       " 'members': 285,\n",
       " 'cuts': 286,\n",
       " 'future': 287,\n",
       " 'whether': 288,\n",
       " 'during': 289,\n",
       " 'change': 290,\n",
       " 'doing': 291,\n",
       " 'workers': 292,\n",
       " 'address': 293,\n",
       " 'food': 294,\n",
       " 'world': 295,\n",
       " 'give': 296,\n",
       " 'bills': 297,\n",
       " 'h': 298,\n",
       " 'doctors': 299,\n",
       " 'cannot': 300,\n",
       " 'medicaid': 301,\n",
       " 'reduce': 302,\n",
       " 'control': 303,\n",
       " 'washington': 304,\n",
       " 'since': 305,\n",
       " 'done': 306,\n",
       " 'lives': 307,\n",
       " 'few': 308,\n",
       " 'r': 309,\n",
       " 'let': 310,\n",
       " 'administration': 311,\n",
       " 'obama': 312,\n",
       " 'ensure': 313,\n",
       " 'next': 314,\n",
       " 'actually': 315,\n",
       " 'own': 316,\n",
       " 'unemployment': 317,\n",
       " '3': 318,\n",
       " 'employees': 319,\n",
       " 'providing': 320,\n",
       " 'service': 321,\n",
       " 'provides': 322,\n",
       " 'leadership': 323,\n",
       " 'forward': 324,\n",
       " 'months': 325,\n",
       " 'premiums': 326,\n",
       " 'dollars': 327,\n",
       " 'current': 328,\n",
       " '10': 329,\n",
       " 'option': 330,\n",
       " 'problem': 331,\n",
       " 'afford': 332,\n",
       " 'major': 333,\n",
       " 'already': 334,\n",
       " 'protect': 335,\n",
       " 'ago': 336,\n",
       " 'big': 337,\n",
       " 'district': 338,\n",
       " 'together': 339,\n",
       " 'got': 340,\n",
       " 'research': 341,\n",
       " 'between': 342,\n",
       " 'democratic': 343,\n",
       " 'patients': 344,\n",
       " 'gentleman': 345,\n",
       " '4': 346,\n",
       " 'theyre': 347,\n",
       " 'use': 348,\n",
       " 's': 349,\n",
       " 'bipartisan': 350,\n",
       " 'little': 351,\n",
       " 'real': 352,\n",
       " 'communities': 353,\n",
       " 'best': 354,\n",
       " 'still': 355,\n",
       " 'doctor': 356,\n",
       " 'leader': 357,\n",
       " 'around': 358,\n",
       " 'trying': 359,\n",
       " 'question': 360,\n",
       " 'plans': 361,\n",
       " 'against': 362,\n",
       " 'having': 363,\n",
       " 'understand': 364,\n",
       " 'find': 365,\n",
       " 'efforts': 366,\n",
       " 'office': 367,\n",
       " 'seen': 368,\n",
       " 'individuals': 369,\n",
       " 'water': 370,\n",
       " 'crisis': 371,\n",
       " 'came': 372,\n",
       " 'spend': 373,\n",
       " 'high': 374,\n",
       " 'lose': 375,\n",
       " 'kind': 376,\n",
       " 'yet': 377,\n",
       " 'governmentrun': 378,\n",
       " 'rural': 379,\n",
       " 'saying': 380,\n",
       " 'constituents': 381,\n",
       " 'clear': 382,\n",
       " 'increases': 383,\n",
       " 'him': 384,\n",
       " 'areas': 385,\n",
       " 'says': 386,\n",
       " 'proposal': 387,\n",
       " 'means': 388,\n",
       " 'debt': 389,\n",
       " 'citizens': 390,\n",
       " 'times': 391,\n",
       " 'bring': 392,\n",
       " 'social': 393,\n",
       " 'cut': 394,\n",
       " 'aisle': 395,\n",
       " 'deal': 396,\n",
       " 'congressional': 397,\n",
       " 'off': 398,\n",
       " 'worked': 399,\n",
       " 'information': 400,\n",
       " 'agree': 401,\n",
       " 'u': 402,\n",
       " 'americas': 403,\n",
       " 'housing': 404,\n",
       " 'save': 405,\n",
       " 'comes': 406,\n",
       " 'stimulus': 407,\n",
       " 'three': 408,\n",
       " 'past': 409,\n",
       " 'assistance': 410,\n",
       " 'speak': 411,\n",
       " 'colleague': 412,\n",
       " 'serious': 413,\n",
       " 'getting': 414,\n",
       " 'taking': 415,\n",
       " 'chairman': 416,\n",
       " 'comprehensive': 417,\n",
       " 'deficit': 418,\n",
       " 'child': 419,\n",
       " 'strong': 420,\n",
       " 'military': 421,\n",
       " 'childrens': 422,\n",
       " 'concerned': 423,\n",
       " 'tell': 424,\n",
       " 'nothing': 425,\n",
       " 'development': 426,\n",
       " 'each': 427,\n",
       " 'disease': 428,\n",
       " '2009': 429,\n",
       " 'friend': 430,\n",
       " 'fix': 431,\n",
       " 'example': 432,\n",
       " 'weve': 433,\n",
       " 'longterm': 434,\n",
       " 'im': 435,\n",
       " 'coming': 436,\n",
       " 'cancer': 437,\n",
       " 'human': 438,\n",
       " 'try': 439,\n",
       " 'higher': 440,\n",
       " 'hope': 441,\n",
       " '8': 442,\n",
       " 'month': 443,\n",
       " 'infrastructure': 444,\n",
       " 'doesnt': 445,\n",
       " 'process': 446,\n",
       " 'makes': 447,\n",
       " 'lets': 448,\n",
       " 'start': 449,\n",
       " 'impact': 450,\n",
       " 'nearly': 451,\n",
       " 'technology': 452,\n",
       " 'wish': 453,\n",
       " 'medicine': 454,\n",
       " 'order': 455,\n",
       " 'almost': 456,\n",
       " 'school': 457,\n",
       " 'result': 458,\n",
       " 'raise': 459,\n",
       " 'investments': 460,\n",
       " 'less': 461,\n",
       " 'weeks': 462,\n",
       " '11': 463,\n",
       " 'resolution': 464,\n",
       " 'spent': 465,\n",
       " 'provisions': 466,\n",
       " 'resources': 467,\n",
       " 'effort': 468,\n",
       " 'funds': 469,\n",
       " 'told': 470,\n",
       " 'simply': 471,\n",
       " 'uninsured': 472,\n",
       " 'lower': 473,\n",
       " 'challenges': 474,\n",
       " 'step': 475,\n",
       " 'includes': 476,\n",
       " 'expand': 477,\n",
       " 'hard': 478,\n",
       " 'providers': 479,\n",
       " 'course': 480,\n",
       " 'ask': 481,\n",
       " 'move': 482,\n",
       " 'enough': 483,\n",
       " 'different': 484,\n",
       " 'report': 485,\n",
       " 'hear': 486,\n",
       " 'certainly': 487,\n",
       " 'place': 488,\n",
       " 'significant': 489,\n",
       " 'body': 490,\n",
       " 'others': 491,\n",
       " 'face': 492,\n",
       " 'several': 493,\n",
       " 'allow': 494,\n",
       " 'environment': 495,\n",
       " 'individual': 496,\n",
       " 'finally': 497,\n",
       " 'among': 498,\n",
       " 'end': 499,\n",
       " 'needed': 500,\n",
       " 'pelosi': 501,\n",
       " 'discussion': 502,\n",
       " 'environmental': 503,\n",
       " 'treatment': 504,\n",
       " 'fiscal': 505,\n",
       " 'talked': 506,\n",
       " 'additional': 507,\n",
       " 'reason': 508,\n",
       " 'prevention': 509,\n",
       " 'concerns': 510,\n",
       " 'whole': 511,\n",
       " 'rights': 512,\n",
       " 'dr': 513,\n",
       " 'risk': 514,\n",
       " 'ways': 515,\n",
       " 'centers': 516,\n",
       " '6': 517,\n",
       " 'young': 518,\n",
       " 'drug': 519,\n",
       " 'policies': 520,\n",
       " 'never': 521,\n",
       " 'particularly': 522,\n",
       " 'clean': 523,\n",
       " 'recovery': 524,\n",
       " 'might': 525,\n",
       " 'va': 526,\n",
       " 'company': 527,\n",
       " 'days': 528,\n",
       " 'tonight': 529,\n",
       " 'call': 530,\n",
       " 'friends': 531,\n",
       " 'recently': 532,\n",
       " 'hospitals': 533,\n",
       " 'kids': 534,\n",
       " 'local': 535,\n",
       " 'focus': 536,\n",
       " 'benefit': 537,\n",
       " 'history': 538,\n",
       " '7': 539,\n",
       " 'far': 540,\n",
       " 'buy': 541,\n",
       " 'member': 542,\n",
       " 'goes': 543,\n",
       " '20': 544,\n",
       " 'choice': 545,\n",
       " 'defense': 546,\n",
       " 'paying': 547,\n",
       " 'receive': 548,\n",
       " 'called': 549,\n",
       " 'healthy': 550,\n",
       " 'ever': 551,\n",
       " 'finance': 552,\n",
       " 'single': 553,\n",
       " '9': 554,\n",
       " 'provided': 555,\n",
       " 'savings': 556,\n",
       " 'losing': 557,\n",
       " 'improving': 558,\n",
       " 'didnt': 559,\n",
       " 'decisions': 560,\n",
       " 'homes': 561,\n",
       " 'democrat': 562,\n",
       " 'currently': 563,\n",
       " 'center': 564,\n",
       " 'away': 565,\n",
       " 'package': 566,\n",
       " 'proposed': 567,\n",
       " 'thousands': 568,\n",
       " 'everyone': 569,\n",
       " 'earlier': 570,\n",
       " 'department': 571,\n",
       " 'concern': 572,\n",
       " 'fund': 573,\n",
       " 'growth': 574,\n",
       " 'historic': 575,\n",
       " 'used': 576,\n",
       " 'conditions': 577,\n",
       " 'however': 578,\n",
       " 'read': 579,\n",
       " 'according': 580,\n",
       " 'facing': 581,\n",
       " 'income': 582,\n",
       " 'tobacco': 583,\n",
       " 'presidents': 584,\n",
       " 'schools': 585,\n",
       " 'taken': 586,\n",
       " 'protection': 587,\n",
       " 'throughout': 588,\n",
       " 'looking': 589,\n",
       " 'capandtrade': 590,\n",
       " 'approach': 591,\n",
       " 'abortion': 592,\n",
       " 'especially': 593,\n",
       " 'area': 594,\n",
       " 'instead': 595,\n",
       " 'broken': 596,\n",
       " 'youre': 597,\n",
       " 'true': 598,\n",
       " 'include': 599,\n",
       " '100': 600,\n",
       " 'party': 601,\n",
       " '50': 602,\n",
       " '30': 603,\n",
       " 'global': 604,\n",
       " 'struggling': 605,\n",
       " 'recent': 606,\n",
       " 'lead': 607,\n",
       " 'group': 608,\n",
       " 'importance': 609,\n",
       " 'physician': 610,\n",
       " 'necessary': 611,\n",
       " 'become': 612,\n",
       " 'choices': 613,\n",
       " 'unfortunately': 614,\n",
       " 'patient': 615,\n",
       " 'terms': 616,\n",
       " 'proud': 617,\n",
       " 'special': 618,\n",
       " 'deserve': 619,\n",
       " 'always': 620,\n",
       " 'vital': 621,\n",
       " 'increased': 622,\n",
       " 'massive': 623,\n",
       " 'least': 624,\n",
       " 'attention': 625,\n",
       " 'offer': 626,\n",
       " 'stand': 627,\n",
       " 'four': 628,\n",
       " 'within': 629,\n",
       " 'responsibility': 630,\n",
       " 'credit': 631,\n",
       " 'rising': 632,\n",
       " 'half': 633,\n",
       " 'often': 634,\n",
       " 'paid': 635,\n",
       " 'street': 636,\n",
       " '15': 637,\n",
       " 'emergency': 638,\n",
       " 'increasing': 639,\n",
       " 'matter': 640,\n",
       " 'pt': 641,\n",
       " 'sector': 642,\n",
       " 'climate': 643,\n",
       " 'white': 644,\n",
       " 'discuss': 645,\n",
       " 'personal': 646,\n",
       " 'mean': 647,\n",
       " 'once': 648,\n",
       " 'commitment': 649,\n",
       " 'appropriations': 650,\n",
       " 'until': 651,\n",
       " 'investment': 652,\n",
       " 'physicians': 653,\n",
       " 'addition': 654,\n",
       " 'professionals': 655,\n",
       " 'wellbeing': 656,\n",
       " 'add': 657,\n",
       " 'morning': 658,\n",
       " 'live': 659,\n",
       " 'middle': 660,\n",
       " 'yesterday': 661,\n",
       " 'stop': 662,\n",
       " 'provision': 663,\n",
       " 'yield': 664,\n",
       " 'amendments': 665,\n",
       " 'free': 666,\n",
       " 'poor': 667,\n",
       " 'given': 668,\n",
       " 'difficult': 669,\n",
       " 'idea': 670,\n",
       " 'universal': 671,\n",
       " 'second': 672,\n",
       " 'class': 673,\n",
       " 'advantage': 674,\n",
       " 'changes': 675,\n",
       " 'essential': 676,\n",
       " 'asked': 677,\n",
       " 'cover': 678,\n",
       " 'ought': 679,\n",
       " 'happen': 680,\n",
       " 'basic': 681,\n",
       " 'hospital': 682,\n",
       " 'urge': 683,\n",
       " 'meet': 684,\n",
       " 'wanted': 685,\n",
       " 'rate': 686,\n",
       " 'senators': 687,\n",
       " 'further': 688,\n",
       " 'appreciate': 689,\n",
       " 'person': 690,\n",
       " 'everybody': 691,\n",
       " 'role': 692,\n",
       " 'reforms': 693,\n",
       " 'prevent': 694,\n",
       " 'ability': 695,\n",
       " 'lack': 696,\n",
       " 'serve': 697,\n",
       " 'war': 698,\n",
       " 'moment': 699,\n",
       " 'opportunities': 700,\n",
       " 'living': 701,\n",
       " 'womens': 702,\n",
       " 'representatives': 703,\n",
       " 'training': 704,\n",
       " 'night': 705,\n",
       " 'run': 706,\n",
       " 'anything': 707,\n",
       " 'mandate': 708,\n",
       " 'awareness': 709,\n",
       " 'market': 710,\n",
       " 'probably': 711,\n",
       " 'upon': 712,\n",
       " 'men': 713,\n",
       " 'available': 714,\n",
       " 'leaders': 715,\n",
       " 'huge': 716,\n",
       " 'theres': 717,\n",
       " 'join': 718,\n",
       " 'left': 719,\n",
       " 'along': 720,\n",
       " 'goal': 721,\n",
       " 'nurses': 722,\n",
       " 'helping': 723,\n",
       " 'creating': 724,\n",
       " 'offered': 725,\n",
       " 'cause': 726,\n",
       " 'brought': 727,\n",
       " 'behind': 728,\n",
       " 'political': 729,\n",
       " 'force': 730,\n",
       " '2010': 731,\n",
       " 'sick': 732,\n",
       " 'couple': 733,\n",
       " 'consider': 734,\n",
       " 'folks': 735,\n",
       " 'whats': 736,\n",
       " 'parents': 737,\n",
       " 'congressman': 738,\n",
       " 'knows': 739,\n",
       " 'top': 740,\n",
       " 'bit': 741,\n",
       " 'created': 742,\n",
       " 'share': 743,\n",
       " 'included': 744,\n",
       " 'passing': 745,\n",
       " 'cobra': 746,\n",
       " 'wrong': 747,\n",
       " 'action': 748,\n",
       " 'competition': 749,\n",
       " 'subject': 750,\n",
       " 'protecting': 751,\n",
       " 'everything': 752,\n",
       " 'wall': 753,\n",
       " 'pleased': 754,\n",
       " 'found': 755,\n",
       " 'case': 756,\n",
       " 'rather': 757,\n",
       " 'yes': 758,\n",
       " 'exactly': 759,\n",
       " 'hours': 760,\n",
       " 'bad': 761,\n",
       " 'cbo': 762,\n",
       " 'words': 763,\n",
       " 'set': 764,\n",
       " 'extend': 765,\n",
       " 'takes': 766,\n",
       " 'systems': 767,\n",
       " 'texas': 768,\n",
       " 'recognize': 769,\n",
       " 'entire': 770,\n",
       " 'votes': 771,\n",
       " 'priorities': 772,\n",
       " '500': 773,\n",
       " 'particular': 774,\n",
       " 'thought': 775,\n",
       " 'invest': 776,\n",
       " 'caucus': 777,\n",
       " 'largest': 778,\n",
       " 'status': 779,\n",
       " 'prescription': 780,\n",
       " 'maybe': 781,\n",
       " 'chamber': 782,\n",
       " 'lowincome': 783,\n",
       " 'products': 784,\n",
       " 'fight': 785,\n",
       " 'opposition': 786,\n",
       " 'trade': 787,\n",
       " 'reducing': 788,\n",
       " 'abuse': 789,\n",
       " 'voted': 790,\n",
       " 'saw': 791,\n",
       " 'ive': 792,\n",
       " 'schip': 793,\n",
       " 'begin': 794,\n",
       " 'heart': 795,\n",
       " 'require': 796,\n",
       " 'continues': 797,\n",
       " 'respect': 798,\n",
       " 'promote': 799,\n",
       " 'language': 800,\n",
       " 'transportation': 801,\n",
       " 'themselves': 802,\n",
       " 'growing': 803,\n",
       " 'later': 804,\n",
       " 'organizations': 805,\n",
       " 'level': 806,\n",
       " 'though': 807,\n",
       " 'senior': 808,\n",
       " 'primary': 809,\n",
       " 'seems': 810,\n",
       " 'absolutely': 811,\n",
       " 'record': 812,\n",
       " 'situation': 813,\n",
       " 'rules': 814,\n",
       " 'expensive': 815,\n",
       " 'rates': 816,\n",
       " 'period': 817,\n",
       " 'students': 818,\n",
       " 'experience': 819,\n",
       " 'laws': 820,\n",
       " 'expansion': 821,\n",
       " 'creation': 822,\n",
       " 'large': 823,\n",
       " 'mandates': 824,\n",
       " 'employers': 825,\n",
       " 'retirement': 826,\n",
       " 'introduced': 827,\n",
       " 'putting': 828,\n",
       " 'went': 829,\n",
       " 'early': 830,\n",
       " 'facilities': 831,\n",
       " 'whose': 832,\n",
       " 'drugs': 833,\n",
       " 'greater': 834,\n",
       " 'freedom': 835,\n",
       " 'domestic': 836,\n",
       " 'amount': 837,\n",
       " 'hour': 838,\n",
       " 'addressing': 839,\n",
       " 'isnt': 840,\n",
       " 'debating': 841,\n",
       " 'consumers': 842,\n",
       " 'kennedy': 843,\n",
       " 'agencies': 844,\n",
       " 'toward': 845,\n",
       " 'nursing': 846,\n",
       " 'reasons': 847,\n",
       " 'finding': 848,\n",
       " 'bringing': 849,\n",
       " 'happened': 850,\n",
       " 'socalled': 851,\n",
       " 'fair': 852,\n",
       " 'woman': 853,\n",
       " 'poverty': 854,\n",
       " 'reforming': 855,\n",
       " 'minutes': 856,\n",
       " 'simple': 857,\n",
       " 'moving': 858,\n",
       " 'sense': 859,\n",
       " 'someone': 860,\n",
       " 'power': 861,\n",
       " 'solution': 862,\n",
       " 'regard': 863,\n",
       " 'delivery': 864,\n",
       " 'longer': 865,\n",
       " 'dealing': 866,\n",
       " 'numbers': 867,\n",
       " 'decades': 868,\n",
       " 'governments': 869,\n",
       " 'study': 870,\n",
       " 'works': 871,\n",
       " 'took': 872,\n",
       " 'billions': 873,\n",
       " 'pages': 874,\n",
       " 'countries': 875,\n",
       " 'physical': 876,\n",
       " 'effects': 877,\n",
       " 'obviously': 878,\n",
       " 'august': 879,\n",
       " 'served': 880,\n",
       " 'projects': 881,\n",
       " 'college': 882,\n",
       " 'ideas': 883,\n",
       " 'death': 884,\n",
       " 'effective': 885,\n",
       " 'open': 886,\n",
       " 'hundreds': 887,\n",
       " 'due': 888,\n",
       " 'full': 889,\n",
       " 'remember': 890,\n",
       " '12': 891,\n",
       " 'tennessee': 892,\n",
       " '40': 893,\n",
       " 'general': 894,\n",
       " 'york': 895,\n",
       " 'passage': 896,\n",
       " 'taxpayers': 897,\n",
       " 'recession': 898,\n",
       " 'key': 899,\n",
       " 'despite': 900,\n",
       " 'town': 901,\n",
       " 'california': 902,\n",
       " 'committees': 903,\n",
       " 'per': 904,\n",
       " 'educational': 905,\n",
       " 'preexisting': 906,\n",
       " 'priority': 907,\n",
       " 'relief': 908,\n",
       " 'consideration': 909,\n",
       " 'responsible': 910,\n",
       " 'payment': 911,\n",
       " 'air': 912,\n",
       " 'wont': 913,\n",
       " 'doors': 914,\n",
       " 'enforcement': 915,\n",
       " 'helped': 916,\n",
       " 'conference': 917,\n",
       " 'fraud': 918,\n",
       " 'line': 919,\n",
       " 'based': 920,\n",
       " 'chance': 921,\n",
       " '60': 922,\n",
       " 'compensation': 923,\n",
       " 'possible': 924,\n",
       " 'commerce': 925,\n",
       " 'common': 926,\n",
       " 'waste': 927,\n",
       " 'related': 928,\n",
       " 'clearly': 929,\n",
       " 'average': 930,\n",
       " 'agency': 931,\n",
       " 'employer': 932,\n",
       " 'choose': 933,\n",
       " 'build': 934,\n",
       " 'achieve': 935,\n",
       " 'solve': 936,\n",
       " 'burden': 937,\n",
       " 'leave': 938,\n",
       " 'letter': 939,\n",
       " 'wants': 940,\n",
       " 'oppose': 941,\n",
       " 'threat': 942,\n",
       " 'term': 943,\n",
       " 'diabetes': 944,\n",
       " 'foreign': 945,\n",
       " 'nutrition': 946,\n",
       " 'building': 947,\n",
       " 'proposals': 948,\n",
       " 'show': 949,\n",
       " 'news': 950,\n",
       " 'afghanistan': 951,\n",
       " 'covered': 952,\n",
       " 'science': 953,\n",
       " 'price': 954,\n",
       " 'secretary': 955,\n",
       " 'wyoming': 956,\n",
       " 'giving': 957,\n",
       " 'goals': 958,\n",
       " 'pretty': 959,\n",
       " 'tremendous': 960,\n",
       " 'employment': 961,\n",
       " 'overall': 962,\n",
       " 'hearing': 963,\n",
       " 'chart': 964,\n",
       " 'cutting': 965,\n",
       " 'breast': 966,\n",
       " 'held': 967,\n",
       " 'received': 968,\n",
       " 'considering': 969,\n",
       " 'rule': 970,\n",
       " 'effect': 971,\n",
       " 'iraq': 972,\n",
       " 'interest': 973,\n",
       " 'gives': 974,\n",
       " 'focused': 975,\n",
       " 'affect': 976,\n",
       " 'story': 977,\n",
       " 'welfare': 978,\n",
       " 'either': 979,\n",
       " 'steps': 980,\n",
       " 'strengthen': 981,\n",
       " 'considered': 982,\n",
       " '17': 983,\n",
       " 'population': 984,\n",
       " 'groups': 985,\n",
       " 'old': 986,\n",
       " 'age': 987,\n",
       " 'peoples': 988,\n",
       " 'd': 989,\n",
       " 'progress': 990,\n",
       " 'chair': 991,\n",
       " 'appropriate': 992,\n",
       " 'rest': 993,\n",
       " 'remarks': 994,\n",
       " 'started': 995,\n",
       " 'began': 996,\n",
       " 'comments': 997,\n",
       " 'loss': 998,\n",
       " 'purchase': 999,\n",
       " 'management': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17985"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13302"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = tokenizer.texts_to_sequences(sample_df['speech'].values)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = WINDOW_DEFAULT + 1\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 155,   25,  297, ...,    0,    0,    0],\n",
       "       [   5,   10, 2508, ...,    0,    0,    0],\n",
       "       [   1,   20,   10, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  29, 1615,  183, ...,    0,    0,    0],\n",
       "       [ 601,   38,  595, ...,    0,    0,    0],\n",
       "       [ 444,    5,  131, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the sentences need to be in integer-tokenized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Iyyer et el.\n",
    "\n",
    "\"Each input to the RMN is a tuple that contains identifiers for a book and two character, as well as the spans corresponding to their relationship: $(b, c_1, c_2, S_{c_1,c_2})$. Given one such input, our objective is to reconstruct $S_(c_1,c_2)$ using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needs for Baseline goal\n",
    "\n",
    "Let...\n",
    "* $s_{v_t}$ be the $t_{th}$ span of text in the span set $S_{c_1,c_2}$\n",
    "* $v_{s_t}$ be the vector that results from taking the element-wise average of the word vectors in $s_{v_t}$\n",
    "* $C$ be the set metadata embeddings\n",
    "* $m_{t,c}$ be the metadata embeddings vector for metadata $c$ with \n",
    "* $d$ be the dimension of the embedding\n",
    "* $k$ be the number of decsriptors\n",
    "\n",
    "\n",
    "Compute Sequence: Given $s_{v_t}$, do the following steps:\n",
    "1. compute avg speech vector, $v_{s_t}$,\n",
    "    * $v_{s_t} \\in \\mathbb{R}^{d}$\n",
    "2. concat avg span and metadate embeddings\n",
    "    * $ m_{t,c} \\in \\mathbb{R}^{d}$\n",
    "    * [$v_{s_t}; m_{t,1};...; m_{t,|C|}$]\n",
    "2. compute hidden state with Relu activation: \n",
    "    * $h_t =  relu \\space (W_h \\cdot [v_{s_t}; m_{t,1};...; m_{t,|C|}])$\n",
    "    * $W_h \\in \\mathbb{R}^{d \\times (d + d|C|)}$ \n",
    "    * $h_t \\in  \\mathbb{R}^{d}$\n",
    "3. get distribution over topics using another hidden layer: \n",
    "    * $d_t = softmax \\space (W_d \\cdot h_t)$\n",
    "    * $W_d \\in  \\mathbb{R}^{k \\times d}$\n",
    "    * $d_t \\in  \\mathbb{R}^{k}$\n",
    "    * $d_{t,i} \\in (0,1) \\space \\forall i$ \n",
    "4. recompose original sentence using the distribution over descriptors and the descriptor matrix:\n",
    "    * $r_t = R^Td_t$\n",
    "    * $R^T \\in \\mathbb{R}^{d \\times k}$\n",
    "    * $r_t \\in \\mathbb{R}^{d}$\n",
    "5. score distance between $r_t$ and $v_{s_t}$\n",
    "    * $distance = dist(r_t, v_{s_t})$\n",
    "    \n",
    "    \n",
    "#### Notes on implementing it with keras\n",
    "Every step that uses a matrix multiplication above can be implemented in keras using a dense layer, formatted like this:\n",
    "* `h = keras.layers.Dense(units = a, input_shape = (b, ), activation= \"the_activation\")(prev_layer)`\n",
    "    * This will make the dense layer use a weight matrix $W \\in \\mathbb{R}^{a \\times b}$, and activation \"`the_activation`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe embeddings are on a local VM, and are not yet in `gs://rwc1/embeddings/`. Attemtps to access embeddings from the gcloud bucket had bugs. You can find the embeddings used [here](https://nlp.stanford.edu/projects/glove/), which are the Wikipedia + Gigaword 5 trained embeddings with 6 billion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "GLOVE_DIMS = [50, 100, 200, 300]\n",
    "EMBEDDING_DIM = GLOVE_DIMS[0]\n",
    "\n",
    "embeddings_index = {}\n",
    "glove = open('../../../glove/glove.6B.%dd.txt' % EMBEDDING_DIM)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except Exception as e:\n",
    "        print(values[1:])\n",
    "        raise\n",
    "        \n",
    "    embeddings_index[word] = coefs\n",
    "glove.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13302, 50)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average of spane embeddings\n",
    "Vst_train = embedding_matrix[x_train_padded].mean(axis=1)\n",
    "Vst_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'chamber', 'state', 'gender', 'party'])"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker metadata inputs\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "for col in speak_map_cols:\n",
    "    df = sample_df[sample_df[col].unique()].values\n",
    "    dim = df.shape[1]\n",
    "    metadata_dict[col] = {'input': df, 'input_dim': dim}\n",
    "\n",
    "metadata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input avg span embeddings\n",
    "vt = Input(shape=(EMBEDDING_DIM,), name='Avg.Span.Embed.Input')\n",
    "\n",
    "# masking layer to account for padding\n",
    "masking_layer = Masking(mask_value=0.0, input_shape = (EMBEDDING_DIM,), name = \"Mask\")(vt)\n",
    "\n",
    "## initializing speaker metadata embeddings\n",
    "\n",
    "input_layers = [vt]\n",
    "embedding_layers = [masking_layer]\n",
    "for col in speak_map_cols:\n",
    "    input_layer = Input(shape=(metadata_dict[col]['input_dim'],), name= col + '.Embed.Input')\n",
    "    embedding_init = (Dense(units = EMBEDDING_DIM,\n",
    "                            kernel_initializer = 'glorot_normal',\n",
    "                            input_shape = (metadata_dict[col]['input_dim'], ),\n",
    "                            activation = \"linear\",\n",
    "                            name = 'W_' + col)(input_layer))\n",
    "    input_layers.append(input_layer)\n",
    "    embedding_layers.append(embedding_init)\n",
    "\n",
    "# concat speaker metadata embeddings\n",
    "_ht = tf.keras.layers.concatenate(embedding_layers, axis=1, name = 'Concat.Layer')\n",
    "\n",
    "# dense layer\n",
    "ht = Dense(units = EMBEDDING_DIM, input_shape = (_ht.shape[1], ), activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "# dense layer with softmax activation, (where previous states will eventually be inserted) \n",
    "dt = Dense(units = k, input_shape = (EMBEDDING_DIM, ), activation = \"softmax\", name = \"Wd\")(ht)\n",
    "\n",
    "# reconstruction layer\n",
    "rt = Dense(units = EMBEDDING_DIM, input_shape = (k, ), activation = \"linear\", name = \"R\")(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "model.compile(optimizer = 'adam', loss=\"hinge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Avg.Span.Embed.Input (InputLaye (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "speakerid.Embed.Input (InputLay (None, 536)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chamber.Embed.Input (InputLayer (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "state.Embed.Input (InputLayer)  (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gender.Embed.Input (InputLayer) (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "party.Embed.Input (InputLayer)  (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Mask (Masking)                  (None, 50)           0           Avg.Span.Embed.Input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "W_speakerid (Dense)             (None, 50)           26850       speakerid.Embed.Input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "W_chamber (Dense)               (None, 50)           150         chamber.Embed.Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "W_state (Dense)                 (None, 50)           2850        state.Embed.Input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "W_gender (Dense)                (None, 50)           150         gender.Embed.Input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "W_party (Dense)                 (None, 50)           200         party.Embed.Input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Concat.Layer (Concatenate)      (None, 300)          0           Mask[0][0]                       \n",
      "                                                                 W_speakerid[0][0]                \n",
      "                                                                 W_chamber[0][0]                  \n",
      "                                                                 W_state[0][0]                    \n",
      "                                                                 W_gender[0][0]                   \n",
      "                                                                 W_party[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Wh (Dense)                      (None, 50)           15050       Concat.Layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Wd (Dense)                      (None, 20)           1020        Wh[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "R (Dense)                       (None, 50)           1050        Wd[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 47,320\n",
      "Trainable params: 47,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13302/13302 [==============================] - 10s 747us/sample - loss: 0.9715\n",
      "Epoch 2/5\n",
      "13302/13302 [==============================] - 4s 286us/sample - loss: 0.9291\n",
      "Epoch 3/5\n",
      "13302/13302 [==============================] - 4s 293us/sample - loss: 0.8956\n",
      "Epoch 4/5\n",
      "13302/13302 [==============================] - 4s 291us/sample - loss: 0.8661\n",
      "Epoch 5/5\n",
      "13302/13302 [==============================] - 4s 295us/sample - loss: 0.8387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa98fa59250>"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [Vst_train]\n",
    "for key in metadata_dict.keys():\n",
    "    inputs.append(metadata_dict[key]['input'])\n",
    "\n",
    "model.fit(x=inputs, y=Vst_train, batch_size=25, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in model.layers:\n",
    "#     print(l)\n",
    "#     print(50*\"=\")\n",
    "#     print(\"input shape\", l.input_shape)\n",
    "#     print(\"output shape\", l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47553602,  0.41967615,  0.40148813, ...,  0.5893099 ,\n",
       "        -0.39425093,  0.29394513],\n",
       "       [ 0.47567827,  0.41992933,  0.40180397, ...,  0.5896522 ,\n",
       "        -0.39440036,  0.29401407],\n",
       "       [ 0.47560775,  0.4197976 ,  0.4016418 , ...,  0.5894731 ,\n",
       "        -0.39432645,  0.29397035],\n",
       "       ...,\n",
       "       [ 0.47676247,  0.42206305,  0.4044199 , ...,  0.5924783 ,\n",
       "        -0.3954901 ,  0.29482332],\n",
       "       [ 0.47675496,  0.42203835,  0.40439522, ...,  0.59245217,\n",
       "        -0.3954876 ,  0.29480982],\n",
       "       [ 0.47676533,  0.42207053,  0.40442747, ...,  0.5924863 ,\n",
       "        -0.3954913 ,  0.29482657]], dtype=float32)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07116484,  0.00912573,  0.06147658, ...,  0.00436425,\n",
       "        -0.02306813, -0.00891188],\n",
       "       [ 0.0666854 ,  0.01963167, -0.00729108, ...,  0.03347551,\n",
       "         0.00260285,  0.03218179],\n",
       "       [ 0.05064938,  0.00156331,  0.00330125, ...,  0.03080652,\n",
       "        -0.0103821 ,  0.021787  ],\n",
       "       ...,\n",
       "       [ 0.06979422,  0.00540201, -0.0045938 , ...,  0.01659401,\n",
       "        -0.01673172, -0.02367188],\n",
       "       [ 0.04147102,  0.011088  ,  0.00466978, ...,  0.00896139,\n",
       "        -0.01126754,  0.00458058],\n",
       "       [ 0.05942265,  0.01376962,  0.01775837, ...,  0.01814727,\n",
       "        -0.01915259,  0.0160123 ]])"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vst_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
