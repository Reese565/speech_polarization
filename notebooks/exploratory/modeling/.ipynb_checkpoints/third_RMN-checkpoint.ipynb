{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# assembly imports\n",
    "os.chdir(\"../../../scripts/assembly\")\n",
    "from session_speaker_assembly import *\n",
    "from preprocess import *\n",
    "from document import *\n",
    "from constant import SPEECHES, SPEAKER_MAP, HB_PATH, EMBEDDINGS\n",
    "\n",
    "# modeling imports\n",
    "os.chdir(\"../modeling\")\n",
    "from token_mapping import ohe_attribures, build_tokenizer_dict, build_metadata_dict\n",
    "os.chdir(\"../modeling\")\n",
    "from embeddings import *\n",
    "os.chdir(\"../modeling\")\n",
    "from rmn import RMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building importing subject documents\n",
    "subject_df = pd.read_csv('../../data/gen-docs/documents_health.txt', sep = '|')\n",
    "feature_columns = subject_df.columns.drop('speech')\n",
    "subject_df = ohe_attribures(subject_df)\n",
    "token_dict = build_tokenizer_dict(subject_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 535 Members of Congress. 100 serve in the U.S. Senate and 435 serve in the U.S. House of Representatives. A length of 50 suggests that nearly everyone commented on \"health\" (in a speech of more than 50 words) at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13136"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_word_index = token_dict['speech']['word_index']\n",
    "vocab_size = len(speeches_word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2177"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_train = token_dict['speech']['train']\n",
    "len(speeches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1334,     1,   162, ...,     0,     0,     0],\n",
       "       [    1,   162,    51, ...,     0,     0,     0],\n",
       "       [   14,    57,  2819, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  102,     2,    60, ...,     0,     0,     0],\n",
       "       [    2,   285, 13117, ...,     0,     0,     0],\n",
       "       [   54,   203, 13135, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_train_padded = token_dict['speech']['train_padded']\n",
    "speeches_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the sentences need to be in integer-tokenized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Iyyer et el.\n",
    "\n",
    "\"Each input to the RMN is a tuple that contains identifiers for a book and two character, as well as the spans corresponding to their relationship: $(b, c_1, c_2, S_{c_1,c_2})$. Given one such input, our objective is to reconstruct $S_(c_1,c_2)$ using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needs for Baseline goal\n",
    "\n",
    "Let...\n",
    "* $s_{v_t}$ be the $t_{th}$ span of text in the span set $S_{c_1,c_2}$\n",
    "* $v_{s_t}$ be the vector that results from taking the element-wise average of the word vectors in $s_{v_t}$\n",
    "* $C$ be the set metadata embeddings\n",
    "* $m_{t,c}$ be the metadata embeddings vector for metadata $c$ with \n",
    "* $d$ be the dimension of the embedding\n",
    "* $k$ be the number of decsriptors\n",
    "\n",
    "\n",
    "Compute Sequence: Given $s_{v_t}$, do the following steps:\n",
    "1. compute avg speech vector, $v_{s_t}$,\n",
    "    * $v_{s_t} \\in \\mathbb{R}^{d}$\n",
    "2. concat avg span and metadate embeddings\n",
    "    * $ m_{t,c} \\in \\mathbb{R}^{d}$\n",
    "    * [$v_{s_t}; m_{t,1};...; m_{t,|C|}$]\n",
    "2. compute hidden state with Relu activation: \n",
    "    * $h_t =  relu \\space (W_h \\cdot [v_{s_t}; m_{t,1};...; m_{t,|C|}])$\n",
    "    * $W_h \\in \\mathbb{R}^{d \\times (d + d|C|)}$ \n",
    "    * $h_t \\in  \\mathbb{R}^{d}$\n",
    "3. get distribution over topics using another hidden layer: \n",
    "    * $d_t = softmax \\space (W_d \\cdot h_t)$\n",
    "    * $W_d \\in  \\mathbb{R}^{k \\times d}$\n",
    "    * $d_t \\in  \\mathbb{R}^{k}$\n",
    "    * $d_{t,i} \\in (0,1) \\space \\forall i$ \n",
    "4. recompose original sentence using the distribution over descriptors and the descriptor matrix:\n",
    "    * $r_t = R^Td_t$\n",
    "    * $R^T \\in \\mathbb{R}^{d \\times k}$\n",
    "    * $r_t \\in \\mathbb{R}^{d}$\n",
    "5. score distance between $r_t$ and $v_{s_t}$\n",
    "    * $distance = dist(r_t, v_{s_t})$\n",
    "    \n",
    "    \n",
    "#### Notes on implementing it with keras\n",
    "Every step that uses a matrix multiplication above can be implemented in keras using a dense layer, formatted like this:\n",
    "* `h = keras.layers.Dense(units = a, input_shape = (b, ), activation= \"the_activation\")(prev_layer)`\n",
    "    * This will make the dense layer use a weight matrix $W \\in \\mathbb{R}^{a \\times b}$, and activation \"`the_activation`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe embeddings are on a local VM, and are not yet in `gs://rwc1/embeddings/`. Attemtps to access embeddings from the gcloud bucket had bugs. You can find the embeddings used [here](https://nlp.stanford.edu/projects/glove/), which are the Wikipedia + Gigaword 5 trained embeddings with 6 billion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# this cell is if you have the embeddings files stored localled\n",
    "\n",
    "NUM_TOPICS = 20\n",
    "GLOVE_DIMS = [50, 100, 200, 300]\n",
    "EMBEDDING_DIM = GLOVE_DIMS[0]\n",
    "\n",
    "embeddings_index = {}\n",
    "glove = open('../../data/glove/glove.6B.%dd.txt' % EMBEDDING_DIM)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except Exception as e:\n",
    "        print(values[1:])\n",
    "        raise\n",
    "        \n",
    "    embeddings_index[word] = coefs\n",
    "glove.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embeddings_matrix = np.zeros((len(speeches_word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in speeches_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run these cell to use embeddings from gcloud bucket\n",
    "# warning: this takes longer than above\n",
    "\n",
    "## build embedding matrix\n",
    "# embeddings_index = fetch_embeddings()\n",
    "# embeddings_matrix = build_embedding_matrix(speeches_word_index, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speakerid', 'party', 'congress', 'subject'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict = build_metadata_dict(feature_columns, subject_df)\n",
    "metadata_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "speakerid.Embed.Input (InputLay (None, 583)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "party.Embed.Input (InputLayer)  (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "congress.Embed.Input (InputLaye (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subject.Embed.Input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Avg.Span.Embed.Input (InputLaye (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C_speakerid (Dense)             (None, 50)           29200       speakerid.Embed.Input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "C_party (Dense)                 (None, 50)           300         party.Embed.Input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "C_congress (Dense)              (None, 50)           200         congress.Embed.Input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "C_subject (Dense)               (None, 50)           100         subject.Embed.Input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Concat.Layer (Concatenate)      (None, 250)          0           Avg.Span.Embed.Input[0][0]       \n",
      "                                                                 C_speakerid[0][0]                \n",
      "                                                                 C_party[0][0]                    \n",
      "                                                                 C_congress[0][0]                 \n",
      "                                                                 C_subject[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Wh (Dense)                      (None, 50)           12550       Concat.Layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Wd (Dense)                      (None, 20)           1020        Wh[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "R (Dense)                       (None, 50)           1050        Wd[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 44,420\n",
      "Trainable params: 44,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(565)\n",
    "model = RMN().build_model(metadata_dict)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/reese56/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "2177/2177 [==============================] - 1s 483us/sample - loss: 8.0050\n",
      "Epoch 2/10\n",
      "2177/2177 [==============================] - 0s 79us/sample - loss: 7.8508\n",
      "Epoch 3/10\n",
      "2177/2177 [==============================] - 0s 81us/sample - loss: 7.7734\n",
      "Epoch 4/10\n",
      "2177/2177 [==============================] - 0s 80us/sample - loss: 7.7296\n",
      "Epoch 5/10\n",
      "2177/2177 [==============================] - 0s 77us/sample - loss: 7.7060\n",
      "Epoch 6/10\n",
      "2177/2177 [==============================] - 0s 77us/sample - loss: 7.7011\n",
      "Epoch 7/10\n",
      "2177/2177 [==============================] - 0s 77us/sample - loss: 7.6946\n",
      "Epoch 8/10\n",
      "2177/2177 [==============================] - 0s 79us/sample - loss: 7.6893\n",
      "Epoch 9/10\n",
      "2177/2177 [==============================] - 0s 79us/sample - loss: 7.6866\n",
      "Epoch 10/10\n",
      "2177/2177 [==============================] - 0s 81us/sample - loss: 7.6821\n"
     ]
    }
   ],
   "source": [
    "model.compile_RMN()\n",
    "model.ingest_inputs(embeddings_matrix, speeches_train_padded)\n",
    "model.fit(batch_size=50, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
