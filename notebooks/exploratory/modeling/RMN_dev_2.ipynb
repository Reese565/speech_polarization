{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RMN on all session documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/rocassius/w266_final/scripts/assembly\")\n",
    "sys.path.append(\"/home/rocassius/w266_final/scripts/modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document import load_documents\n",
    "from constant import DOC_ALL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_pickled_object\n",
    "from rmn import RMN\n",
    "from rmn_data_generator import RMN_DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding tools\n",
    "local_tools_path = '/home/rocassius/gen-data/tools'\n",
    "tokenizer_dict = load_pickled_object(os.path.join(local_tools_path, \"global_tokenizer_dict\"))\n",
    "metadata_dict = load_pickled_object(os.path.join(local_tools_path, \"global_metadata_dict\"))\n",
    "embedding_matrix = load_pickled_object(os.path.join(local_tools_path, \"global_embedding_matrix_50d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "docs_df = load_documents([111], DOC_ALL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subject import subject_keywords\n",
    "from constant import DOC_SAMPLE_PATH\n",
    "DOC = 'documents_%s.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.concat([pd.read_csv(os.path.join(DOC_SAMPLE_PATH, DOC % s), sep = \"|\") \n",
    "                     for s in list(subject_keywords.keys())[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.rename(columns={'congress':'session'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speakerid    float64\n",
       "lastname      object\n",
       "firstname     object\n",
       "chamber       object\n",
       "state         object\n",
       "gender        object\n",
       "party         object\n",
       "document      object\n",
       "session        int64\n",
       "subject       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct types\n",
    "docs_df['speakerid'] = docs_df['speakerid'].astype(int).astype(str)\n",
    "docs_df['session'] = docs_df['session'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>document</th>\n",
       "      <th>session</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>105111840</td>\n",
       "      <td>GINGRICH</td>\n",
       "      <td>NEWTON</td>\n",
       "      <td>H</td>\n",
       "      <td>GA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>lyndon johnsons former advisor jimmy carters s...</td>\n",
       "      <td>105</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>105110921</td>\n",
       "      <td>ASHCROFT</td>\n",
       "      <td>JOHN</td>\n",
       "      <td>S</td>\n",
       "      <td>MO</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>defective products placing burden responsible ...</td>\n",
       "      <td>105</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>105114091</td>\n",
       "      <td>DODD</td>\n",
       "      <td>CHRISTOPHER</td>\n",
       "      <td>S</td>\n",
       "      <td>CT</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>supervision devastating longterm impact one st...</td>\n",
       "      <td>105</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>105111431</td>\n",
       "      <td>MOYNIHAN</td>\n",
       "      <td>DANIEL</td>\n",
       "      <td>S</td>\n",
       "      <td>NY</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>protracted series negotiations ensued able upd...</td>\n",
       "      <td>105</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>105116401</td>\n",
       "      <td>INOUYE</td>\n",
       "      <td>DANIEL</td>\n",
       "      <td>S</td>\n",
       "      <td>HI</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>days hospitalization  emergency room visits vi...</td>\n",
       "      <td>105</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163292</td>\n",
       "      <td>111121290</td>\n",
       "      <td>SCOTT</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>H</td>\n",
       "      <td>VA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>time consume first want well introducing impor...</td>\n",
       "      <td>111</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163293</td>\n",
       "      <td>111121290</td>\n",
       "      <td>SCOTT</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>H</td>\n",
       "      <td>VA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>provisions false claims act order effectively ...</td>\n",
       "      <td>111</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163294</td>\n",
       "      <td>111121290</td>\n",
       "      <td>SCOTT</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>H</td>\n",
       "      <td>VA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>consensus worked together bipartisan basis sen...</td>\n",
       "      <td>111</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163295</td>\n",
       "      <td>111121290</td>\n",
       "      <td>SCOTT</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>H</td>\n",
       "      <td>VA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>yielding along colleague would like briefly di...</td>\n",
       "      <td>111</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163296</td>\n",
       "      <td>111116930</td>\n",
       "      <td>LEWIS</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>H</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>established san bernardino manufacture equipme...</td>\n",
       "      <td>111</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1997661 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        speakerid  lastname    firstname chamber state gender party  \\\n",
       "0       105111840  GINGRICH       NEWTON       H    GA      M     R   \n",
       "1       105110921  ASHCROFT         JOHN       S    MO      M     R   \n",
       "2       105114091      DODD  CHRISTOPHER       S    CT      M     D   \n",
       "3       105111431  MOYNIHAN       DANIEL       S    NY      M     D   \n",
       "4       105116401    INOUYE       DANIEL       S    HI      M     D   \n",
       "...           ...       ...          ...     ...   ...    ...   ...   \n",
       "163292  111121290     SCOTT       ROBERT       H    VA      M     D   \n",
       "163293  111121290     SCOTT       ROBERT       H    VA      M     D   \n",
       "163294  111121290     SCOTT       ROBERT       H    VA      M     D   \n",
       "163295  111121290     SCOTT       ROBERT       H    VA      M     D   \n",
       "163296  111116930     LEWIS        JERRY       H    CA      M     R   \n",
       "\n",
       "                                                 document session  subject  \n",
       "0       lyndon johnsons former advisor jimmy carters s...     105  alcohol  \n",
       "1       defective products placing burden responsible ...     105  alcohol  \n",
       "2       supervision devastating longterm impact one st...     105  alcohol  \n",
       "3       protracted series negotiations ensued able upd...     105  alcohol  \n",
       "4       days hospitalization  emergency room visits vi...     105  alcohol  \n",
       "...                                                   ...     ...      ...  \n",
       "163292  time consume first want well introducing impor...     111    trade  \n",
       "163293  provisions false claims act order effectively ...     111    trade  \n",
       "163294  consensus worked together bipartisan basis sen...     111    trade  \n",
       "163295  yielding along colleague would like briefly di...     111    trade  \n",
       "163296  established san bernardino manufacture equipme...     111    trade  \n",
       "\n",
       "[1997661 rows x 10 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = docs_df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_mapping import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#os.chdir(\"../assembly/\")\n",
    "from constant import EMBEDDINGS\n",
    "\n",
    "\n",
    "# constants\n",
    "GLOVE_DIMS = [50, 100, 200, 300]\n",
    "EMBEDDING_DIM = GLOVE_DIMS[0]\n",
    "GLOVE_PATH = os.path.join(EMBEDDINGS, 'glove6B/glove.6B.%dd.csv' % EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "def fetch_embeddings(embeddings_dim = EMBEDDING_DIM):\n",
    "    \n",
    "    path = os.path.join(EMBEDDINGS, 'glove6B/glove.6B.%dd.csv' % EMBEDDING_DIM)\n",
    "    \n",
    "    embeddings_index = pd.read_csv(path).to_dict(orient = 'list')\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "\n",
    "def build_embedding_matrix(word_index, embeddings_index, stopwords=[]):\n",
    "    \n",
    "    # get the embedding dimension\n",
    "    embedding_dim = len(embeddings_index['the'])\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None and word not in stopwords:\n",
    "            # words not found in embedding index and stopwords will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector \n",
    "            \n",
    "    embedding_matrix = embedding_matrix.astype('float16')\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dict = build_tokenizer_dict(docs_df, max_span_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['speakerid', 'chamber', 'state', 'gender','party', 'session', 'subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = build_metadata_dict(docs_df, metadata_columns=meta_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = fetch_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding_matrix(tokenizer_dict['word_index'], embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================#\n",
    "#=*= RMN Module =*=#\n",
    "#==================#\n",
    "\n",
    "# RMN Class for training Relationship Modeling Networks \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda, Input, Masking, Reshape, Concatenate\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "\n",
    "from rmn_data_generator import RMN_DataGenerator\n",
    "from helper import pickle_object, load_pickled_object\n",
    "from vector_math import find_nn_cos\n",
    "\n",
    "# constants\n",
    "MAX_SPAN_LENGTH = 50\n",
    "NUM_TOPICS = 20\n",
    "LAMBDA = 1.0\n",
    "GAMMA = 1.0\n",
    "\n",
    "# hyperparameters\n",
    "OPTIMIZER = 'adam'\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "\n",
    "# saving tags\n",
    "RMN_TAG = \"rmn_%s\"\n",
    "MODEL = \"model.h5\"\n",
    "ATTR = \"attributes\"\n",
    "\n",
    "# attribute keys\n",
    "N_TOP_KEY = 'num_topics'\n",
    "LAMB_KEY  = 'lambda'\n",
    "EMBED_KEY = 'emedding_matrix'\n",
    "TOKEN_KEY = 'tokenizer_dict'\n",
    "META_KEY  = 'metadata_dict'\n",
    "\n",
    "\n",
    "class RMN(object):\n",
    "    \"\"\"\n",
    "    Class for constructing a Relationship Modeling Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # model parameters\n",
    "        self.num_topics = NUM_TOPICS\n",
    "        self.lamb = LAMBDA\n",
    "        self.gamma = GAMMA\n",
    "        \n",
    "        # model attrbiutes\n",
    "        self.embedding_matrix = None\n",
    "        self.tokenizer_dict = None\n",
    "        self.metadata_dict = None\n",
    "        \n",
    "        # models \n",
    "        self.model = None\n",
    "        self.topic_model = None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.embedding_matrix.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def topic_matrix(self):\n",
    "        \"\"\"Return the topic matrix associated with the rmn\"\"\"\n",
    "        # dim = [num_topics, embedding_dim]\n",
    "        return self.model.get_layer('Wd').get_weights()[0].T\n",
    "    \n",
    "    @property\n",
    "    def tuned_embedding_matrix(self):\n",
    "        \"\"\"Return the current embedding matrix of the rmn\"\"\"\n",
    "        return rmn.model.get_layer('Span.Embedding').get_weights()[0]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"Hinge loss function.\n",
    "        \"\"\"\n",
    "        # def sample_spans()\n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            \n",
    "            # hinge_loss\n",
    "            y_true_normalized = K.l2_normalize(y_true, axis=-1)\n",
    "            y_pred_normalized = K.l2_normalize(y_pred, axis=-1)\n",
    "            dot_product = K.sum(y_true_normalized * y_pred_normalized, axis=-1)\n",
    "            hinge_loss = K.mean(K.maximum(0., 1. - dot_product))\n",
    "\n",
    "            return hinge_loss \n",
    "\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "    def build_model(self, embedding_trainable=False):\n",
    "        \"\"\"Connstruct the RMN model architecture\n",
    "        \"\"\"\n",
    "        # Span Input\n",
    "        span_input = Input(shape=(self.tokenizer_dict['max_span_length'],), \n",
    "                           name='Span.Input')\n",
    "        span_embedding = Embedding(input_dim=len(self.tokenizer_dict['word_index']) + 1, \n",
    "                                   output_dim=self.embedding_dim, \n",
    "                                   weights=[self.embedding_matrix],\n",
    "                                   input_length=self.tokenizer_dict['max_span_length'],\n",
    "                                   trainable=embedding_trainable, \n",
    "                                   name = 'Span.Embedding')(span_input)\n",
    "        \n",
    "        # Take elementwise average over vectors\n",
    "        span_avg = Lambda(lambda x: K.mean(x, axis=1), name = \"Span.Avg.Layer\")(span_embedding)\n",
    "\n",
    "        input_layers = [span_input]\n",
    "        embedding_layers = [span_avg]\n",
    "        \n",
    "        for col in self.metadata_dict.keys():\n",
    "            \n",
    "            input_layer = Input(shape=(1,), name= col + '.Input')\n",
    "            \n",
    "            # embedding layer for col\n",
    "            embedding_init = Embedding(\n",
    "                input_dim = self.metadata_dict[col]['input_dim'] + 1, \n",
    "                output_dim = self.embedding_dim,\n",
    "                input_length = 1)(input_layer)\n",
    "            \n",
    "            # reshape\n",
    "            embedding_layer = Reshape((self.embedding_dim, ), name=col + '.Embed.Layer')(embedding_init)\n",
    "            \n",
    "            input_layers.append(input_layer)\n",
    "            embedding_layers.append(embedding_layer)\n",
    "\n",
    "        # concatenate span vector with metadata embeddings\n",
    "        _ht = Concatenate(axis=1, name = 'Concat.Layer')(embedding_layers)\n",
    "\n",
    "        # dense layer\n",
    "        ht = Dense(units = self.embedding_dim, \n",
    "                   input_shape = (_ht.shape[1], ), \n",
    "                   activation = \"relu\", name = \"Wh\")(_ht)\n",
    "\n",
    "        # dense layer whose output is a probability distribution\n",
    "        dt = Dense(units = self.num_topics, \n",
    "                   input_shape = (self.embedding_dim, ), \n",
    "                   activation = \"softmax\", \n",
    "                   activity_regularizer = Purity(self.gamma),\n",
    "                   name = \"Wd\")(ht)\n",
    "\n",
    "        # reconstruction layer\n",
    "        rt = Dense(units = self.embedding_dim,\n",
    "                   input_shape = (self.num_topics, ),\n",
    "                   activation = \"linear\",\n",
    "                   kernel_regularizer = Orthogonality(self.lamb),\n",
    "                   name = \"R\")(dt)\n",
    "\n",
    "        # compile\n",
    "        model = tf.keras.Model(inputs=input_layers, outputs=rt)\n",
    "        model.compile(optimizer = OPTIMIZER, loss = self.model_loss())\n",
    "        self.model = model\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "        \n",
    "    def build_topic_model(self, topic_layer = \"Wd\"):\n",
    "        \"\"\"Contruct model whose output is the topic distribution layer\n",
    "        \"\"\"\n",
    "        topic_model = tf.keras.Model(\n",
    "            inputs = self.model.input,\n",
    "            outputs = self.model.get_layer(topic_layer).output)\n",
    "        \n",
    "        self.topic_model = topic_model\n",
    "          \n",
    "    \n",
    "    def prep_spans(self, documents):\n",
    "        \"\"\"Returns the lists of word ids associated with the text\n",
    "        \"\"\"\n",
    "        return self.tokenizer_dict['tokenize_pad'](documents)\n",
    "    \n",
    "    \n",
    "    def prep_metadata(self, df):\n",
    "        \"\"\"Preps metadata for training or prediction\n",
    "        \"\"\"\n",
    "        metadata_x = [np.array(self.metadata_dict[col]['tokenize'](df[col]))\n",
    "                      for col in self.metadata_dict.keys()]\n",
    "\n",
    "        return metadata_x\n",
    "        \n",
    "    \n",
    "    def prep_X(self, df, for_training=False):\n",
    "        \"\"\"Preps metadata and spans for training or prediction\n",
    "        \"\"\"\n",
    "        spans_y = self.prep_spans(df['document'])\n",
    "        metadata_x = self.prep_metadata(df)\n",
    "        X = [spans_y] + metadata_x\n",
    "        \n",
    "        if for_training:\n",
    "            y = self.embedding_matrix[spans_y].mean(axis=1)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def predict_y(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the rmn outputs for a df\n",
    "        \"\"\"\n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.model)\n",
    "        else:\n",
    "            return self.predict_(df, self.model)\n",
    "    \n",
    "    \n",
    "    def predict_topics(self, df, use_generator=True):\n",
    "        \"\"\"Predicts the topic distributions for a df\n",
    "        \"\"\"        \n",
    "        # ensure the topic model has been built\n",
    "        if self.topic_model is None:\n",
    "            self.build_topic_model()\n",
    "        \n",
    "        if use_generator:\n",
    "            return self.predict_with_generator(df, self.topic_model)\n",
    "        else:\n",
    "            return self.predict_(df, self.topic_model)\n",
    "\n",
    "        \n",
    "    def predict_(self, df, model):\n",
    "        \"\"\"Makes a predictions for a df with a model\n",
    "        \"\"\"\n",
    "        return model.predict(x=self.prep_X(df))\n",
    "        \n",
    "    \n",
    "    def predict_with_generator(self, df, model):\n",
    "        \"\"\"Predict topic distributions with a generator\n",
    "        \"\"\"\n",
    "        # Make sure data is not empty\n",
    "        assert not df.empty\n",
    "\n",
    "        # Calculate good batch size, \n",
    "        batch_size = max(1, min(10000, df.shape[0] // 10))\n",
    "        n_batches = df.shape[0] // batch_size\n",
    "\n",
    "        if n_batches < 2: \n",
    "            return self.predict_(df, model)\n",
    "        else:\n",
    "            # calculate remainder batch size\n",
    "            r = df.shape[0] % batch_size\n",
    "            if r == 0:\n",
    "                g_index = df.index[:-batch_size]\n",
    "                r_index = df.index[-batch_size:]\n",
    "            else:\n",
    "                g_index = df.index[:-r]\n",
    "                r_index = df.index[-r:]\n",
    "\n",
    "            # Make generator\n",
    "            g = RMN_DataGenerator(self, df.loc[g_index], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Predict on remainder batch\n",
    "            r_pred = self.predict_(df.loc[r_index], model)\n",
    "            # predict on generated batches\n",
    "            g_pred = model.predict_generator(g, use_multiprocessing=True, workers=10, verbose=1)\n",
    "\n",
    "            assert r_pred.shape[1] == g_pred.shape[1]\n",
    "            pred = np.vstack([g_pred, r_pred])\n",
    "\n",
    "            return pred\n",
    "        \n",
    "    \n",
    "    def save_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Save the model's weights, architecture and attributes\n",
    "        \"\"\"\n",
    "        # assemble attribute dictionary\n",
    "        attribute_dict = {\n",
    "            N_TOP_KEY:  self.num_topics,\n",
    "            LAMB_KEY:   self.lamb,\n",
    "            EMBED_KEY:  self.embedding_matrix,\n",
    "            TOKEN_KEY:  self.tokenizer_dict,\n",
    "            META_KEY:   self.metadata_dict}\n",
    "        \n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        # save model weights\n",
    "        self.model.save_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # save model attributes\n",
    "        pickle_object(attribute_dict, os.path.join(model_path, ATTR))\n",
    "        \n",
    "        \n",
    "    def load_rmn(self, name, save_path):\n",
    "        \"\"\"\n",
    "        Load the model, weights, architecture and attributes from a saved model\n",
    "        \"\"\"\n",
    "        # make directory for model\n",
    "        model_path = os.path.join(save_path, RMN_TAG % name)\n",
    "        \n",
    "        # load attributes\n",
    "        attributes_dict = load_pickled_object(os.path.join(model_path, ATTR))\n",
    "        \n",
    "        # update attributes\n",
    "        self.num_topics       = attributes_dict[N_TOP_KEY]\n",
    "        self.lamb             = attributes_dict[LAMB_KEY]\n",
    "        self.embedding_matrix = attributes_dict[EMBED_KEY]\n",
    "        self.tokenizer_dict   = attributes_dict[TOKEN_KEY]\n",
    "        self.metadata_dict    = attributes_dict[META_KEY]\n",
    "        \n",
    "        # construct identical model architecture\n",
    "        self.build_model()\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_weights(os.path.join(model_path, MODEL))\n",
    "        \n",
    "        # build associated topic model\n",
    "        self.build_topic_model()\n",
    "        \n",
    "    \n",
    "    def inspect_topics(self, k_neighbors=10, tuned_embedding=False):\n",
    "        \"\"\"\n",
    "        Ouput the nearest neighbors of every topic vector in\n",
    "        the model's topic layer\n",
    "        \"\"\"\n",
    "        if tuned_embedding:\n",
    "            E = self.tuned_embedding_matrix # dim = [num_words, embedding_dim]\n",
    "        else:\n",
    "            E = self.embedding_matrix\n",
    "        Wd = self.topic_matrix    # dim = [num_topics, embedding_dim]\n",
    "        \n",
    "        for i in range(Wd.shape[0]):\n",
    "            \n",
    "            neighbors, sim = find_nn_cos(Wd[i], E, k_neighbors)\n",
    "            words = [self.tokenizer_dict['tokenizer'].index_word[v] for v in neighbors]\n",
    "            \n",
    "            print(20*\"=\" +\"\\n\")\n",
    "            print(\"Topic\", i)\n",
    "            print(words)\n",
    "    \n",
    "    \n",
    "# Orthogonality Regularizer #\n",
    "\n",
    "class Orthogonality(Regularizer):\n",
    "    \"\"\"\n",
    "    Regularizer for penalizing non-orthogonal components of a weight matrix.\n",
    "    \n",
    "    Args:\n",
    "    - lamb: (Float) regularization penalty weight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lamb = 1.):\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def __call__(self, R):\n",
    "        \"\"\"Returns a component dependence penalty for matrix R\n",
    "        \"\"\"\n",
    "        RRT = K.dot(R, K.transpose(R))\n",
    "        I = K.eye(int(RRT.shape[0]))\n",
    "        penalty = self.lamb * K.sqrt(K.sum(K.square(RRT - I)))\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    \n",
    "# Topic Purity Regularizer #\n",
    "\n",
    "class Purity(Regularizer):\n",
    "    \"\"\"Regularizer for penalizing highly impure probability distributions\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma = 1.):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, p):\n",
    "        \"\"\"Returns the avergage shannon entropy of the distribution(s) p\n",
    "        \"\"\"\n",
    "        # impurity = K.constant(7) - K.mean(K.sum(p*-K.log(p)/K.log(K.constant(2)), axis=-1))\n",
    "        impurity = K.mean(K.sum(p*-K.log(p)/K.log(K.constant(2)), axis=-1)) + K.\n",
    "        penalty = self.gamma * impurity\n",
    "        \n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmn = RMN()\n",
    "rmn.embedding_matrix = embedding_matrix\n",
    "rmn.tokenizer_dict = tokenizer_dict\n",
    "rmn.metadata_dict = metadata_dict\n",
    "rmn.num_topics = 25\n",
    "rmn.lamb = 0.1\n",
    "rmn.gamma = 0.1\n",
    "rmn.build_model(embedding_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = rmn.prep_X(d, for_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 24s 470us/sample - loss: 0.5798\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 19s 388us/sample - loss: 0.0797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe66b98ca90>"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmn.model.fit(X, y, epochs=2, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n",
      "/home/rocassius/w266_final/scripts/modeling/vector_math.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Ds = np.dot(Wv, v) / (np.linalg.norm(v) * np.linalg.norm(Wv, axis = 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "\n",
      "Topic 0\n",
      "['nrc', 'objection', 'travers', 'delaet', 'lundberg', 'dietetic', 'pathologist', 'screwtape', 'reprimand', 'kellenberger']\n",
      "====================\n",
      "\n",
      "Topic 1\n",
      "['circa', 'tarzan', 'fifties', 'sixties', 'orbison', 'thirties', 'bengali', 'orwell', 'sacagawea', 'everest']\n",
      "====================\n",
      "\n",
      "Topic 2\n",
      "['nscc', 'rrc', 'ucu', 'altmeyer', 'naag', 'iosco', 'sdb', 'emm', 'praetorian', 'inf']\n",
      "====================\n",
      "\n",
      "Topic 3\n",
      "['buttered', 'defecates', 'cellophane', 'hicksville', 'slathered', 'haberdasher', 'seedy', 'twinkies', 'hostess', 'overstuffed']\n",
      "====================\n",
      "\n",
      "Topic 4\n",
      "['sdlp', 'strife', 'burundi', 'slivka', 'displacing', 'bottlenecks', 'mostar', 'bloodshed', 'spiralling', 'meller']\n",
      "====================\n",
      "\n",
      "Topic 5\n",
      "['cotchett', 'salvatori', 'zeidler', 'hussman', 'spohr', 'laymon', 'lichenstein', 'zollo', 'harriott', 'elitch']\n",
      "====================\n",
      "\n",
      "Topic 6\n",
      "['sided', 'presses', 'mallet', 'andouille', 'sterner', 'ergonomically', 'liipfert', 'sausage', 'grater', 'pcr']\n",
      "====================\n",
      "\n",
      "Topic 7\n",
      "['walford', 'saltzman', 'porur', 'rowlands', 'ifill', 'gatton', 'feshbach', 'rmi', 'wanaque', 'doucet']\n",
      "====================\n",
      "\n",
      "Topic 8\n",
      "['willin', 'shap', 'fadel', 'cryan', 'gramlich', 'jom', 'holms', 'ith', 'mcmillion', 'halm']\n",
      "====================\n",
      "\n",
      "Topic 9\n",
      "['volusia', 'gregoire', 'camilo', 'isaias', 'waldheim', 'escalante', 'ilham', 'malino', 'izetbegovic', 'iditarod']\n",
      "====================\n",
      "\n",
      "Topic 10\n",
      "['reciprocates', 'kups', 'mesodermal', 'weizman', 'aris', 'conferring', 'tzipi', 'fulminations', 'hypoplasia', 'plosives']\n",
      "====================\n",
      "\n",
      "Topic 11\n",
      "['alito', 'reelected', 'nominee', 'respondent', 'nomination', 'appointee', 'strom', 'obituary', 'officeholder', 'mukasey']\n",
      "====================\n",
      "\n",
      "Topic 12\n",
      "['endurance', 'triathlon', 'pruett', 'treadmill', 'ironman', 'sled', 'dixiecrat', 'aerobic', 'betterment', 'oef']\n",
      "====================\n",
      "\n",
      "Topic 13\n",
      "['janzen', 'batten', 'montgomerie', 'elkington', 'narrowly', 'stadler', 'allenby', 'rflp', 'ogilvy', 'carded']\n",
      "====================\n",
      "\n",
      "Topic 14\n",
      "['bertie', 'wooster', 'mcphee', 'breck', 'mulcahy', 'shakespeare', 'cavalier', 'unabridged', 'trenchard', 'hugh']\n",
      "====================\n",
      "\n",
      "Topic 15\n",
      "['shank', 'demag', 'killingsworth', 'karcher', 'eimer', 'lanyard', 'reprogramming', 'subtraction', 'jogi', 'rosse']\n",
      "====================\n",
      "\n",
      "Topic 16\n",
      "['mattis', 'battey', 'natonski', 'indecisive', 'augur', 'widgery', 'tactician', 'saye', 'rommel', 'ilario']\n",
      "====================\n",
      "\n",
      "Topic 17\n",
      "['sete', 'lurdes', 'mozambique', 'lepa', 'tine', 'corr', 'colonna', 'nowa', 'centa', 'luxembourg']\n",
      "====================\n",
      "\n",
      "Topic 18\n",
      "['maronite', 'assyrian', 'sciri', 'canon', 'bishopric', 'bce', 'abbot', 'archbishop', 'byzantine', 'roman']\n",
      "====================\n",
      "\n",
      "Topic 19\n",
      "['flaxen', 'rapping', 'mentored', 'duo', 'vocalists', 'percussionist', 'instrumentalist', 'trio', 'songwriting', 'vocalist']\n",
      "====================\n",
      "\n",
      "Topic 20\n",
      "['romley', 'stansel', 'hurtgen', 'hatfill', 'dodson', 'tygart', 'mcloughlin', 'cayugas', 'pagones', 'stonewall']\n",
      "====================\n",
      "\n",
      "Topic 21\n",
      "['praising', 'cuomo', 'earmark', 'gono', 'ign', 'critics', 'kahan', 'earmarking', 'racist', 'radovich']\n",
      "====================\n",
      "\n",
      "Topic 22\n",
      "['fazlullah', 'bastion', 'sobriety', 'kandahar', 'brimley', 'accuracy', 'gumball', 'shamokin', 'taliban', 'battlefield']\n",
      "====================\n",
      "\n",
      "Topic 23\n",
      "['track', 'jai', 'intro', 'nintendo', 'ermey', 'speed', 'sec', 'pak', 'periscope', 'bedi']\n",
      "====================\n",
      "\n",
      "Topic 24\n",
      "['taxicab', 'pratts', 'mauer', 'tadahito', 'crede', 'durango', 'klinghoffer', 'mientkiewicz', 'glock', 'pierzynski']\n"
     ]
    }
   ],
   "source": [
    "rmn.inspect_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmn_analyzer import RMN_Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = RMN_Analyzer(rmn, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 4s 431ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.09029715511581263,\n",
       " 'lower': 0.08724137165459755,\n",
       " 'upper': 0.09335293857702773}"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js({'subject': 'minorities'}, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(p):\n",
    "    p = np.asarray(p)\n",
    "    return np.sum(p*-np.log2(p), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5836267"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(analyzer.topic_preds[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5617456"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(analyzer.topic_preds[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057870764"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(analyzer.topic_preds[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23,  2, 23, ...,  2, 15,  0])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(analyzer.topic_preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'crime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.08887185003763831,\n",
       " 'lower': 0.08536517650600806,\n",
       " 'upper': 0.0923785235692686}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js({'subject': s, 'party': 'R'}, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.08277032549908123,\n",
       " 'lower': 0.07963555455652582,\n",
       " 'upper': 0.08590509644163666}"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.group_js({'subject': s, 'party': 'D'}, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.08559621174848,\n",
       " 'lower': 0.08240782150303054,\n",
       " 'upper': 0.08878460199392947}"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.inter_party_js({'subject': s}, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
